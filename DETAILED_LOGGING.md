# –î–µ—Ç–∞–ª—å–Ω–æ–µ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ LLM –∑–∞–ø—Ä–æ—Å–æ–≤

## ‚úÖ –ß—Ç–æ –¥–æ–±–∞–≤–ª–µ–Ω–æ

–î–µ—Ç–∞–ª—å–Ω–æ–µ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ **–≤—Å–µ–≥–æ** —á—Ç–æ –ø—Ä–æ–∏—Å—Ö–æ–¥–∏—Ç –ø—Ä–∏ –≤—ã–∑–æ–≤–µ LLM:
- üìù INPUT: —á—Ç–æ –æ—Ç–ø—Ä–∞–≤–ª—è–µ—Ç—Å—è –≤ LLM
- üì§ OUTPUT: —á—Ç–æ –ø–æ–ª—É—á–∞–µ–º –≤ –æ—Ç–≤–µ—Ç–µ
- ‚è±Ô∏è –ú–µ—Ç—Ä–∏–∫–∏: –≤—Ä–µ–º—è, —Å–∫–æ—Ä–æ—Å—Ç—å, —Ä–∞–∑–º–µ—Ä—ã

---

## üìã –§–æ—Ä–º–∞—Ç –ª–æ–≥–æ–≤

### 1. RAG Context (–≤ /ask endpoint)

```
================================================================================
[RAG CONTEXT] Query: '–ö–∞–∫–æ–π –±—é–¥–∂–µ—Ç –ø—Ä–æ–µ–∫—Ç–∞?'
================================================================================
üìö RETRIEVED DOCUMENTS:
  - Chunks found: 3
  - Context size: ~450 tokens
  - Model: llama3.1:8b
  - Context window: 8192
  - Summarization threshold: 3000
  - Mode: auto
  [1] test_doc_564945be chunk_0: # –¢–µ—Å—Ç–æ–≤—ã–π –¥–æ–∫—É–º–µ–Ω—Ç –¥–ª—è —Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏–∏

## –ü—Ä–æ–µ–∫—Ç Alpha

–ü—Ä–æ–µ–∫—Ç Alpha –Ω–∞—á–∞–ª—Å—è 1 ...
  [2] test_doc_564945be chunk_1: ...
  [3] SUMMARIZATION_COMPLETE_GUIDE_eda9ebe5 chunk_0: ...
================================================================================
```

### 2. LLM Request (—á—Ç–æ –æ—Ç–ø—Ä–∞–≤–ª—è–µ–º)

```
================================================================================
[LLM REQUEST ‚Üí Ollama] Model: llama3.1:8b
================================================================================
üìù INPUT:
  - Prompt length: 1234 chars, ~456 tokens
  - Max output tokens: 2048
  - Context window: 8192
  - Timeout: 300s
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
PROMPT PREVIEW (first 500 chars):
–¢—ã ‚Äî –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç, –æ—Ç–≤–µ—á–∞–π —Å—Ç—Ä–æ–≥–æ –ø–æ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω–Ω–æ–º—É –ö–û–ù–¢–ï–ö–°–¢–£. 
–ï—Å–ª–∏ –¥–∞–Ω–Ω—ã—Ö –Ω–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ ‚Äî —Ç–∞–∫ –∏ —Å–∫–∞–∂–∏.

–ö–û–ù–¢–ï–ö–°–¢:
[1] doc_id=test_doc_564945be chunk=0 type=technical_docs
"# –¢–µ—Å—Ç–æ–≤—ã–π –¥–æ–∫—É–º–µ–Ω—Ç –¥–ª—è —Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏–∏..."

–í–û–ü–†–û–°:
–ö–∞–∫–æ–π –±—é–¥–∂–µ—Ç –ø—Ä–æ–µ–∫—Ç–∞?

–û—Ç–≤–µ—Ç—å –∫—Ä–∞—Ç–∫–æ –∏ –ø–æ –¥–µ–ª—É...
================================================================================
```

### 3. LLM Response (—á—Ç–æ –ø–æ–ª—É—á–∞–µ–º)

```
================================================================================
[LLM RESPONSE ‚Üê Ollama]
================================================================================
üì§ OUTPUT:
  - Response length: 123 chars, ~25 tokens
  - Generation time: 3.45s
  - Speed: ~7.2 tokens/sec
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
RESPONSE PREVIEW (first 500 chars):
Based on the context, Project Alpha has a budget of $200,000.
This budget was established at the start of the project on March 1, 2025.
================================================================================
```

---

## üéØ –ß—Ç–æ –ª–æ–≥–∏—Ä—É–µ—Ç—Å—è

### INPUT (–∑–∞–ø—Ä–æ—Å –≤ LLM):

| –ú–µ—Ç—Ä–∏–∫–∞ | –û–ø–∏—Å–∞–Ω–∏–µ |
|---------|----------|
| **Prompt length** | –†–∞–∑–º–µ—Ä –ø—Ä–æ–º–ø—Ç–∞ –≤ —Å–∏–º–≤–æ–ª–∞—Ö –∏ —Ç–æ–∫–µ–Ω–∞—Ö |
| **Max output tokens** | –õ–∏–º–∏—Ç –Ω–∞ –æ—Ç–≤–µ—Ç (–¥–∏–Ω–∞–º–∏—á–µ—Å–∫–∏–π –∏–ª–∏ —Ñ–∏–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã–π) |
| **Context window** | –†–∞–∑–º–µ—Ä –∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω–æ–≥–æ –æ–∫–Ω–∞ –º–æ–¥–µ–ª–∏ |
| **Timeout** | –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –≤—Ä–µ–º—è –æ–∂–∏–¥–∞–Ω–∏—è |
| **Prompt preview** | –ü–µ—Ä–≤—ã–µ 500 —Å–∏–º–≤–æ–ª–æ–≤ –ø—Ä–æ–º–ø—Ç–∞ |

### OUTPUT (–æ—Ç–≤–µ—Ç –æ—Ç LLM):

| –ú–µ—Ç—Ä–∏–∫–∞ | –û–ø–∏—Å–∞–Ω–∏–µ |
|---------|----------|
| **Response length** | –†–∞–∑–º–µ—Ä –æ—Ç–≤–µ—Ç–∞ –≤ —Å–∏–º–≤–æ–ª–∞—Ö –∏ —Ç–æ–∫–µ–Ω–∞—Ö |
| **Generation time** | –í—Ä–µ–º—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –≤ —Å–µ–∫—É–Ω–¥–∞—Ö |
| **Speed** | –°–∫–æ—Ä–æ—Å—Ç—å –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ (—Ç–æ–∫–µ–Ω–æ–≤/—Å–µ–∫) |
| **Response preview** | –ü–µ—Ä–≤—ã–µ 500 —Å–∏–º–≤–æ–ª–æ–≤ –æ—Ç–≤–µ—Ç–∞ |

### RAG Context (–¥–ª—è /ask):

| –ú–µ—Ç—Ä–∏–∫–∞ | –û–ø–∏—Å–∞–Ω–∏–µ |
|---------|----------|
| **Query** | –ó–∞–ø—Ä–æ—Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è |
| **Chunks found** | –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –Ω–∞–π–¥–µ–Ω–Ω—ã—Ö —á–∞–Ω–∫–æ–≤ |
| **Context size** | –†–∞–∑–º–µ—Ä –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –≤ —Ç–æ–∫–µ–Ω–∞—Ö |
| **Model info** | –ú–æ–¥–µ–ª—å, context window, threshold |
| **Mode** | –†–µ–∂–∏–º —Ä–∞–±–æ—Ç—ã (auto/normal/summarize) |
| **Chunks preview** | –ü—Ä–µ–¥–ø—Ä–æ—Å–º–æ—Ç—Ä –ø–µ—Ä–≤—ã—Ö 5 —á–∞–Ω–∫–æ–≤ |

---

## üîç –ü—Ä–∏–º–µ—Ä—ã –ª–æ–≥–æ–≤

### –ü—Ä–∏–º–µ—Ä 1: –û–±—ã—á–Ω—ã–π RAG (–±–µ–∑ —Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏–∏)

```bash
curl -X POST /ask -d '{"q":"–ö–∞–∫–æ–π –±—é–¥–∂–µ—Ç?","space_id":"demo","mode":"auto"}'
```

**–õ–æ–≥–∏:**

```
================================================================================
[RAG CONTEXT] Query: '–ö–∞–∫–æ–π –±—é–¥–∂–µ—Ç?'
================================================================================
üìö RETRIEVED DOCUMENTS:
  - Chunks found: 3
  - Context size: ~450 tokens
  - Model: llama3.1:8b
  - Context window: 8192
  - Summarization threshold: 3000
  - Mode: auto
  [1] test_doc_564945be chunk_0: # –¢–µ—Å—Ç–æ–≤—ã–π –¥–æ–∫—É–º–µ–Ω—Ç...
  [2] test_doc_564945be chunk_1: ...
  [3] test_doc_564945be chunk_2: ...
================================================================================

[RAG] Mode: auto. Context 450 tokens ‚â§ threshold 3000. Using normal RAG.

================================================================================
[LLM REQUEST ‚Üí Ollama] Model: llama3.1:8b
================================================================================
üìù INPUT:
  - Prompt length: 678 chars, ~156 tokens
  - Max output tokens: 2048
  - Context window: 8192
  - Timeout: 300s
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
PROMPT PREVIEW (first 500 chars):
–¢—ã ‚Äî –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç, –æ—Ç–≤–µ—á–∞–π —Å—Ç—Ä–æ–≥–æ –ø–æ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω–Ω–æ–º—É –ö–û–ù–¢–ï–ö–°–¢–£...
================================================================================

================================================================================
[LLM RESPONSE ‚Üê Ollama]
================================================================================
üì§ OUTPUT:
  - Response length: 89 chars, ~18 tokens
  - Generation time: 2.34s
  - Speed: ~7.7 tokens/sec
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
RESPONSE PREVIEW (first 500 chars):
Based on the context, Project Alpha has a budget of $200,000.
================================================================================
```

### –ü—Ä–∏–º–µ—Ä 2: –° –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–π —Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏–µ–π

```bash
curl -X POST /ask -d '{"q":"–†–∞—Å—Å–∫–∞–∂–∏ –≤—Å–µ –ø—Ä–æ –ø—Ä–æ–µ–∫—Ç","mode":"auto","top_k":20}'
```

**–õ–æ–≥–∏:**

```
================================================================================
[RAG CONTEXT] Query: '–†–∞—Å—Å–∫–∞–∂–∏ –≤—Å–µ –ø—Ä–æ –ø—Ä–æ–µ–∫—Ç'
================================================================================
üìö RETRIEVED DOCUMENTS:
  - Chunks found: 20
  - Context size: ~5500 tokens
  - Model: llama3.1:8b
  - Context window: 8192
  - Summarization threshold: 3000
  - Mode: auto
  [1] doc_1 chunk_0: ...
  [2] doc_2 chunk_3: ...
  ... –∏ –µ—â–µ 15 —á–∞–Ω–∫–æ–≤
================================================================================

[RAG] Mode: auto. Context 5500 tokens > threshold 3000. Using summarization.

[summarize_chunks] Context: ~5500 tokens, dynamic max_tokens: 1500

[Summarization] Document is large (5500 tokens). Starting Map-Reduce...
[Summarization] Split into 2 chunks
[Summarization] Processing chunk 1/2...

[Summarization] Dynamic max_tokens: 300 (requested: 300, available: 3500)

================================================================================
[LLM REQUEST ‚Üí Ollama] Model: llama3.1:8b
================================================================================
üìù INPUT:
  - Prompt length: 2890 chars, ~650 tokens
  - Max output tokens: 300
  - Context window: 8192
  - Timeout: 300s
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
PROMPT PREVIEW (first 500 chars):
Summarize the following text concisely in approximately 300 words or less...
================================================================================

================================================================================
[LLM RESPONSE ‚Üê Ollama]
================================================================================
üì§ OUTPUT:
  - Response length: 567 chars, ~95 tokens
  - Generation time: 8.23s
  - Speed: ~11.5 tokens/sec
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
RESPONSE PREVIEW (first 500 chars):
Part 1 summary: Project Alpha is a software development initiative with a $200K budget...
================================================================================

[Summarization] Processing chunk 2/2...
[... –ø–æ–≤—Ç–æ—Ä—è–µ—Ç—Å—è –¥–ª—è chunk 2 ...]

[Summarization] REDUCE phase: Combining 2 summaries...
[Summarization] Final summary max_tokens: 800

[... —Ñ–∏–Ω–∞–ª—å–Ω–∞—è —Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏—è ...]

================================================================================
[LLM REQUEST ‚Üí Ollama] Model: llama3.1:8b
================================================================================
üìù INPUT:
  - Prompt length: 1456 chars, ~312 tokens
  - Max output tokens: 2048
  - Context window: 8192
  - Timeout: 300s
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
PROMPT PREVIEW (first 500 chars):
–¢—ã ‚Äî –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç, –æ—Ç–≤–µ—á–∞–π —Å—Ç—Ä–æ–≥–æ –ø–æ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω–Ω–æ–º—É –ö–û–ù–¢–ï–ö–°–¢–£...
–ö–û–ù–¢–ï–ö–°–¢ (—Å—É–º–º–∞—Ä–∏–∑–∏—Ä–æ–≤–∞–Ω–æ –∏–∑ 20 –Ω–∞–π–¥–µ–Ω–Ω—ã—Ö —Ñ—Ä–∞–≥–º–µ–Ω—Ç–æ–≤):
[Final summary here...]
================================================================================

================================================================================
[LLM RESPONSE ‚Üê Ollama]
================================================================================
üì§ OUTPUT:
  - Response length: 345 chars, ~67 tokens
  - Generation time: 5.67s
  - Speed: ~11.8 tokens/sec
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
RESPONSE PREVIEW (first 500 chars):
Project Alpha is a $200,000 software development project that started March 1, 2025...
================================================================================
```

---

## üéØ –ü–æ–ª—å–∑–∞ –¥–ª—è –æ—Ç–ª–∞–¥–∫–∏

### 1. –í–∏–¥–Ω–æ —Ç–æ—á–Ω—ã–π —Ä–∞–∑–º–µ—Ä –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞

–ú–æ–∂–µ–º –ø—Ä–æ–≤–µ—Ä–∏—Ç—å:
- –î–µ–π—Å—Ç–≤–∏—Ç–µ–ª—å–Ω–æ –ª–∏ –∫–æ–Ω—Ç–µ–∫—Å—Ç –ø—Ä–µ–≤—ã—à–∞–µ—Ç threshold?
- –°–∫–æ–ª—å–∫–æ —Ç–æ–∫–µ–Ω–æ–≤ —É—Ö–æ–¥–∏—Ç –Ω–∞ –ø—Ä–æ–º–ø—Ç vs –æ—Ç–≤–µ—Ç?
- –í–ª–µ–∑–∞–µ—Ç –ª–∏ –≤—Å—ë –≤ context_window?

### 2. –î–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∞ –ø—Ä–æ–±–ª–µ–º

```
# –ï—Å–ª–∏ –æ—Ç–≤–µ—Ç –æ–±—Ä–µ–∑–∞–Ω:
Response length: 2048 chars, ~512 tokens  
‚Üí –î–æ—Å—Ç–∏–≥ –ª–∏–º–∏—Ç–∞ max_tokens! –ù—É–∂–Ω–æ —É–≤–µ–ª–∏—á–∏—Ç—å.

# –ï—Å–ª–∏ timeout:
Generation time: 301.23s
‚Üí –ü—Ä–µ–≤—ã—Å–∏–ª LLM_TIMEOUT! –£–≤–µ–ª–∏—á–∏—Ç—å –∏–ª–∏ —É–ø—Ä–æ—Å—Ç–∏—Ç—å –ø—Ä–æ–º–ø—Ç.

# –ï—Å–ª–∏ –∫–æ–Ω—Ç–µ–∫—Å—Ç –Ω–µ –≤–ª–µ–∑–∞–µ—Ç:
Prompt: ~7500 tokens, Max output: 2048
Total: 9548 > Context window: 8192
‚Üí –ù—É–∂–Ω–∞ —Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏—è!
```

### 3. –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏

```
Speed: ~7.2 tokens/sec  (llama3.1:8b –Ω–∞ CPU)
Speed: ~150 tokens/sec  (vLLM –Ω–∞ GPU)

‚Üí –í–∏–¥–∏–º —Ä–∞–∑–Ω–∏—Ü—É –∏ –º–æ–∂–µ–º –ø—Ä–∏–Ω—è—Ç—å —Ä–µ—à–µ–Ω–∏–µ –æ –º–∏–≥—Ä–∞—Ü–∏–∏ –Ω–∞ GPU
```

---

## üìä –ú–µ—Ç—Ä–∏–∫–∏ –≤ –ª–æ–≥–∞—Ö

### –î–ª—è –∫–∞–∂–¥–æ–≥–æ LLM –≤—ã–∑–æ–≤–∞:

```
INPUT:
  Prompt: X chars, ~Y tokens
  Max output: Z tokens
  Context window: W tokens
  
  –ü—Ä–æ–≤–µ—Ä–∫–∞: Y + Z < W ‚úÖ (–¥–æ–ª–∂–Ω–æ –≤–ª–µ–∑–∞—Ç—å)

OUTPUT:
  Response: A chars, ~B tokens
  Time: C seconds
  Speed: B/C tokens/sec
  
  –ü—Ä–æ–≤–µ—Ä–∫–∞: B ‚â§ Z ‚úÖ (–Ω–µ –ø—Ä–µ–≤—ã—Å–∏–ª –ª–∏–º–∏—Ç)
```

---

## üîß –£–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ–º

### –û—Ç–∫–ª—é—á–∏—Ç—å –¥–µ—Ç–∞–ª—å–Ω—ã–µ –ª–æ–≥–∏ (production):

–ú–æ–∂–Ω–æ –¥–æ–±–∞–≤–∏—Ç—å –ø–µ—Ä–µ–º–µ–Ω–Ω—É—é –æ–∫—Ä—É–∂–µ–Ω–∏—è:

```python
# –í config.py
VERBOSE_LLM_LOGGING = os.getenv("VERBOSE_LLM_LOGGING", "true").lower() == "true"

# –í rag.py
if VERBOSE_LLM_LOGGING:
    print(f"[LLM REQUEST] ...")
```

```yaml
# docker-compose.yml
environment:
  - VERBOSE_LLM_LOGGING=false  # –û—Ç–∫–ª—é—á–∏—Ç—å –≤ production
```

---

## üéâ –ì–æ—Ç–æ–≤–æ –∫ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—é!

–¢–µ–ø–µ—Ä—å –ø—Ä–∏ –∫–∞–∂–¥–æ–º –≤—ã–∑–æ–≤–µ LLM –≤—ã –±—É–¥–µ—Ç–µ –≤–∏–¥–µ—Ç—å:

1. **–ß—Ç–æ –æ—Ç–ø—Ä–∞–≤–ª—è–µ—Ç—Å—è:**
   - –¢–æ—á–Ω—ã–π —Ä–∞–∑–º–µ—Ä –ø—Ä–æ–º–ø—Ç–∞
   - –õ–∏–º–∏—Ç—ã –Ω–∞ –æ—Ç–≤–µ—Ç
   - –ü—Ä–µ–¥–ø—Ä–æ—Å–º–æ—Ç—Ä –ø—Ä–æ–º–ø—Ç–∞

2. **–ß—Ç–æ –ø–æ–ª—É—á–∞–µ–º:**
   - –†–∞–∑–º–µ—Ä –æ—Ç–≤–µ—Ç–∞
   - –í—Ä–µ–º—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏
   - –°–∫–æ—Ä–æ—Å—Ç—å
   - –ü—Ä–µ–¥–ø—Ä–æ—Å–º–æ—Ç—Ä –æ—Ç–≤–µ—Ç–∞

3. **RAG –∫–æ–Ω—Ç–µ–∫—Å—Ç:**
   - –ù–∞–π–¥–µ–Ω–Ω—ã–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã
   - –†–∞–∑–º–µ—Ä –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
   - –ë—É–¥–µ—Ç –ª–∏ —Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏—è

**–°–º–æ—Ç—Ä–∏—Ç–µ –ª–æ–≥–∏:** `docker compose logs -f backend`

