# –ö–∞–∫ —Ä–∞–±–æ—Ç–∞–µ—Ç –ø–æ—Ä–æ–≥ —Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏–∏ (summarization_threshold)

## üìã TL;DR

**–ü–æ—Ä–æ–≥ —Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏–∏ –¥–ª—è –í–°–ï–• –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–π (Ollama –∏ vLLM) –±–µ—Ä—ë—Ç—Å—è –∏–∑ `config/llm_models.json`**

---

## üîç –î–µ—Ç–∞–ª—å–Ω–æ–µ –æ–±—ä—è—Å–Ω–µ–Ω–∏–µ

### 1. –ì–¥–µ —Ö—Ä–∞–Ω–∏—Ç—Å—è –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è?

**–§–∞–π–ª:** `config/llm_models.json`

–°–æ–¥–µ—Ä–∂–∏—Ç –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ –¥–ª—è –≤—Å–µ—Ö –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã—Ö –º–æ–¥–µ–ª–µ–π:
- Ollama –º–æ–¥–µ–ª–∏ (qwen3:14b, llama3.1:8b, mistral:7b)
- vLLM –º–æ–¥–µ–ª–∏ (openai/gpt-oss-20b, Meta-Llama-3.1-8B, Mixtral-8x7B)
- Cloud –º–æ–¥–µ–ª–∏ (gpt-4, claude-3)

### 2. –ö–∞–∫ –æ–ø—Ä–µ–¥–µ–ª—è–µ—Ç—Å—è —Ç–µ–∫—É—â–∞—è –º–æ–¥–µ–ª—å?

**–ü–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –æ–∫—Ä—É–∂–µ–Ω–∏—è (.env –∏–ª–∏ docker-compose.yml):**

```bash
# –î–ª—è Ollama
LLM_MODE=ollama
LLM_MODEL=qwen3:14b

# –î–ª—è vLLM
LLM_MODE=vllm
LLM_MODEL=openai/gpt-oss-20b
```

### 3. –ö–∞–∫ –∑–∞–≥—Ä—É–∂–∞–µ—Ç—Å—è –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è?

**–ö–æ–¥ –≤ `backend/services/llm_config.py`:**

```python
def get_current_model_config() -> LLMModelConfig:
    """
    Get configuration for currently active LLM model
    Reads from environment variables LLM_MODE and LLM_MODEL
    """
    from . import config
    
    registry = get_llm_registry()  # –ó–∞–≥—Ä—É–∂–∞–µ—Ç llm_models.json
    return registry.get_or_default(config.LLM_MODE, config.LLM_MODEL)
```

**–ü—Ä–æ—Ü–µ—Å—Å:**
1. –ß–∏—Ç–∞–µ—Ç `LLM_MODE` –∏ `LLM_MODEL` –∏–∑ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö –æ–∫—Ä—É–∂–µ–Ω–∏—è
2. –ò—â–µ—Ç `{provider}::{model_name}` –≤ registry
3. –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é (–∏–ª–∏ defaults –µ—Å–ª–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ)

### 4. –ì–¥–µ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –ø–æ—Ä–æ–≥?

**–ö–æ–¥ –≤ `backend/app.py` (endpoint `/ask`):**

```python
# –°—Ç—Ä–æ–∫–∞ 256: –ü–æ–ª—É—á–∏—Ç—å –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é –º–æ–¥–µ–ª–∏
model_config = get_current_model_config()

# –°—Ç—Ä–æ–∫–∞ 276: –ü—Ä–æ–≤–µ—Ä–∫–∞ –ø–æ—Ä–æ–≥–∞ (—Ä–µ–∂–∏–º "auto")
should_summarize = context_tokens > model_config.summarization_threshold

if should_summarize:
    # –°—É–º–º–∞—Ä–∏–∑–∏—Ä–æ–≤–∞—Ç—å –∫–æ–Ω—Ç–µ–∫—Å—Ç
    summary = summarize_chunks(fused, query=req.q)
    prompt = build_prompt_with_summary(summary, req.q)
else:
    # –û–±—ã—á–Ω—ã–π RAG
    prompt = build_prompt(fused, req.q)
```

---

## üìä –ü—Ä–∏–º–µ—Ä—ã –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–π

### Ollama: qwen3:14b

```json
{
  "model_name": "qwen3:14b",
  "provider": "ollama",
  "context_window": 40960,
  "max_output_tokens": 2048,
  "summarization_threshold": 15000,
  "summarization_max_output": 5000
}
```

**–ü–æ–≤–µ–¥–µ–Ω–∏–µ:**
- –ï—Å–ª–∏ –∫–æ–Ω—Ç–µ–∫—Å—Ç ‚â§ 15000 —Ç–æ–∫–µ–Ω–æ–≤ ‚Üí –æ–±—ã—á–Ω—ã–π RAG
- –ï—Å–ª–∏ –∫–æ–Ω—Ç–µ–∫—Å—Ç > 15000 —Ç–æ–∫–µ–Ω–æ–≤ ‚Üí —Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏—è

### vLLM: openai/gpt-oss-20b

```json
{
  "model_name": "openai/gpt-oss-20b",
  "provider": "vllm",
  "context_window": 8192,
  "max_output_tokens": 512,
  "summarization_threshold": 3000,
  "summarization_max_output": 1500
}
```

**–ü–æ–≤–µ–¥–µ–Ω–∏–µ:**
- –ï—Å–ª–∏ –∫–æ–Ω—Ç–µ–∫—Å—Ç ‚â§ 3000 —Ç–æ–∫–µ–Ω–æ–≤ ‚Üí –æ–±—ã—á–Ω—ã–π RAG
- –ï—Å–ª–∏ –∫–æ–Ω—Ç–µ–∫—Å—Ç > 3000 —Ç–æ–∫–µ–Ω–æ–≤ ‚Üí —Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏—è

### vLLM: mistralai/Mistral-7B-Instruct-v0.3

```json
{
  "model_name": "mistralai/Mistral-7B-Instruct-v0.3",
  "provider": "vllm",
  "context_window": 32768,
  "max_output_tokens": 1024,
  "summarization_threshold": 12000,
  "summarization_max_output": 4000
}
```

**–ü–æ–≤–µ–¥–µ–Ω–∏–µ:**
- –ï—Å–ª–∏ –∫–æ–Ω—Ç–µ–∫—Å—Ç ‚â§ 12000 —Ç–æ–∫–µ–Ω–æ–≤ ‚Üí –æ–±—ã—á–Ω—ã–π RAG
- –ï—Å–ª–∏ –∫–æ–Ω—Ç–µ–∫—Å—Ç > 12000 —Ç–æ–∫–µ–Ω–æ–≤ ‚Üí —Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏—è

---

## üîß –î–≤–∞ —Ä–∞–∑–Ω—ã—Ö –ø–∞—Ä–∞–º–µ—Ç—Ä–∞

### –ù–µ –ø—É—Ç–∞—Ç—å!

| –ß—Ç–æ | –ì–¥–µ –Ω–∞—Å—Ç—Ä–∞–∏–≤–∞–µ—Ç—Å—è | –î–ª—è —á–µ–≥–æ |
|-----|-------------------|----------|
| **Context Window** | `.env` ‚Üí `VLLM_MAX_MODEL_LEN` –∏–ª–∏ `OLLAMA_NUM_CTX` | –†–∞–∑–º–µ—Ä –∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω–æ–≥–æ –æ–∫–Ω–∞ LLM —Å–µ—Ä–≤–µ—Ä–∞ |
| **Summarization Threshold** | `llm_models.json` ‚Üí `summarization_threshold` | –ü–æ—Ä–æ–≥ –¥–ª—è –≤–∫–ª—é—á–µ–Ω–∏—è —Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏–∏ –≤ RAG |

### –î–ª—è vLLM

```yaml
# docker-compose.vllm-mig.yml
vllm-medium:
  environment:
    - VLLM_MODEL=openai/gpt-oss-20b
    - VLLM_MAX_MODEL_LEN=8192  # ‚Üê Context window —Å–µ—Ä–≤–µ—Ä–∞
```

```json
// config/llm_models.json
{
  "model_name": "openai/gpt-oss-20b",
  "context_window": 8192,           // ‚Üê –î–æ–ª–∂–Ω–æ —Å–æ–≤–ø–∞–¥–∞—Ç—å
  "summarization_threshold": 3000   // ‚Üê –ü–æ—Ä–æ–≥ —Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏–∏ (~37%)
}
```

### –î–ª—è Ollama

```yaml
# docker-compose.yml
backend:
  environment:
    - LLM_MODEL=qwen3:14b
    - OLLAMA_NUM_CTX=40960  # ‚Üê Context window –ø—Ä–∏ –≤—ã–∑–æ–≤–∞—Ö
```

```json
// config/llm_models.json
{
  "model_name": "qwen3:14b",
  "context_window": 40960,          // ‚Üê –î–æ–ª–∂–Ω–æ —Å–æ–≤–ø–∞–¥–∞—Ç—å
  "summarization_threshold": 15000  // ‚Üê –ü–æ—Ä–æ–≥ —Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏–∏ (~37%)
}
```

---

## ‚úÖ –ü—Ä–æ–≤–µ—Ä–∫–∞ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏

### API endpoint –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏

```bash
curl http://localhost:8000/model-config | jq
```

**–û–∂–∏–¥–∞–µ–º—ã–π –æ—Ç–≤–µ—Ç:**

```json
{
  "model_name": "qwen3:14b",
  "provider": "ollama",
  "context_window": 40960,
  "max_output_tokens": 2048,
  "effective_context_for_rag": 38912,
  "summarization_threshold": 15000,
  "summarization_max_output": 5000,
  "recommended_chunk_limit": 129,
  "tokens_per_second": 45
}
```

### –ü—Ä–æ–≤–µ—Ä–∫–∞ –≤ –ª–æ–≥–∞—Ö

–ü—Ä–∏ –≤—ã–∑–æ–≤–µ `/ask` —Å —Ä–µ–∂–∏–º–æ–º `auto`:

```bash
curl -X POST http://localhost:8000/ask \
  -d '{"q":"test","space_id":"demo","mode":"auto","top_k":20}'
```

**–í –ª–æ–≥–∞—Ö backend –±—É–¥–µ—Ç:**

```
[RAG] Mode: auto. Context 5000 tokens ‚â§ threshold 15000. Using normal RAG.
# –∏–ª–∏
[RAG] Mode: auto. Context 18000 tokens > threshold 15000. Using summarization.
```

---

## üéØ –ö–∞–∫ –¥–æ–±–∞–≤–∏—Ç—å –Ω–æ–≤—É—é –º–æ–¥–µ–ª—å?

### 1. –î–æ–±–∞–≤–∏—Ç—å –≤ llm_models.json

```json
{
  "model_name": "your-model-name",
  "provider": "vllm",  // –∏–ª–∏ "ollama"
  "context_window": 16384,
  "max_output_tokens": 1024,
  "summarization_threshold": 6000,  // ~37-40% –æ—Ç effective context
  "summarization_max_output": 3000,
  "tokens_per_second": 200,
  "supports_streaming": true,
  "supports_function_calling": true,
  "description": "Your model description"
}
```

### 2. –û–±–Ω–æ–≤–∏—Ç—å .env

```bash
LLM_MODE=vllm
LLM_MODEL=your-model-name
VLLM_MAX_MODEL_LEN=16384  # –î–æ–ª–∂–Ω–æ —Å–æ–≤–ø–∞–¥–∞—Ç—å —Å context_window
```

### 3. –ü—Ä–æ–≤–µ—Ä–∏—Ç—å

```bash
make up
make wait
curl http://localhost:8000/model-config | jq
```

---

## üìö –°–≤—è–∑–∞–Ω–Ω—ã–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã

- `config/llm_models.json` - –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ –º–æ–¥–µ–ª–µ–π
- `backend/services/llm_config.py` - –∑–∞–≥—Ä—É–∑–∫–∞ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–π
- `backend/app.py` - –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –ø–æ—Ä–æ–≥–∞ –≤ `/ask`
- `CONTEXT_WINDOW_CONFIGURATION.md` - –Ω–∞—Å—Ç—Ä–æ–π–∫–∞ –∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω—ã—Ö –æ–∫–æ–Ω
- `OLLAMA_QWEN3_SETUP.md` - –Ω–∞—Å—Ç—Ä–æ–π–∫–∞ Ollama

---

**–í—ã–≤–æ–¥:** –ü–æ—Ä–æ–≥ —Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏–∏ –í–°–ï–ì–î–ê –≤ `llm_models.json`, –Ω–µ–∑–∞–≤–∏—Å–∏–º–æ –æ—Ç Ollama –∏–ª–∏ vLLM! üéØ

