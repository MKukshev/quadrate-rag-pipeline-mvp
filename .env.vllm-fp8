# Configuration for vLLM with FP8 on RTX 6000 Blackwell
# FP8 allows running larger models (70B+) on single GPU

LLM_MODE=vllm
LLM_VLLM_URL=http://vllm:8001/v1
LLM_MODEL=neuralmagic/Meta-Llama-3.1-70B-Instruct-FP8
LLM_TIMEOUT=180
LLM_MAX_TOKENS=512
LLM_TEMPERATURE=0.7

# vLLM Model - FP8 quantized
VLLM_MODEL=neuralmagic/Meta-Llama-3.1-70B-Instruct-FP8

# Blackwell FP8 settings (70B model fits in 96GB!)
VLLM_GPU_MEMORY_UTILIZATION=0.95
VLLM_MAX_MODEL_LEN=16384
VLLM_TENSOR_PARALLEL_SIZE=1
VLLM_MAX_NUM_SEQS=256

# Enable FP8 - primary Blackwell advantage
VLLM_ENABLE_FP8=true
VLLM_QUANTIZATION=fp8
VLLM_USE_FLASHINFER=true
VLLM_ENABLE_CHUNKED_PREFILL=true

# HuggingFace Token (required for Llama models)
HUGGING_FACE_HUB_TOKEN=your_token_here

# Other settings
QDRANT_URL=http://qdrant:6333
EMBED_MODEL=sentence-transformers/all-MiniLM-L6-v2
