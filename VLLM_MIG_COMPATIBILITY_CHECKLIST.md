# –ß–µ–∫–ª–∏—Å—Ç —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏ —Å vLLM MIG

**–°—Ç–∞—Ç—É—Å:** ‚úÖ –õ–æ–∫–∞–ª—å–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ –ø—Ä–æ–π–¥–µ–Ω–∞  
**–¢—Ä–µ–±—É–µ—Ç—Å—è:** –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ GPU —Å–µ—Ä–≤–µ—Ä–µ —Å NVIDIA MIG

---

## ‚úÖ –ß—Ç–æ –ø—Ä–æ–≤–µ—Ä–µ–Ω–æ –ª–æ–∫–∞–ª—å–Ω–æ (–±–µ–∑ GPU)

### 1. –°–∏–Ω—Ç–∞–∫—Å–∏—Å docker-compose.vllm-mig.yml
```bash
docker compose -f docker-compose.vllm-mig.yml config
```
**–†–µ–∑—É–ª—å—Ç–∞—Ç:** ‚úÖ –í–∞–ª–∏–¥–µ–Ω, –æ—à–∏–±–æ–∫ –Ω–µ—Ç

### 2. –ö–æ–¥ —Å–æ–≤–º–µ—Å—Ç–∏–º —Å vLLM –∏ Ollama
**–ü—Ä–æ–≤–µ—Ä–µ–Ω–æ:**
- ‚úÖ `call_llm()` –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç –æ–±–∞ —Ä–µ–∂–∏–º–∞ (LLM_MODE=ollama/vllm)
- ‚úÖ –î–∏–Ω–∞–º–∏—á–µ—Å–∫–∏–π max_tokens —Ä–∞–±–æ—Ç–∞–µ—Ç –¥–ª—è –æ–±–æ–∏—Ö
- ‚úÖ Language detection —É–Ω–∏–≤–µ—Ä—Å–∞–ª–µ–Ω
- ‚úÖ –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –æ–¥–∏–Ω–∞–∫–æ–≤–æ–µ

### 3. –ü–∞—Ä–∞–º–µ—Ç—Ä—ã —Å–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∏—Ä–æ–≤–∞–Ω—ã

**–î–æ–±–∞–≤–ª–µ–Ω–æ –≤ vLLM MIG:**
- ‚úÖ `PYTHONUNBUFFERED=1` - –¥–ª—è real-time –ª–æ–≥–æ–≤
- ‚úÖ `LLM_MAX_TOKENS=${LLM_MAX_TOKENS:-2048}` - —É–≤–µ–ª–∏—á–µ–Ω–æ —Å 512

### 4. –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –º–æ–¥–µ–ª–µ–π

**–í `config/llm_models.json` –¥–æ–±–∞–≤–ª–µ–Ω—ã:**
- ‚úÖ `openai/gpt-oss-20b` (MEDIUM model)
- ‚úÖ `OpenGPT/gpt-oss-20b` (–∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω–æ–µ –∏–º—è)
- ‚úÖ `mistralai/Mistral-7B-Instruct-v0.3` (SMALL model)

---

## üîç –ß—Ç–æ –Ω—É–∂–Ω–æ –ø—Ä–æ–≤–µ—Ä–∏—Ç—å –Ω–∞ GPU —Å–µ—Ä–≤–µ—Ä–µ

### –®–∞–≥ 1: –ó–∞–ø—É—Å–∫ vLLM MIG

```bash
# –ù–∞ GPU —Å–µ—Ä–≤–µ—Ä–µ —Å NVIDIA MIG
cd /path/to/project

# –°–∫–æ–ø–∏—Ä–æ–≤–∞—Ç—å .env –¥–ª—è vLLM MIG
cp .env.vllm-mig .env

# –ó–∞–ø—É—Å—Ç–∏—Ç—å
docker compose -f docker-compose.vllm-mig.yml up -d

# –ü—Ä–æ–≤–µ—Ä–∏—Ç—å –ª–æ–≥–∏
docker compose -f docker-compose.vllm-mig.yml logs -f vllm-medium
docker compose -f docker-compose.vllm-mig.yml logs -f vllm-small
docker compose -f docker-compose.vllm-mig.yml logs -f backend
```

**–û–∂–∏–¥–∞–µ–º–æ:** –ö–æ–Ω—Ç–µ–π–Ω–µ—Ä—ã –∑–∞–ø—É—Å–∫–∞—é—Ç—Å—è –±–µ–∑ –æ—à–∏–±–æ–∫

---

### –®–∞–≥ 2: –ü—Ä–æ–≤–µ—Ä–∏—Ç—å health

```bash
# vLLM MEDIUM (port 8001)
curl http://localhost:8001/health

# vLLM SMALL (port 8002)
curl http://localhost:8002/health

# Backend
curl http://localhost:8000/health
```

**–û–∂–∏–¥–∞–µ–º–æ:** –í—Å–µ –≤–æ–∑–≤—Ä–∞—â–∞—é—Ç 200 OK

---

### –®–∞–≥ 3: –ü—Ä–æ–≤–µ—Ä–∏—Ç—å model config

```bash
curl http://localhost:8000/model-config | jq
```

**–û–∂–∏–¥–∞–µ–º–æ:**
```json
{
  "model_name": "openai/gpt-oss-20b",
  "provider": "vllm",
  "context_window": 8192,
  "summarization_threshold": 3000,
  "summarization_max_output": 1500
}
```

---

### –®–∞–≥ 4: –¢–µ—Å—Ç –ü–∞—Ç—Ç–µ—Ä–Ω–∞ 1 (–ø—Ä—è–º–∞—è —Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏—è)

```bash
# –ó–∞–≥—Ä—É–∑–∏—Ç—å –¥–æ–∫—É–º–µ–Ω—Ç
curl -X POST http://localhost:8000/ingest \
  -F "file=@test_doc.md" \
  -F "space_id=gpu_test"

# –ü–æ–ª—É—á–∏—Ç—å doc_id –∏–∑ –æ—Ç–≤–µ—Ç–∞, –∑–∞—Ç–µ–º:
curl -X POST http://localhost:8000/summarize \
  -H "Content-Type: application/json" \
  -d '{
    "doc_id": "...",
    "space_id": "gpu_test"
  }' | jq
```

**–û–∂–∏–¥–∞–µ–º–æ:**
- ‚úÖ Summary –≥–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç—Å—è
- ‚úÖ –ù–∞ —Ç–æ–º –∂–µ —è–∑—ã–∫–µ —á—Ç–æ –∏ –¥–æ–∫—É–º–µ–Ω—Ç
- ‚úÖ –°–∫–æ—Ä–æ—Å—Ç—å: 150-250 tok/s (vs 0.8-4.8 –Ω–∞ Ollama)
- ‚úÖ –õ–æ–≥–∏ –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç: `[LLM REQUEST ‚Üí vLLM]`
- ‚úÖ Timing breakdown —Ä–∞–±–æ—Ç–∞–µ—Ç

---

### –®–∞–≥ 5: –¢–µ—Å—Ç –ü–∞—Ç—Ç–µ—Ä–Ω–∞ 2-3 (/ask —Å —Ä–µ–∂–∏–º–∞–º–∏)

```bash
# Mode: auto (—Å –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–π —Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏–µ–π)
curl -X POST http://localhost:8000/ask \
  -H "Content-Type: application/json" \
  -d '{
    "q": "–†–∞—Å—Å–∫–∞–∂–∏ –ø–æ–¥—Ä–æ–±–Ω–æ –æ–±–æ –≤—Å–µ–º",
    "space_id": "gpu_test",
    "mode": "auto",
    "top_k": 30
  }' | jq '{summarized, context_tokens, mode}'

# Mode: summarize (–ø—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–∞—è)
curl -X POST http://localhost:8000/ask \
  -d '{"q":"–î–∞–π –∫—Ä–∞—Ç–∫–∏–π –æ–±–∑–æ—Ä","mode":"summarize"}' | jq '{summarized, mode}'
```

**–û–∂–∏–¥–∞–µ–º–æ:**
- ‚úÖ `summarized: true/false` –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç —Ä–µ–∂–∏–º–∞
- ‚úÖ –õ–æ–≥–∏: `[RAG CONTEXT]` ‚Üí `[RAG] Mode: ...` ‚Üí `[LLM REQUEST ‚Üí vLLM]`

---

### –®–∞–≥ 6: –¢–µ—Å—Ç –ü–∞—Ç—Ç–µ—Ä–Ω–∞ 4 (Pre-computed)

```bash
# –° –≥–µ–Ω–µ—Ä–∞—Ü–∏–µ–π summary
curl -X POST http://localhost:8000/ingest \
  -F "file=@large_doc.pdf" \
  -F "space_id=gpu_test" \
  -F "generate_summary=true"

# –ü–æ–¥–æ–∂–¥–∞—Ç—å background task (–ø—Ä–æ–≤–µ—Ä–∏—Ç—å –ª–æ–≥–∏)
docker compose -f docker-compose.vllm-mig.yml logs backend | grep Background

# –ü—Ä–æ–≤–µ—Ä–∏—Ç—å –∫—ç—à
curl -X POST http://localhost:8000/summarize \
  -d '{"doc_id":"...","space_id":"gpu_test"}' | jq '{cached}'
```

**–û–∂–∏–¥–∞–µ–º–æ:**
- ‚úÖ `summary_pending: true` –ø—Ä–∏ –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏
- ‚úÖ Background task –∑–∞–ø—É—Å–∫–∞–µ—Ç—Å—è
- ‚úÖ `cached: true` –ø—Ä–∏ –ø–æ–≤—Ç–æ—Ä–Ω–æ–º –∑–∞–ø—Ä–æ—Å–µ
- ‚úÖ –°–∫–æ—Ä–æ—Å—Ç—å —Å –∫—ç—à–µ–º: < 0.1s

---

### –®–∞–≥ 7: –¢–µ—Å—Ç –ü–∞—Ç—Ç–µ—Ä–Ω–∞ 5 (Streaming)

```bash
curl -N -X POST http://localhost:8000/summarize-stream \
  -d '{"doc_id":"...","space_id":"gpu_test"}'
```

**–û–∂–∏–¥–∞–µ–º–æ:**
- ‚úÖ –°–æ–±—ã—Ç–∏—è: start ‚Üí processing ‚Üí summary ‚Üí complete
- ‚úÖ –†—É—Å—Å–∫–∏–π —Ç–µ–∫—Å—Ç —á–∏—Ç–∞–µ–º—ã–π (–Ω–µ \uXXXX)
- ‚úÖ ETA –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç—Å—è
- ‚úÖ –°–∫–æ—Ä–æ—Å—Ç—å –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –Ω–∞–º–Ω–æ–≥–æ –≤—ã—à–µ

---

### –®–∞–≥ 8: –ü—Ä–æ–≤–µ—Ä–∏—Ç—å –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ

**–°–º–æ—Ç—Ä–µ—Ç—å –ª–æ–≥–∏ backend:**
```bash
docker compose -f docker-compose.vllm-mig.yml logs -f backend
```

**–û–∂–∏–¥–∞–µ–º–æ –≤ –ª–æ–≥–∞—Ö:**
```
üåê [40 –∑–≤–µ–∑–¥–æ—á–µ–∫]
[HTTP REQUEST ‚¨áÔ∏è ] POST /summarize
  ‚è∞ Request time: HH:MM:SS.mmm  ‚Üê –°–†–ê–ó–£ –ø–æ—è–≤–ª—è–µ—Ç—Å—è!

[Language Detection] Input text language: ru (–†—É—Å—Å–∫–∏–π)

[DYNAMIC MAX_TOKENS CALCULATION]
üìê Formula: available = context_window - prompt_tokens - safety_margin
  context_window  = 8,192
  prompt_tokens   =   X
  ...

[LLM REQUEST ‚Üí vLLM] Model: openai/gpt-oss-20b
üìù INPUT:
  - Prompt: X chars, ~Y tokens
  - Max output tokens: Z
üöÄ Sending request to vLLM...
  ‚è∞ LLM send time: HH:MM:SS.mmm

‚úÖ Response received from vLLM!
  ‚è∞ LLM receive time: HH:MM:SS.mmm

[LLM RESPONSE ‚Üê vLLM]
üì§ OUTPUT:
  - Response: A chars, ~B tokens
  - Generation time: C.Cs  ‚Üê –î–æ–ª–∂–Ω–æ –±—ã—Ç—å < 1s!
  - Speed: ~150-250 tokens/sec  ‚Üê –ù–∞–º–Ω–æ–≥–æ –±—ã—Å—Ç—Ä–µ–µ!

[HTTP RESPONSE ‚¨ÜÔ∏è ]
  ‚è±Ô∏è  Total HTTP time: X.Xs
```

---

## ‚ö†Ô∏è –ß—Ç–æ –¥–æ–±–∞–≤–∏—Ç—å –≤ vLLM MIG –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é

### –û–±–Ω–æ–≤–ª–µ–Ω–∏—è –Ω—É–∂–Ω—ã–µ:

<function_calls>
<invoke name="search_replace">
<parameter name="file_path">/Users/maksimkuksev/cursor/ai-assistant-mvp/docker-compose.vllm-mig.yml
