# Configuration for vLLM on NVIDIA RTX 6000 Blackwell 96GB
LLM_MODE=vllm
LLM_VLLM_URL=http://vllm:8001/v1
LLM_MODEL=meta-llama/Meta-Llama-3.1-8B-Instruct
LLM_TIMEOUT=120
LLM_MAX_TOKENS=512
LLM_TEMPERATURE=0.7

# vLLM Model
VLLM_MODEL=meta-llama/Meta-Llama-3.1-8B-Instruct

# Blackwell-optimized settings (increased from Ada defaults)
VLLM_GPU_MEMORY_UTILIZATION=0.95
VLLM_MAX_MODEL_LEN=16384
VLLM_TENSOR_PARALLEL_SIZE=1
VLLM_MAX_NUM_SEQS=512

# Blackwell specific optimizations
VLLM_ENABLE_FP8=false
VLLM_USE_FLASHINFER=true
VLLM_ENABLE_CHUNKED_PREFILL=true

# HuggingFace Token
HUGGING_FACE_HUB_TOKEN=your_token_here

# Qdrant & Backend
QDRANT_URL=http://qdrant:6333
EMBED_MODEL=sentence-transformers/all-MiniLM-L6-v2
