# Blackwell Quick Start –¥–ª—è RTX 6000 Blackwell 96GB

## üöÄ –û—Å–Ω–æ–≤–Ω—ã–µ –ø—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞ Blackwell

- ‚úÖ **2x –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å** vs Ada Lovelace
- ‚úÖ **FP8 precision** - –∑–∞–ø—É—Å–∫ Llama-70B –Ω–∞ –æ–¥–Ω–æ–π GPU!
- ‚úÖ **–î–æ 400 tokens/sec** –¥–ª—è Llama-8B  
- ‚úÖ **128K –∫–æ–Ω—Ç–µ–∫—Å—Ç** –±–µ–∑ –ø—Ä–æ–±–ª–µ–º
- ‚úÖ **–£–ª—É—á—à–µ–Ω–Ω—ã–µ Tensor Cores** –¥–ª—è transformers

## üìã –ß—Ç–æ –∏–∑–º–µ–Ω–∏–ª–æ—Å—å vs Ada

| –ü–∞—Ä–∞–º–µ—Ç—Ä | Ada (—Å—Ç–∞—Ä–∞—è –∫–æ–Ω—Ñ–∏–≥) | Blackwell (–Ω–æ–≤–∞—è) |
|----------|---------------------|-------------------|
| CUDA | 12.1 | **12.4+** ‚úÖ |
| vLLM | 0.4.2 | **0.6.2+** ‚úÖ |
| GPU Memory | 0.90 | **0.95** ‚úÖ |
| Max Context | 8K | **16K** ‚úÖ |
| Max Seqs | 256 | **512** ‚úÖ |
| FP8 Support | ‚ùå | **‚úÖ** |
| FlashInfer | ‚ùå | **‚úÖ** |

## üöÄ –ë—ã—Å—Ç—Ä—ã–π —Å—Ç–∞—Ä—Ç (Llama-8B)

```bash
# 1. –û–±–Ω–æ–≤–∏—Ç—å –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é
cp .env.vllm .env
nano .env  # –î–æ–±–∞–≤–∏—Ç—å HUGGING_FACE_HUB_TOKEN

# 2. –ó–∞–ø—É—Å—Ç–∏—Ç—å (–∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç Blackwell –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏)
make up-vllm

# 3. –ü—Ä–æ–≤–µ—Ä–∏—Ç—å
curl http://localhost:8000/health
```

## üéØ –ó–∞–ø—É—Å–∫ Llama-70B —Å FP8 (—Ç–æ–ª—å–∫–æ –Ω–∞ Blackwell!)

```bash
# 1. –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å FP8 –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é
cp .env.vllm-fp8 .env

# 2. –û–±–Ω–æ–≤–∏—Ç—å —Ç–æ–∫–µ–Ω
nano .env  # HUGGING_FACE_HUB_TOKEN=hf_xxxxx

# 3. –ó–∞–ø—É—Å—Ç–∏—Ç—å 70B –º–æ–¥–µ–ª—å!
make up-vllm

# –ú–æ–¥–µ–ª—å: neuralmagic/Meta-Llama-3.1-70B-Instruct-FP8
# VRAM: ~70GB (–≤–ª–µ–∑–∞–µ—Ç –≤ 96GB!)
# –°–∫–æ—Ä–æ—Å—Ç—å: 200-300 tokens/sec
```

## üìä –†–µ–∫–æ–º–µ–Ω–¥—É–µ–º—ã–µ –º–æ–¥–µ–ª–∏

### –ë–µ–∑ FP8 (FP16/BF16)
```bash
# Llama-8B - –º–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è —Å–∫–æ—Ä–æ—Å—Ç—å
VLLM_MODEL=meta-llama/Meta-Llama-3.1-8B-Instruct
# ‚Üí 300-400 tokens/sec, 16GB VRAM

# Mixtral-8x7B - –±–æ–ª—å—à–æ–π –∫–æ–Ω—Ç–µ–∫—Å—Ç
VLLM_MODEL=mistralai/Mixtral-8x7B-Instruct-v0.1  
# ‚Üí 200-280 tokens/sec, 48GB VRAM
```

### –° FP8 (—Ä–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è –¥–ª—è Blackwell)
```bash
# Llama-70B FP8 - –ª—É—á—à–∏–π –±–∞–ª–∞–Ω—Å
VLLM_MODEL=neuralmagic/Meta-Llama-3.1-70B-Instruct-FP8
# ‚Üí 200-300 tokens/sec, 70GB VRAM ‚úÖ

# Qwen-72B FP8 - –¥–ª–∏–Ω–Ω—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç
VLLM_MODEL=Qwen/Qwen2.5-72B-Instruct-FP8
# ‚Üí 180-250 tokens/sec, 72GB VRAM ‚úÖ
```

## ‚öôÔ∏è –í–∫–ª—é—á–∏—Ç—å –≤—Å–µ Blackwell –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏

```bash
# –í .env
VLLM_GPU_MEMORY_UTILIZATION=0.95    # –í—ã—à–µ —á–µ–º Ada
VLLM_MAX_MODEL_LEN=16384             # –ë–æ–ª—å—à–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç
VLLM_MAX_NUM_SEQS=512                # –ë–æ–ª—å—à–µ –±–∞—Ç—á–∏–Ω–≥
VLLM_ENABLE_FP8=true                 # FP8 precision
VLLM_USE_FLASHINFER=true             # FlashInfer attention
VLLM_ENABLE_CHUNKED_PREFILL=true     # Chunked prefill
```

## üîç –ü—Ä–æ–≤–µ—Ä–∫–∞ —Å–∏—Å—Ç–µ–º—ã

```bash
# CUDA –≤–µ—Ä—Å–∏—è (–¥–æ–ª–∂–Ω–∞ –±—ã—Ç—å 12.4+)
nvidia-smi

# Compute capability (–¥–æ–ª–∂–Ω–∞ –±—ã—Ç—å 10.0 –¥–ª—è Blackwell)
nvidia-smi --query-gpu=compute_cap --format=csv

# –î—Ä–∞–π–≤–µ—Ä (–¥–æ–ª–∂–µ–Ω –±—ã—Ç—å 550+)
nvidia-smi --query-gpu=driver_version --format=csv
```

## üìö –ü–æ–ª–Ω–∞—è –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è

- [docs/BLACKWELL_OPTIMIZATIONS.md](docs/BLACKWELL_OPTIMIZATIONS.md) - –í—Å–µ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏
- [docs/VLLM_DEPLOYMENT.md](docs/VLLM_DEPLOYMENT.md) - vLLM —Ä–∞–∑–≤–µ—Ä—Ç—ã–≤–∞–Ω–∏–µ
- [MIG_QUICKSTART.md](MIG_QUICKSTART.md) - MIG –¥–ª—è Blackwell

## üí° –°–æ–≤–µ—Ç

**–î–ª—è –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–π –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏:**
1. –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ FP8 –º–æ–¥–µ–ª–∏ (`.env.vllm-fp8`)
2. –í–∫–ª—é—á–∏—Ç–µ FlashInfer (`VLLM_USE_FLASHINFER=true`)
3. –£–≤–µ–ª–∏—á—å—Ç–µ batch size (`VLLM_MAX_NUM_SEQS=512`)

Blackwell –≤ 2x –±—ã—Å—Ç—Ä–µ–µ Ada Lovelace! üöÄ

