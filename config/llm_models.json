{
  "models": [
    {
      "model_name": "llama3.1:8b",
      "provider": "ollama",
      "context_window": 8192,
      "max_output_tokens": 512,
      "summarization_threshold": 3000,
      "summarization_max_output": 1500,
      "tokens_per_second": 50,
      "supports_streaming": true,
      "supports_function_calling": false,
      "description": "Llama 3.1 8B via Ollama - balanced performance for general use",
      "recommended_use_cases": ["general", "chat", "rag", "small_documents"]
    },
    {
      "model_name": "llama3.1:70b",
      "provider": "ollama",
      "context_window": 8192,
      "max_output_tokens": 512,
      "summarization_threshold": 3000,
      "summarization_max_output": 1500,
      "tokens_per_second": 15,
      "supports_streaming": true,
      "supports_function_calling": false,
      "description": "Llama 3.1 70B via Ollama - high quality reasoning",
      "recommended_use_cases": ["complex_reasoning", "analysis", "long_form"]
    },
    {
      "model_name": "meta-llama/Meta-Llama-3.1-8B-Instruct",
      "provider": "vllm",
      "context_window": 8192,
      "max_output_tokens": 512,
      "summarization_threshold": 3000,
      "summarization_max_output": 1500,
      "tokens_per_second": 250,
      "supports_streaming": true,
      "supports_function_calling": true,
      "description": "Llama 3.1 8B via vLLM - GPU optimized, very fast",
      "recommended_use_cases": ["high_throughput", "real_time", "rag"]
    },
    {
      "model_name": "neuralmagic/Meta-Llama-3.1-70B-Instruct-FP8",
      "provider": "vllm",
      "context_window": 16384,
      "max_output_tokens": 1024,
      "summarization_threshold": 6000,
      "summarization_max_output": 3000,
      "tokens_per_second": 200,
      "supports_streaming": true,
      "supports_function_calling": true,
      "description": "Llama 3.1 70B FP8 on Blackwell - large context, high quality",
      "recommended_use_cases": ["large_documents", "complex_reasoning", "multi_document"]
    },
    {
      "model_name": "mistralai/Mixtral-8x7B-Instruct-v0.1",
      "provider": "vllm",
      "context_window": 32768,
      "max_output_tokens": 1024,
      "summarization_threshold": 12000,
      "summarization_max_output": 4000,
      "tokens_per_second": 180,
      "supports_streaming": true,
      "supports_function_calling": true,
      "description": "Mixtral 8x7B - very large context window",
      "recommended_use_cases": ["very_large_documents", "multi_document", "long_context"]
    },
    {
      "model_name": "mistral:7b",
      "provider": "ollama",
      "context_window": 8192,
      "max_output_tokens": 512,
      "summarization_threshold": 3000,
      "summarization_max_output": 1500,
      "tokens_per_second": 60,
      "supports_streaming": true,
      "supports_function_calling": false,
      "description": "Mistral 7B - fast inference",
      "recommended_use_cases": ["speed", "simple_queries"]
    },
    {
      "model_name": "qwen2.5:7b",
      "provider": "ollama",
      "context_window": 32768,
      "max_output_tokens": 1024,
      "summarization_threshold": 12000,
      "summarization_max_output": 4000,
      "tokens_per_second": 55,
      "supports_streaming": true,
      "supports_function_calling": false,
      "description": "Qwen 2.5 7B - large context window",
      "recommended_use_cases": ["long_documents", "multi_turn_conversations"]
    },
    {
      "model_name": "gpt-4",
      "provider": "openai",
      "context_window": 8192,
      "max_output_tokens": 1024,
      "summarization_threshold": 3000,
      "summarization_max_output": 1500,
      "tokens_per_second": 40,
      "supports_streaming": true,
      "supports_function_calling": true,
      "cost_per_1k_input_tokens": 0.03,
      "cost_per_1k_output_tokens": 0.06,
      "description": "GPT-4 - highest quality (cloud)",
      "recommended_use_cases": ["production", "critical_tasks", "function_calling"]
    },
    {
      "model_name": "gpt-4-turbo",
      "provider": "openai",
      "context_window": 128000,
      "max_output_tokens": 4096,
      "summarization_threshold": 50000,
      "summarization_max_output": 10000,
      "tokens_per_second": 100,
      "supports_streaming": true,
      "supports_function_calling": true,
      "cost_per_1k_input_tokens": 0.01,
      "cost_per_1k_output_tokens": 0.03,
      "description": "GPT-4 Turbo - massive context (cloud)",
      "recommended_use_cases": ["very_large_documents", "multi_document_analysis"]
    },
    {
      "model_name": "claude-3-sonnet",
      "provider": "anthropic",
      "context_window": 200000,
      "max_output_tokens": 4096,
      "summarization_threshold": 80000,
      "summarization_max_output": 15000,
      "tokens_per_second": 80,
      "supports_streaming": true,
      "supports_function_calling": true,
      "cost_per_1k_input_tokens": 0.003,
      "cost_per_1k_output_tokens": 0.015,
      "description": "Claude 3 Sonnet - largest context (cloud)",
      "recommended_use_cases": ["massive_documents", "code_analysis", "book_summarization"]
    }
  ]
}

