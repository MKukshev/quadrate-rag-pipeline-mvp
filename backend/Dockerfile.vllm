# Dockerfile for vLLM service
# Optimized for NVIDIA RTX 6000 Blackwell (96GB)
# Blackwell architecture requires CUDA 12.4+ for optimal performance
FROM nvidia/cuda:12.4.1-runtime-ubuntu22.04

# Установка Python и зависимостей
RUN apt-get update && apt-get install -y \
    python3.11 \
    python3-pip \
    python3.11-venv \
    curl \
    git \
    && rm -rf /var/lib/apt/lists/*

# Установка python3.11 как default
RUN update-alternatives --install /usr/bin/python3 python3 /usr/bin/python3.11 1

# Обновление pip
RUN pip3 install --upgrade pip setuptools wheel

# Установка vLLM и зависимостей
# vLLM 0.6.0+ включает оптимизации для Blackwell
RUN pip3 install vllm==0.6.2 \
    fastapi>=0.112 \
    uvicorn[standard]>=0.30 \
    pydantic>=2.0 \
    ray>=2.9.0

WORKDIR /app

# Копирование vLLM сервиса
COPY backend/vllm_service.py /app/vllm_service.py

# Порт для vLLM API
EXPOSE 8001

# Переменные окружения
ENV VLLM_MODEL="meta-llama/Llama-2-7b-chat-hf"
ENV VLLM_GPU_MEMORY_UTILIZATION=0.9
ENV VLLM_MAX_MODEL_LEN=4096
ENV VLLM_TENSOR_PARALLEL_SIZE=1

# Запуск vLLM сервера
CMD python3 -m vllm.entrypoints.openai.api_server \
    --model ${VLLM_MODEL} \
    --host 0.0.0.0 \
    --port 8001 \
    --gpu-memory-utilization ${VLLM_GPU_MEMORY_UTILIZATION} \
    --max-model-len ${VLLM_MAX_MODEL_LEN} \
    --tensor-parallel-size ${VLLM_TENSOR_PARALLEL_SIZE}

