# Dockerfile for vLLM service and Backend
# Optimized for NVIDIA RTX 6000 Blackwell (96GB) with CUDA 13.0
# Blackwell architecture (compute capability 10.0) requires CUDA 12.8+ for native support
FROM nvidia/cuda:13.0.1-devel-ubuntu24.04

# Установка Python и зависимостей
RUN apt-get update && apt-get install -y --no-install-recommends \
    python3.12 \
    python3-pip \
    python3.12-venv \
    python3.12-dev \
    build-essential \
    libglib2.0-0 \
    libgl1 \
    libxml2 \
    libxslt1.1 \
    libjpeg-turbo8 \
    curl \
    git \
    && rm -rf /var/lib/apt/lists/*

# Создание и активация виртуального окружения
RUN python3.12 -m venv /opt/venv
ENV VIRTUAL_ENV=/opt/venv
ENV PATH="/opt/venv/bin:${PATH}"

# Обновление pip
RUN python -m pip install --upgrade pip setuptools wheel

# Установка PyTorch 2.9.0 с поддержкой CUDA 13.0 (cu130)
ENV TORCH_CUDA_ARCH_LIST="8.0;8.6;8.9;9.0;10.0"
ENV CUDA_HOME=/usr/local/cuda

RUN python -m pip install --no-cache-dir \
    --extra-index-url https://download.pytorch.org/whl/cu130 \
    torch==2.9.0+cu130 \
    torchvision==0.24.0+cu130 \
    torchaudio==2.9.0+cu130

# Установка vLLM (0.11.0 с поддержкой MXFP4) и FastAPI для LLM service
RUN python -m pip install --no-cache-dir \
    vllm==0.11.0 \
    fastapi>=0.112 \
    uvicorn[standard]>=0.30 \
    pydantic>=2.0 \
    ray>=2.9.0 \
    xformers

# Установка Backend зависимостей
COPY backend/requirements.txt ./requirements.txt
RUN python -m pip install --no-cache-dir -r requirements.txt

WORKDIR /app
ENV PYTHONPATH=/app/backend:/app

# Копируем весь проект
COPY . /app

# Порт для vLLM API
EXPOSE 8001

# Переменные окружения для vLLM
ENV VLLM_MODEL="meta-llama/Llama-2-7b-chat-hf"
ENV VLLM_GPU_MEMORY_UTILIZATION=0.9
ENV VLLM_MAX_MODEL_LEN=4096
ENV VLLM_TENSOR_PARALLEL_SIZE=1

# По умолчанию запускаем vLLM сервер
# Для backend контейнера используется command override в docker-compose
CMD python3 -m vllm.entrypoints.openai.api_server \
    --model ${VLLM_MODEL} \
    --host 0.0.0.0 \
    --port 8001 \
    --gpu-memory-utilization ${VLLM_GPU_MEMORY_UTILIZATION} \
    --max-model-len ${VLLM_MAX_MODEL_LEN} \
    --tensor-parallel-size ${VLLM_TENSOR_PARALLEL_SIZE}
