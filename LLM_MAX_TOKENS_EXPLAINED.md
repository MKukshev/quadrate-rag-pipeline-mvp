# LLM_MAX_TOKENS - –û–±—ä—è—Å–Ω–µ–Ω–∏–µ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞

## üìã TL;DR

**`LLM_MAX_TOKENS`** - —ç—Ç–æ –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–æ–∫–µ–Ω–æ–≤ –≤ **–û–¢–í–ï–¢–ï** –º–æ–¥–µ–ª–∏ (output).

**–ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –≤ –û–ë–ï–ò–• –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è—Ö: Ollama –∏ vLLM!**

---

## üîç –ì–¥–µ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è

### 1. –í config.py (–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ)

```python
# backend/services/config.py
LLM_MAX_TOKENS = int(os.getenv("LLM_MAX_TOKENS", "256"))
```

**Default:** 256 —Ç–æ–∫–µ–Ω–æ–≤ (–æ—á–µ–Ω—å –º–∞–ª–æ!)

---

### 2. –î–ª—è Ollama (rag.py)

```python
# backend/services/rag.py

def call_llm(prompt: str) -> str:
    if LLM_MODE == "ollama":
        r = requests.post(
            "http://ollama:11434/api/generate",
            json={
                "model": LLM_MODEL,
                "prompt": prompt,
                "options": {
                    "num_predict": LLM_MAX_TOKENS,  # ‚Üê –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –∑–¥–µ—Å—å!
                    "num_ctx": OLLAMA_NUM_CTX,
                },
            },
            ...
        )
```

**–ü–∞—Ä–∞–º–µ—Ç—Ä Ollama API:**
- **`num_predict`** - –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–æ–∫–µ–Ω–æ–≤ –≤ –≥–µ–Ω–µ—Ä–∏—Ä—É–µ–º–æ–º –æ—Ç–≤–µ—Ç–µ
- –ê–Ω–∞–ª–æ–≥ `max_tokens` –≤ –¥—Ä—É–≥–∏—Ö API

---

### 3. –î–ª—è vLLM (rag_vllm.py)

```python
# backend/services/rag_vllm.py

def call_vllm(prompt: str, model: Optional[str] = None, **kwargs) -> str:
    payload = {
        "model": model,
        "messages": [{"role": "user", "content": prompt}],
        "max_tokens": kwargs.get("max_tokens", config.LLM_MAX_TOKENS),  # ‚Üê –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –∑–¥–µ—Å—å!
        "temperature": ...,
    }
    
    response = requests.post(f"{VLLM_URL}/chat/completions", json=payload)
```

**–ü–∞—Ä–∞–º–µ—Ç—Ä vLLM OpenAI-compatible API:**
- **`max_tokens`** - –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–æ–∫–µ–Ω–æ–≤ –≤ –æ—Ç–≤–µ—Ç–µ
- –°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–π –ø–∞—Ä–∞–º–µ—Ç—Ä OpenAI API

---

## üìä –î–≤–∞ —Ä–∞–∑–Ω—ã—Ö –ø–∞—Ä–∞–º–µ—Ç—Ä–∞!

### –ù–ï –ü–£–¢–ê–¢–¨:

| –ß—Ç–æ | –ü–∞—Ä–∞–º–µ—Ç—Ä | –î–ª—è —á–µ–≥–æ |
|-----|----------|----------|
| **–†–∞–∑–º–µ—Ä –æ—Ç–≤–µ—Ç–∞ (output)** | `LLM_MAX_TOKENS` | –ú–∞–∫—Å–∏–º—É–º —Ç–æ–∫–µ–Ω–æ–≤ –≤ –æ—Ç–≤–µ—Ç–µ LLM |
| **–†–∞–∑–º–µ—Ä –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ (input)** | `OLLAMA_NUM_CTX` –∏–ª–∏ `VLLM_MAX_MODEL_LEN` | –†–∞–∑–º–µ—Ä –∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω–æ–≥–æ –æ–∫–Ω–∞ |

### –î–ª—è Ollama:

```yaml
# docker-compose.yml
environment:
  - LLM_MODEL=llama3.1:8b
  - OLLAMA_NUM_CTX=40960      # ‚Üê INPUT: –ö–æ–Ω—Ç–µ–∫—Å—Ç–Ω–æ–µ –æ–∫–Ω–æ (—á—Ç–æ –º–æ–∂–µ–º –æ—Ç–ø—Ä–∞–≤–∏—Ç—å)
  - LLM_MAX_TOKENS=2048        # ‚Üê OUTPUT: –ú–∞–∫—Å–∏–º—É–º –≤ –æ—Ç–≤–µ—Ç–µ (—á—Ç–æ –ø–æ–ª—É—á–∏–º)
```

**–í API –∑–∞–ø—Ä–æ—Å–µ:**
```json
{
  "options": {
    "num_ctx": 40960,      // –†–∞–∑–º–µ—Ä –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ (input)
    "num_predict": 2048    // –†–∞–∑–º–µ—Ä –æ—Ç–≤–µ—Ç–∞ (output)
  }
}
```

### –î–ª—è vLLM:

```yaml
# docker-compose.vllm-mig.yml
vllm-medium:
  environment:
    - VLLM_MODEL=openai/gpt-oss-20b
    - VLLM_MAX_MODEL_LEN=8192  # ‚Üê INPUT: –ö–æ–Ω—Ç–µ–∫—Å—Ç–Ω–æ–µ –æ–∫–Ω–æ —Å–µ—Ä–≤–µ—Ä–∞

backend:
  environment:
    - LLM_MAX_TOKENS=512       # ‚Üê OUTPUT: –ú–∞–∫—Å–∏–º—É–º –≤ –æ—Ç–≤–µ—Ç–µ
```

**–í API –∑–∞–ø—Ä–æ—Å–µ:**
```json
{
  "max_tokens": 512      // –†–∞–∑–º–µ—Ä –æ—Ç–≤–µ—Ç–∞ (output)
}
```

---

## üéØ –ó–∞—á–µ–º –æ–≥—Ä–∞–Ω–∏—á–∏–≤–∞—Ç—å –æ—Ç–≤–µ—Ç?

### –ü—Ä–∏—á–∏–Ω—ã –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è LLM_MAX_TOKENS:

1. **–ö–æ–Ω—Ç—Ä–æ–ª—å —Å—Ç–æ–∏–º–æ—Å—Ç–∏** (–¥–ª—è –æ–±–ª–∞—á–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π)
   - –ú–µ–Ω—å—à–µ —Ç–æ–∫–µ–Ω–æ–≤ = –º–µ–Ω—å—à–µ –ø–ª–∞—Ç–∏–º

2. **–°–∫–æ—Ä–æ—Å—Ç—å –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏**
   - 256 —Ç–æ–∫–µ–Ω–æ–≤ –≥–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç—Å—è –±—ã—Å—Ç—Ä–µ–µ —á–µ–º 2048

3. **–ö—Ä–∞—Ç–∫–∏–µ –æ—Ç–≤–µ—Ç—ã**
   - –î–ª—è RAG –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ 256-512 —Ç–æ–∫–µ–Ω–æ–≤
   - –°—É–º–º–∞—Ä–∏–∑–∞—Ü–∏—è —Ç—Ä–µ–±—É–µ—Ç –±–æ–ª—å—à–µ (1000-2048)

4. **–ü—Ä–µ–¥—Å–∫–∞–∑—É–µ–º–æ—Å—Ç—å**
   - –ú–æ–¥–µ–ª—å –Ω–µ –±—É–¥–µ—Ç –≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å —Å–ª–∏—à–∫–æ–º –¥–ª–∏–Ω–Ω—ã–µ –æ—Ç–≤–µ—Ç—ã

---

## ‚öôÔ∏è –†–µ–∫–æ–º–µ–Ω–¥—É–µ–º—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è

### –î–ª—è RAG (/ask endpoint):

```bash
LLM_MAX_TOKENS=512   # –î–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–ª—è –±–æ–ª—å—à–∏–Ω—Å—Ç–≤–∞ –æ—Ç–≤–µ—Ç–æ–≤
```

### –î–ª—è –°—É–º–º–∞—Ä–∏–∑–∞—Ü–∏–∏ (/summarize):

```bash
LLM_MAX_TOKENS=2048  # –ù—É–∂–Ω–æ –±–æ–ª—å—à–µ –¥–ª—è –ø–æ–ª–Ω–æ–≥–æ summary
```

### –î–ª—è –¥–ª–∏–Ω–Ω—ã—Ö –æ—Ç–≤–µ—Ç–æ–≤:

```bash
LLM_MAX_TOKENS=4096  # –î–ª—è —Ä–∞–∑–≤—ë—Ä–Ω—É—Ç—ã—Ö –æ–±—ä—è—Å–Ω–µ–Ω–∏–π
```

---

## üîß –ü—Ä–æ–±–ª–µ–º–∞ –≤ —Ç–µ–∫—É—â–µ–π –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏

### –ë—ã–ª–æ:

```yaml
# docker-compose.yml (—Å—Ç–∞—Ä–æ–µ)
- LLM_TIMEOUT=240
- CHUNK_TOKENS=500
- CHUNK_OVERLAP=50

# config.py (default)
LLM_MAX_TOKENS = 256  # ‚Üê –°–ª–∏—à–∫–æ–º –º–∞–ª–æ!
```

**–ü—Ä–æ–±–ª–µ–º–∞:** 256 —Ç–æ–∫–µ–Ω–æ–≤ –Ω–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–ª—è summary!

### –ò—Å–ø—Ä–∞–≤–ª–µ–Ω–æ:

```yaml
# docker-compose.yml (–Ω–æ–≤–æ–µ)
- LLM_MAX_TOKENS=${LLM_MAX_TOKENS:-2048}  # ‚Üê –¢–µ–ø–µ—Ä—å 2048
- LLM_TIMEOUT=300
```

---

## üìñ –ü—Ä–∏–º–µ—Ä—ã

### –ü—Ä–∏–º–µ—Ä 1: RAG –æ—Ç–≤–µ—Ç (256 —Ç–æ–∫–µ–Ω–æ–≤)

**–ó–∞–ø—Ä–æ—Å:**
```bash
curl -X POST /ask -d '{"q":"–ö–∞–∫–æ–π –±—é–¥–∂–µ—Ç –ø—Ä–æ–µ–∫—Ç–∞?"}'
```

**–û—Ç–≤–µ—Ç (256 —Ç–æ–∫–µ–Ω–æ–≤):**
```
Based on the context, Project Alpha has a budget of $200,000.
This budget was established at the start of the project on March 1, 2025.
```

**–†–∞–∑–º–µ—Ä:** ~30 —Ç–æ–∫–µ–Ω–æ–≤ (–≤ –ø—Ä–µ–¥–µ–ª–∞—Ö –ª–∏–º–∏—Ç–∞ 256)

### –ü—Ä–∏–º–µ—Ä 2: –°—É–º–º–∞—Ä–∏–∑–∞—Ü–∏—è (–Ω—É–∂–Ω–æ > 256)

**–ó–∞–ø—Ä–æ—Å:**
```bash
curl -X POST /summarize -d '{"doc_id":"..."}'
```

**–û—Ç–≤–µ—Ç (–Ω—É–∂–Ω–æ ~500-1000 —Ç–æ–∫–µ–Ω–æ–≤):**
```
**Summary of Project Alpha**

Project Alpha, initiated on 1 March 2025, is a $200,000 initiative...
[–ø–æ–ª–Ω—ã–π summary –ø—Ä–æ–¥–æ–ª–∂–∞–µ—Ç—Å—è...]

**Team**
The project involves 5 engineers: Alice (team lead), Bob (backend)...

**Technologies**
Backend: Python + FastAPI
Frontend: React + TypeScript
...
```

**–†–∞–∑–º–µ—Ä:** ~500-1000 —Ç–æ–∫–µ–Ω–æ–≤

**–° –ª–∏–º–∏—Ç–æ–º 256:** –û–±—Ä–µ–∂–µ—Ç—Å—è –Ω–∞ —Å–µ—Ä–µ–¥–∏–Ω–µ! ‚ùå  
**–° –ª–∏–º–∏—Ç–æ–º 2048:** –ü–æ–ª–Ω—ã–π –æ—Ç–≤–µ—Ç ‚úÖ

---

## üîÑ –°—Ä–∞–≤–Ω–µ–Ω–∏–µ Ollama vs vLLM

| –ê—Å–ø–µ–∫—Ç | Ollama | vLLM |
|--------|--------|------|
| **API –ø–∞—Ä–∞–º–µ—Ç—Ä –¥–ª—è max output** | `num_predict` | `max_tokens` |
| **Backend –ø–µ—Ä–µ–º–µ–Ω–Ω–∞—è** | `LLM_MAX_TOKENS` | `LLM_MAX_TOKENS` |
| **–ì–¥–µ —É—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ—Ç—Å—è** | –ü—Ä–∏ –∫–∞–∂–¥–æ–º –≤—ã–∑–æ–≤–µ API | –ü—Ä–∏ –∫–∞–∂–¥–æ–º –≤—ã–∑–æ–≤–µ API |
| **Default –≤ config.py** | 256 | 256 |
| **–†–µ–∫–æ–º–µ–Ω–¥—É–µ–º–æ–µ –¥–ª—è summary** | 2048 | 2048 |

---

## ‚úÖ –ò—Ç–æ–≥–æ–≤–∞—è –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è

### –î–ª—è Ollama

```yaml
# docker-compose.yml
environment:
  - LLM_MODEL=llama3.1:8b
  
  # INPUT parameters (—á—Ç–æ –º–æ–∂–µ–º –æ—Ç–ø—Ä–∞–≤–∏—Ç—å)
  - OLLAMA_NUM_CTX=8192         # –†–∞–∑–º–µ—Ä –∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω–æ–≥–æ –æ–∫–Ω–∞
  
  # OUTPUT parameters (—á—Ç–æ –ø–æ–ª—É—á–∏–º)
  - LLM_MAX_TOKENS=2048         # –ú–∞–∫—Å–∏–º—É–º —Ç–æ–∫–µ–Ω–æ–≤ –≤ –æ—Ç–≤–µ—Ç–µ
  - LLM_TIMEOUT=300             # –¢–∞–π–º–∞—É—Ç –Ω–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏—é
```

### –î–ª—è vLLM

```yaml
# docker-compose.vllm-mig.yml

# vLLM server
vllm-medium:
  environment:
    - VLLM_MODEL=openai/gpt-oss-20b
    - VLLM_MAX_MODEL_LEN=8192   # INPUT: –†–∞–∑–º–µ—Ä –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ —Å–µ—Ä–≤–µ—Ä–∞

# Backend
backend:
  environment:
    - LLM_MODE=vllm
    - LLM_MAX_TOKENS=512        # OUTPUT: –ú–∞–∫—Å–∏–º—É–º –≤ –æ—Ç–≤–µ—Ç–µ
    - LLM_TIMEOUT=120
```

---

## üéØ –í—ã–≤–æ–¥

**`LLM_MAX_TOKENS` - —ç—Ç–æ –ø–∞—Ä–∞–º–µ—Ç—Ä OUTPUT (–æ—Ç–≤–µ—Ç–∞ –º–æ–¥–µ–ª–∏), –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –û–î–ò–ù–ê–ö–û–í–û –¥–ª—è Ollama –∏ vLLM!**

**–î–ª—è –∫–æ–Ω—Ç—Ä–æ–ª—è INPUT (–∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω–æ–≥–æ –æ–∫–Ω–∞) –∏—Å–ø–æ–ª—å–∑—É—é—Ç—Å—è —Ä–∞–∑–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã:**
- Ollama: `OLLAMA_NUM_CTX`
- vLLM: `VLLM_MAX_MODEL_LEN`

**–î–ª—è —Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏–∏ –Ω—É–∂–Ω–æ —É–≤–µ–ª–∏—á–∏—Ç—å `LLM_MAX_TOKENS` –¥–æ 2048!** ‚úÖ

