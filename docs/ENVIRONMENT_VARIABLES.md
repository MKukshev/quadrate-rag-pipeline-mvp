# –ü–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –æ–∫—Ä—É–∂–µ–Ω–∏—è - –ü–æ–ª–Ω—ã–π —Å–ø—Ä–∞–≤–æ—á–Ω–∏–∫

## üéØ LLM –ü—Ä–æ–≤–∞–π–¥–µ—Ä (Backend)

### `LLM_MODE`
**–ù–∞–∑–Ω–∞—á–µ–Ω–∏–µ:** –í—ã–±–æ—Ä LLM –ø—Ä–æ–≤–∞–π–¥–µ—Ä–∞  
**–ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –≤:** Backend (Python)  
**–ó–Ω–∞—á–µ–Ω–∏—è:**
- `ollama` - –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å Ollama (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é)
- `vllm` - –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å vLLM
- `none` - –æ—Ç–∫–ª—é—á–∏—Ç—å LLM (—Ç–æ–ª—å–∫–æ –ø–æ–∏—Å–∫)

**–ü—Ä–∏–º–µ—Ä:**
```bash
LLM_MODE=vllm  # –ü–µ—Ä–µ–∫–ª—é—á–µ–Ω–∏–µ –Ω–∞ vLLM
```

---

## üì¶ LLM –ú–æ–¥–µ–ª—å

### ‚ö†Ô∏è –í–∞–∂–Ω–æ: –î–≤–µ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –¥–ª—è –º–æ–¥–µ–ª–∏

#### `LLM_MODEL` (Backend)
**–ù–∞–∑–Ω–∞—á–µ–Ω–∏–µ:** –ò–º—è –º–æ–¥–µ–ª–∏ –¥–ª—è backend –ª–æ–≥–∏–∫–∏  
**–ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –≤:** 
- Backend Python –∫–æ–¥
- –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ
- –ú–µ—Ç—Ä–∏–∫–∏
- API responses

**–§–æ—Ä–º–∞—Ç:** –ú–æ–∂–µ—Ç –±—ã—Ç—å –ª—é–±–æ–π —Å—Ç—Ä–æ–∫–æ–π  
**–ü—Ä–∏–º–µ—Ä—ã:**
```bash
# Ollama
LLM_MODEL=llama3.1:8b

# vLLM
LLM_MODEL=meta-llama/Meta-Llama-3.1-8B-Instruct
```

#### `VLLM_MODEL` (vLLM Container)
**–ù–∞–∑–Ω–∞—á–µ–Ω–∏–µ:** –ò–º—è –º–æ–¥–µ–ª–∏ –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏ –≤ vLLM  
**–ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –≤:** vLLM —Å–µ—Ä–≤–∏—Å (–æ—Ç–¥–µ–ª—å–Ω—ã–π –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä)  
**–§–æ—Ä–º–∞—Ç:** HuggingFace model ID  
**–ü—Ä–∏–º–µ—Ä—ã:**
```bash
VLLM_MODEL=meta-llama/Meta-Llama-3.1-8B-Instruct
VLLM_MODEL=neuralmagic/Meta-Llama-3.1-70B-Instruct-FP8
VLLM_MODEL=mistralai/Mixtral-8x7B-Instruct-v0.1
```

### üîó –°–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∞—Ü–∏—è LLM_MODEL –∏ VLLM_MODEL

**–ü—Ä–∞–≤–∏–ª–æ:** –ü—Ä–∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–∏ vLLM, `LLM_MODEL` –∏ `VLLM_MODEL` –¥–æ–ª–∂–Ω—ã –±—ã—Ç—å –æ–¥–∏–Ω–∞–∫–æ–≤—ã–º–∏!

#### ‚úÖ –ü—Ä–∞–≤–∏–ª—å–Ω–æ:
```bash
LLM_MODE=vllm
LLM_MODEL=meta-llama/Meta-Llama-3.1-8B-Instruct
VLLM_MODEL=meta-llama/Meta-Llama-3.1-8B-Instruct
```

#### ‚ùå –ù–µ–ø—Ä–∞–≤–∏–ª—å–Ω–æ:
```bash
LLM_MODE=vllm
LLM_MODEL=llama3.1:8b                                    # ‚Üê Ollama —Ñ–æ—Ä–º–∞—Ç
VLLM_MODEL=meta-llama/Meta-Llama-3.1-8B-Instruct       # ‚Üê HF —Ñ–æ—Ä–º–∞—Ç
# Backend –¥—É–º–∞–µ—Ç —á—Ç–æ –º–æ–¥–µ–ª—å llama3.1:8b, –Ω–æ vLLM –∑–∞–≥—Ä—É–∂–∞–µ—Ç –¥—Ä—É–≥—É—é!
```

---

## üîß –ü–æ—á–µ–º—É –¥–≤–µ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ?

### –ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–Ω–∞—è –ø—Ä–∏—á–∏–Ω–∞

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  Backend Container                   ‚îÇ
‚îÇ                                      ‚îÇ
‚îÇ  - –ß–∏—Ç–∞–µ—Ç LLM_MODE                  ‚îÇ
‚îÇ  - –ò—Å–ø–æ–ª—å–∑—É–µ—Ç LLM_MODEL             ‚îÇ
‚îÇ  - –û—Ç–ø—Ä–∞–≤–ª—è–µ—Ç –∑–∞–ø—Ä–æ—Å—ã –∫ vLLM        ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                 ‚îÇ
                 ‚îÇ HTTP Request
                 ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  vLLM Container                      ‚îÇ
‚îÇ                                      ‚îÇ
‚îÇ  - –ß–∏—Ç–∞–µ—Ç VLLM_MODEL                ‚îÇ
‚îÇ  - –ó–∞–≥—Ä—É–∂–∞–µ—Ç –º–æ–¥–µ–ª—å –∏–∑ HuggingFace  ‚îÇ
‚îÇ  - –ó–∞–ø—É—Å–∫–∞–µ—Ç vLLM —Å–µ—Ä–≤–µ—Ä            ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

**–†–∞–∑–¥–µ–ª–µ–Ω–∏–µ:**
- Backend –Ω–µ –∑–Ω–∞–µ—Ç –¥–µ—Ç–∞–ª–µ–π vLLM
- vLLM –Ω–µ –∑–Ω–∞–µ—Ç –æ backend
- –ö–∞–∂–¥—ã–π –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä –Ω–µ–∑–∞–≤–∏—Å–∏–º

### –ò—Å—Ç–æ—Ä–∏—á–µ—Å–∫–∞—è –ø—Ä–∏—á–∏–Ω–∞

–ò–∑–Ω–∞—á–∞–ª—å–Ω–æ –±—ã–ª–∞ —Ç–æ–ª—å–∫–æ –æ–¥–Ω–∞ –ø–µ—Ä–µ–º–µ–Ω–Ω–∞—è `LLM_MODEL` –¥–ª—è Ollama:
```bash
LLM_MODEL=llama3.1:8b  # Ollama —Ñ–æ—Ä–º–∞—Ç
```

–ü—Ä–∏ –¥–æ–±–∞–≤–ª–µ–Ω–∏–∏ vLLM –ø–æ–Ω–∞–¥–æ–±–∏–ª–∞—Å—å –æ—Ç–¥–µ–ª—å–Ω–∞—è –ø–µ—Ä–µ–º–µ–Ω–Ω–∞—è:
```bash
VLLM_MODEL=meta-llama/Meta-Llama-3.1-8B-Instruct  # HF —Ñ–æ—Ä–º–∞—Ç
```

---

## üìã –ü—Ä–∏–º–µ—Ä—ã –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–π

### –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è 1: Ollama
```bash
LLM_MODE=ollama
LLM_MODEL=llama3.1:8b
# VLLM_MODEL –Ω–µ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è
```

### –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è 2: vLLM (Llama-8B)
```bash
LLM_MODE=vllm
LLM_MODEL=meta-llama/Meta-Llama-3.1-8B-Instruct
VLLM_MODEL=meta-llama/Meta-Llama-3.1-8B-Instruct
```

### –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è 3: vLLM (Llama-70B FP8)
```bash
LLM_MODE=vllm
LLM_MODEL=neuralmagic/Meta-Llama-3.1-70B-Instruct-FP8
VLLM_MODEL=neuralmagic/Meta-Llama-3.1-70B-Instruct-FP8
```

### –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è 4: –ë–µ–∑ LLM
```bash
LLM_MODE=none
# LLM_MODEL –∏ VLLM_MODEL –∏–≥–Ω–æ—Ä–∏—Ä—É—é—Ç—Å—è
```

---

## üîÑ –£–ø—Ä–æ—â–µ–Ω–∏–µ (–†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è)

### –ü—Ä–æ–±–ª–µ–º–∞ –¥—É–±–ª–∏—Ä–æ–≤–∞–Ω–∏—è

–¢–µ–∫—É—â–∞—è –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è —Ç—Ä–µ–±—É–µ—Ç –¥—É–±–ª–∏—Ä–æ–≤–∞–Ω–∏—è:
```bash
LLM_MODEL=meta-llama/Meta-Llama-3.1-8B-Instruct     # 1-–π —Ä–∞–∑
VLLM_MODEL=meta-llama/Meta-Llama-3.1-8B-Instruct   # 2-–π —Ä–∞–∑
```

### –†–µ—à–µ–Ω–∏–µ: –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –æ–¥–Ω—É –ø–µ—Ä–µ–º–µ–Ω–Ω—É—é

–í backend/services/config.py –º–æ–∂–Ω–æ –¥–æ–±–∞–≤–∏—Ç—å:
```python
LLM_MODE = os.getenv("LLM_MODE", "ollama")
LLM_MODEL = os.getenv("LLM_MODEL", "llama3.1:8b")

# –î–ª—è vLLM –∏—Å–ø–æ–ª—å–∑—É–µ–º LLM_MODEL –µ—Å–ª–∏ VLLM_MODEL –Ω–µ —É–∫–∞–∑–∞–Ω
VLLM_MODEL = os.getenv("VLLM_MODEL", LLM_MODEL if LLM_MODE == "vllm" else "")
```

–¢–æ–≥–¥–∞ –≤ .env –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ:
```bash
LLM_MODE=vllm
LLM_MODEL=meta-llama/Meta-Llama-3.1-8B-Instruct
# VLLM_MODEL –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ = LLM_MODEL
```

---

## üìä –ü–æ–ª–Ω—ã–π —Å–ø–∏—Å–æ–∫ LLM –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö

| –ü–µ—Ä–µ–º–µ–Ω–Ω–∞—è | –ö–æ–Ω—Ç–µ–π–Ω–µ—Ä | –û–±—è–∑–∞—Ç–µ–ª—å–Ω–∞—è | –û–ø–∏—Å–∞–Ω–∏–µ |
|-----------|-----------|--------------|----------|
| `LLM_MODE` | Backend | ‚úÖ | –ü—Ä–æ–≤–∞–π–¥–µ—Ä: ollama/vllm/none |
| `LLM_MODEL` | Backend | ‚úÖ | –ò–º—è –º–æ–¥–µ–ª–∏ (–æ–±—â–µ–µ) |
| `LLM_TIMEOUT` | Backend | ‚ùå | –¢–∞–π–º–∞—É—Ç –∑–∞–ø—Ä–æ—Å–∞ (—Å–µ–∫) |
| `LLM_MAX_TOKENS` | Backend | ‚ùå | –ú–∞–∫—Å —Ç–æ–∫–µ–Ω–æ–≤ –æ—Ç–≤–µ—Ç–∞ |
| `LLM_TEMPERATURE` | Backend | ‚ùå | Temperature (0.0-1.0) |
| `LLM_STREAM_ENABLED` | Backend | ‚ùå | Streaming –æ—Ç–≤–µ—Ç–∞ |
| `LLM_VLLM_URL` | Backend | ‚úÖ* | URL vLLM API (–µ—Å–ª–∏ LLM_MODE=vllm) |
| `VLLM_MODEL` | vLLM | ‚úÖ* | –ú–æ–¥–µ–ª—å –¥–ª—è vLLM (–µ—Å–ª–∏ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è) |
| `VLLM_GPU_MEMORY_UTILIZATION` | vLLM | ‚ùå | GPU memory (0.0-1.0) |
| `VLLM_MAX_MODEL_LEN` | vLLM | ‚ùå | –ú–∞–∫—Å –¥–ª–∏–Ω–∞ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ |
| `VLLM_TENSOR_PARALLEL_SIZE` | vLLM | ‚ùå | –ö–æ–ª-–≤–æ GPU |
| `VLLM_ENABLE_FP8` | vLLM | ‚ùå | FP8 precision |
| `VLLM_USE_FLASHINFER` | vLLM | ‚ùå | FlashInfer attention |

\* - –æ–±—è–∑–∞—Ç–µ–ª—å–Ω–∞ –ø—Ä–∏ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–µ–º —Ä–µ–∂–∏–º–µ

---

## üéØ Quick Reference

### –Ø —Ö–æ—á—É –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å Ollama
```bash
LLM_MODE=ollama
LLM_MODEL=llama3.1:8b
```

### –Ø —Ö–æ—á—É –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å vLLM
```bash
LLM_MODE=vllm
LLM_MODEL=meta-llama/Meta-Llama-3.1-8B-Instruct
VLLM_MODEL=meta-llama/Meta-Llama-3.1-8B-Instruct
LLM_VLLM_URL=http://vllm:8001/v1
```

### –Ø —Ö–æ—á—É –æ—Ç–∫–ª—é—á–∏—Ç—å LLM (—Ç–æ–ª—å–∫–æ –ø–æ–∏—Å–∫)
```bash
LLM_MODE=none
```

### –Ø —Ö–æ—á—É —Å–º–µ–Ω–∏—Ç—å –º–æ–¥–µ–ª—å –≤ vLLM
```bash
# 1. –û—Å—Ç–∞–Ω–æ–≤–∏—Ç—å
make down-vllm

# 2. –û–±–Ω–æ–≤–∏—Ç—å –æ–±–µ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ
nano .env
# LLM_MODEL=–Ω–æ–≤–∞—è-–º–æ–¥–µ–ª—å
# VLLM_MODEL=–Ω–æ–≤–∞—è-–º–æ–¥–µ–ª—å

# 3. –ó–∞–ø—É—Å—Ç–∏—Ç—å
make up-vllm
```

---

## üêõ Troubleshooting

### –û—à–∏–±–∫–∞: "Model not found"
**–ü—Ä–∏—á–∏–Ω–∞:** `VLLM_MODEL` –Ω–µ —Å–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∏—Ä–æ–≤–∞–Ω —Å `LLM_MODEL`  
**–†–µ—à–µ–Ω–∏–µ:** –ü—Ä–æ–≤–µ—Ä—å—Ç–µ —á—Ç–æ –æ–±–µ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –æ–¥–∏–Ω–∞–∫–æ–≤—ã–µ

### –û—à–∏–±–∫–∞: "Connection refused"
**–ü—Ä–∏—á–∏–Ω–∞:** `LLM_VLLM_URL` –Ω–µ–ø—Ä–∞–≤–∏–ª—å–Ω—ã–π  
**–†–µ—à–µ–Ω–∏–µ:** –î–æ–ª–∂–Ω–æ –±—ã—Ç—å `http://vllm:8001/v1`

### Backend –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç –Ω–µ–ø—Ä–∞–≤–∏–ª—å–Ω—É—é –º–æ–¥–µ–ª—å
**–ü—Ä–∏—á–∏–Ω–∞:** `LLM_MODEL` –æ—Ç–ª–∏—á–∞–µ—Ç—Å—è –æ—Ç `VLLM_MODEL`  
**–†–µ—à–µ–Ω–∏–µ:** –°–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∏—Ä—É–π—Ç–µ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ

