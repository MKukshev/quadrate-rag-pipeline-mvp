# ÐŸÐ¾Ð»Ð½Ð¾Ðµ Ñ€ÑƒÐºÐ¾Ð²Ð¾Ð´ÑÑ‚Ð²Ð¾ Ð¿Ð¾ Ñ€Ð°Ð·Ð²ÐµÑ€Ñ‚Ñ‹Ð²Ð°Ð½Ð¸ÑŽ Ð½Ð° GPU ÑÐµÑ€Ð²ÐµÑ€Ðµ

## ðŸ“‹ Ð¡Ð¾Ð´ÐµÑ€Ð¶Ð°Ð½Ð¸Ðµ

1. [ÐžÐ±Ð·Ð¾Ñ€ Ñ€Ð°Ð·Ð²ÐµÑ€Ñ‚Ñ‹Ð²Ð°Ð½Ð¸Ñ](#Ð¾Ð±Ð·Ð¾Ñ€)
2. [Ð¢Ñ€ÐµÐ±Ð¾Ð²Ð°Ð½Ð¸Ñ Ðº ÑÐµÑ€Ð²ÐµÑ€Ñƒ](#Ñ‚Ñ€ÐµÐ±Ð¾Ð²Ð°Ð½Ð¸Ñ)
3. [ÐŸÐ¾Ð´Ð³Ð¾Ñ‚Ð¾Ð²ÐºÐ° ÑÐµÑ€Ð²ÐµÑ€Ð°](#Ð¿Ð¾Ð´Ð³Ð¾Ñ‚Ð¾Ð²ÐºÐ°)
4. [Ð Ð°Ð·Ð²ÐµÑ€Ñ‚Ñ‹Ð²Ð°Ð½Ð¸Ðµ Ð½Ð° RTX 6000 Blackwell](#blackwell-deployment)
5. [Ð Ð°Ð·Ð²ÐµÑ€Ñ‚Ñ‹Ð²Ð°Ð½Ð¸Ðµ Ð½Ð° Ð´Ñ€ÑƒÐ³Ð¸Ñ… GPU](#Ð´Ñ€ÑƒÐ³Ð¸Ðµ-gpu)
6. [ÐœÐ¸Ð³Ñ€Ð°Ñ†Ð¸Ñ Ñ Ollama Ð½Ð° vLLM](#Ð¼Ð¸Ð³Ñ€Ð°Ñ†Ð¸Ñ)
7. [ÐœÐ¾Ð½Ð¸Ñ‚Ð¾Ñ€Ð¸Ð½Ð³ Ð¸ Ð¾Ð¿Ñ‚Ð¸Ð¼Ð¸Ð·Ð°Ñ†Ð¸Ñ](#Ð¼Ð¾Ð½Ð¸Ñ‚Ð¾Ñ€Ð¸Ð½Ð³)
8. [Troubleshooting](#troubleshooting)

---

## ðŸŽ¯ ÐžÐ±Ð·Ð¾Ñ€ {#Ð¾Ð±Ð·Ð¾Ñ€}

Ð­Ñ‚Ð¾Ñ‚ RAG-Ð¿Ð°Ð¹Ð¿Ð»Ð°Ð¹Ð½ Ð¼Ð¾Ð¶ÐµÑ‚ Ñ€Ð°Ð±Ð¾Ñ‚Ð°Ñ‚ÑŒ Ð² Ñ‚Ñ€ÐµÑ… Ñ€ÐµÐ¶Ð¸Ð¼Ð°Ñ…:

| Ð ÐµÐ¶Ð¸Ð¼ | GPU | Ð¡ÐºÐ¾Ñ€Ð¾ÑÑ‚ÑŒ | ÐœÐ¾Ð´ÐµÐ»Ð¸ | ÐšÐ¾Ð½Ñ„Ð¸Ð³ Ñ„Ð°Ð¹Ð» |
|-------|-----|----------|---------|-------------|
| **Ollama** | ÐžÐ¿Ñ†Ð¸Ð¾Ð½Ð°Ð»ÑŒÐ½Ð¾ | 40-60 tokens/sec | 8B Ð½Ð° CPU, Ð´Ð¾ 70B Ð½Ð° GPU | `docker-compose.yml` |
| **vLLM** | ÐžÐ±ÑÐ·Ð°Ñ‚ÐµÐ»ÑŒÐ½Ð¾ | 150-400 tokens/sec | 8B-70B+ | `docker-compose.vllm.yml` |
| **vLLM+MIG** | ÐžÐ±ÑÐ·Ð°Ñ‚ÐµÐ»ÑŒÐ½Ð¾ | ÐŸÐ°Ñ€Ð°Ð»Ð»ÐµÐ»ÑŒÐ½Ð¾ | 8B x 6 Ð¸Ð½ÑÑ‚Ð°Ð½ÑÐ¾Ð² | `docker-compose.vllm-mig.yml` |

### Ð ÐµÐºÐ¾Ð¼ÐµÐ½Ð´ÑƒÐµÐ¼Ñ‹Ðµ ÐºÐ¾Ð½Ñ„Ð¸Ð³ÑƒÑ€Ð°Ñ†Ð¸Ð¸:

**Ð”Ð»Ñ RTX 6000 Blackwell 96GB:**
- â­ **Ð ÐµÐºÐ¾Ð¼ÐµÐ½Ð´ÑƒÐµÑ‚ÑÑ:** vLLM Ñ FP8 Ð¼Ð¾Ð´ÐµÐ»ÑÐ¼Ð¸
- ÐœÐ¾Ð´ÐµÐ»Ð¸: Llama-70B FP8, Qwen-72B FP8
- Throughput: 200-300 tokens/sec, 15-30 req/s

**Ð”Ð»Ñ Ð´Ñ€ÑƒÐ³Ð¸Ñ… GPU (RTX 4090, A100, H100):**
- vLLM Ñ FP16/BF16 Ð¼Ð¾Ð´ÐµÐ»ÑÐ¼Ð¸
- ÐœÐ¾Ð´ÐµÐ»Ð¸: Llama-8B, Mistral-7B, Mixtral-8x7B
- Throughput: 100-250 tokens/sec

---

## ðŸ–¥ï¸ Ð¢Ñ€ÐµÐ±Ð¾Ð²Ð°Ð½Ð¸Ñ Ðº ÑÐµÑ€Ð²ÐµÑ€Ñƒ {#Ñ‚Ñ€ÐµÐ±Ð¾Ð²Ð°Ð½Ð¸Ñ}

### ÐœÐ¸Ð½Ð¸Ð¼Ð°Ð»ÑŒÐ½Ñ‹Ðµ Ñ‚Ñ€ÐµÐ±Ð¾Ð²Ð°Ð½Ð¸Ñ (Ollama Ð±ÐµÐ· GPU):
- CPU: 8 cores
- RAM: 16 GB
- Disk: 50 GB SSD
- GPU: ÐÐµ Ñ‚Ñ€ÐµÐ±ÑƒÐµÑ‚ÑÑ (Ð¼ÐµÐ´Ð»ÐµÐ½Ð½Ð¾)

### Ð ÐµÐºÐ¾Ð¼ÐµÐ½Ð´ÑƒÐµÐ¼Ñ‹Ðµ Ñ‚Ñ€ÐµÐ±Ð¾Ð²Ð°Ð½Ð¸Ñ (vLLM Ñ GPU):
- **GPU:** NVIDIA GPU Ñ 16GB+ VRAM (RTX 4090, A100, H100)
- **CPU:** 8-16 cores
- **RAM:** 32-64 GB
- **Disk:** 200 GB NVMe SSD
- **CUDA:** 12.4+ Ð´Ð»Ñ Blackwell, 11.8+ Ð´Ð»Ñ Ð´Ñ€ÑƒÐ³Ð¸Ñ…
- **Docker:** 24.0+ Ñ GPU support

### ÐžÐ¿Ñ‚Ð¸Ð¼Ð°Ð»ÑŒÐ½Ñ‹Ðµ Ñ‚Ñ€ÐµÐ±Ð¾Ð²Ð°Ð½Ð¸Ñ (vLLM Ð½Ð° RTX 6000 Blackwell):
- **GPU:** RTX 6000 Blackwell 96GB
- **CPU:** 16+ cores
- **RAM:** 64-128 GB
- **Disk:** 500 GB+ NVMe SSD
- **CUDA:** 12.6.1+
- **vLLM:** 0.6.2+

---

## ðŸ› ï¸ ÐŸÐ¾Ð´Ð³Ð¾Ñ‚Ð¾Ð²ÐºÐ° ÑÐµÑ€Ð²ÐµÑ€Ð° {#Ð¿Ð¾Ð´Ð³Ð¾Ñ‚Ð¾Ð²ÐºÐ°}

### 1. Ð£ÑÑ‚Ð°Ð½Ð¾Ð²ÐºÐ° NVIDIA Ð´Ñ€Ð°Ð¹Ð²ÐµÑ€Ð° Ð¸ CUDA

```bash
# ÐŸÑ€Ð¾Ð²ÐµÑ€Ð¸Ñ‚ÑŒ Ð²ÐµÑ€ÑÐ¸ÑŽ Ð´Ñ€Ð°Ð¹Ð²ÐµÑ€Ð°
nvidia-smi

# Ð£ÑÑ‚Ð°Ð½Ð¾Ð²Ð¸Ñ‚ÑŒ CUDA 12.6.1 Ð´Ð»Ñ Blackwell
wget https://developer.download.nvidia.com/compute/cuda/12.6.1/local_installers/cuda_12.6.1_550.54.15_linux.run
sudo sh cuda_12.6.1_550.54.15_linux.run

# Ð”Ð¾Ð±Ð°Ð²Ð¸Ñ‚ÑŒ Ð² PATH
echo 'export PATH=/usr/local/cuda-12.6/bin:$PATH' >> ~/.bashrc
echo 'export LD_LIBRARY_PATH=/usr/local/cuda-12.6/lib64:$LD_LIBRARY_PATH' >> ~/.bashrc
source ~/.bashrc
```

### 2. Ð£ÑÑ‚Ð°Ð½Ð¾Ð²ÐºÐ° Docker Ð¸ NVIDIA Container Toolkit

```bash
# Ð£ÑÑ‚Ð°Ð½Ð¾Ð²Ð¸Ñ‚ÑŒ Docker
curl -fsSL https://get.docker.com | sh
sudo usermod -aG docker $USER

# Ð£ÑÑ‚Ð°Ð½Ð¾Ð²Ð¸Ñ‚ÑŒ NVIDIA Container Toolkit
distribution=$(. /etc/os-release;echo $ID$VERSION_ID)
curl -s -L https://nvidia.github.io/nvidia-docker/gpgkey | sudo apt-key add -
curl -s -L https://nvidia.github.io/nvidia-docker/$distribution/nvidia-docker.list | sudo tee /etc/apt/sources.list.d/nvidia-docker.list

sudo apt-get update
sudo apt-get install -y nvidia-container-toolkit
sudo systemctl restart docker

# ÐŸÑ€Ð¾Ð²ÐµÑ€Ð¸Ñ‚ÑŒ GPU Ð´Ð¾ÑÑ‚ÑƒÐ¿ÐµÐ½ Ð² Docker
docker run --rm --gpus all nvidia/cuda:12.6.1-base-ubuntu22.04 nvidia-smi
```

### 3. ÐšÐ»Ð¾Ð½Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð¸Ðµ Ñ€ÐµÐ¿Ð¾Ð·Ð¸Ñ‚Ð¾Ñ€Ð¸Ñ

```bash
# ÐšÐ»Ð¾Ð½Ð¸Ñ€Ð¾Ð²Ð°Ñ‚ÑŒ Ð¿Ñ€Ð¾ÐµÐºÑ‚
git clone https://github.com/your-org/ai-assistant-mvp.git
cd ai-assistant-mvp

# Ð¡Ð¾Ð·Ð´Ð°Ñ‚ÑŒ .env Ñ„Ð°Ð¹Ð»
cp .env.example .env

# Ð”Ð¾Ð±Ð°Ð²Ð¸Ñ‚ÑŒ HuggingFace Ñ‚Ð¾ÐºÐµÐ½
nano .env
# HUGGING_FACE_HUB_TOKEN=hf_xxxxxxxxxxxxxxxx
```

---

## ðŸš€ Ð Ð°Ð·Ð²ÐµÑ€Ñ‚Ñ‹Ð²Ð°Ð½Ð¸Ðµ Ð½Ð° RTX 6000 Blackwell {#blackwell-deployment}

### Ð’Ð°Ñ€Ð¸Ð°Ð½Ñ‚ 1: Ð‘Ð°Ð·Ð¾Ð²Ð¾Ðµ Ñ€Ð°Ð·Ð²ÐµÑ€Ñ‚Ñ‹Ð²Ð°Ð½Ð¸Ðµ Ñ vLLM

```bash
# 1. Ð˜ÑÐ¿Ð¾Ð»ÑŒÐ·Ð¾Ð²Ð°Ñ‚ÑŒ ÐºÐ¾Ð½Ñ„Ð¸Ð³ÑƒÑ€Ð°Ñ†Ð¸ÑŽ vLLM
cp .env.vllm .env

# 2. Ð—Ð°Ð¿ÑƒÑÑ‚Ð¸Ñ‚ÑŒ ÑÐµÑ€Ð²Ð¸ÑÑ‹
make up-vllm

# 3. Ð”Ð¾Ð¶Ð´Ð°Ñ‚ÑŒÑÑ Ð³Ð¾Ñ‚Ð¾Ð²Ð½Ð¾ÑÑ‚Ð¸ (2-5 Ð¼Ð¸Ð½ÑƒÑ‚ Ð´Ð»Ñ Ð·Ð°Ð³Ñ€ÑƒÐ·ÐºÐ¸ Ð¼Ð¾Ð´ÐµÐ»Ð¸)
make wait

# 4. ÐŸÑ€Ð¾Ð²ÐµÑ€Ð¸Ñ‚ÑŒ health
curl http://localhost:8000/health
```

**Ð§Ñ‚Ð¾ Ð·Ð°Ð¿ÑƒÑÐºÐ°ÐµÑ‚ÑÑ:**
- Qdrant (vector database) Ð½Ð° Ð¿Ð¾Ñ€Ñ‚Ñƒ 6333
- vLLM ÑÐµÑ€Ð²ÐµÑ€ Ð½Ð° Ð¿Ð¾Ñ€Ñ‚Ñƒ 8001
- Backend API Ð½Ð° Ð¿Ð¾Ñ€Ñ‚Ñƒ 8000

**ÐœÐ¾Ð´ÐµÐ»ÑŒ Ð¿Ð¾ ÑƒÐ¼Ð¾Ð»Ñ‡Ð°Ð½Ð¸ÑŽ:** `Meta-Llama-3.1-8B-Instruct`
**ÐŸÑ€Ð¾Ð¸Ð·Ð²Ð¾Ð´Ð¸Ñ‚ÐµÐ»ÑŒÐ½Ð¾ÑÑ‚ÑŒ:** 300-400 tokens/sec

### Ð’Ð°Ñ€Ð¸Ð°Ð½Ñ‚ 2: Ð Ð°Ð·Ð²ÐµÑ€Ñ‚Ñ‹Ð²Ð°Ð½Ð¸Ðµ Ñ FP8 Ð¼Ð¾Ð´ÐµÐ»ÑŒÑŽ (Llama-70B!)

```bash
# 1. Ð˜ÑÐ¿Ð¾Ð»ÑŒÐ·Ð¾Ð²Ð°Ñ‚ÑŒ FP8 ÐºÐ¾Ð½Ñ„Ð¸Ð³ÑƒÑ€Ð°Ñ†Ð¸ÑŽ
cp .env.vllm-fp8 .env

# 2. Ð”Ð¾Ð±Ð°Ð²Ð¸Ñ‚ÑŒ HF Ñ‚Ð¾ÐºÐµÐ½
nano .env
# HUGGING_FACE_HUB_TOKEN=hf_xxxxx

# 3. Ð—Ð°Ð¿ÑƒÑÑ‚Ð¸Ñ‚ÑŒ
make up-vllm

# 4. ÐŸÑ€Ð¾Ð²ÐµÑ€Ð¸Ñ‚ÑŒ
curl http://localhost:8000/health
```

**ÐœÐ¾Ð´ÐµÐ»ÑŒ:** `neuralmagic/Meta-Llama-3.1-70B-Instruct-FP8`
**VRAM:** ~70GB (Ð²Ð»ÐµÐ·Ð°ÐµÑ‚ Ð² 96GB!)
**ÐŸÑ€Ð¾Ð¸Ð·Ð²Ð¾Ð´Ð¸Ñ‚ÐµÐ»ÑŒÐ½Ð¾ÑÑ‚ÑŒ:** 200-300 tokens/sec

### Ð’Ð°Ñ€Ð¸Ð°Ð½Ñ‚ 3: Ð Ð°Ð·Ð²ÐµÑ€Ñ‚Ñ‹Ð²Ð°Ð½Ð¸Ðµ Ñ MIG (Multi-Instance GPU)

MIG Ð¿Ð¾Ð·Ð²Ð¾Ð»ÑÐµÑ‚ Ñ€Ð°Ð·Ð±Ð¸Ñ‚ÑŒ RTX 6000 96GB Ð½Ð° 6 Ð½ÐµÐ·Ð°Ð²Ð¸ÑÐ¸Ð¼Ñ‹Ñ… Ð¸Ð½ÑÑ‚Ð°Ð½ÑÐ¾Ð²:

```bash
# 1. ÐÐ°ÑÑ‚Ñ€Ð¾Ð¸Ñ‚ÑŒ MIG
make setup-mig

# 2. ÐŸÑ€Ð¾Ð²ÐµÑ€Ð¸Ñ‚ÑŒ MIG Ð¸Ð½ÑÑ‚Ð°Ð½ÑÑ‹
make list-mig

# 3. Ð—Ð°Ð¿ÑƒÑÑ‚Ð¸Ñ‚ÑŒ Ñ MIG
make up-vllm-mig

# ÐšÐ°Ð¶Ð´Ñ‹Ð¹ Ð¸Ð½ÑÑ‚Ð°Ð½Ñ = Ð¾Ñ‚Ð´ÐµÐ»ÑŒÐ½Ð°Ñ Ð¼Ð¾Ð´ÐµÐ»ÑŒ!
```

Ð¡Ð¼. [MIG_QUICKSTART.md](../MIG_QUICKSTART.md) Ð´Ð»Ñ Ð¿Ð¾Ð´Ñ€Ð¾Ð±Ð½Ð¾ÑÑ‚ÐµÐ¹.

---

## ðŸ–¥ï¸ Ð Ð°Ð·Ð²ÐµÑ€Ñ‚Ñ‹Ð²Ð°Ð½Ð¸Ðµ Ð½Ð° Ð´Ñ€ÑƒÐ³Ð¸Ñ… GPU {#Ð´Ñ€ÑƒÐ³Ð¸Ðµ-gpu}

### RTX 4090 (24GB)

```bash
# 1. Ð¡Ð¾Ð·Ð´Ð°Ñ‚ÑŒ .env
cp .env.vllm .env

# 2. ÐÐ°ÑÑ‚Ñ€Ð¾Ð¸Ñ‚ÑŒ Ð¼Ð¾Ð´ÐµÐ»ÑŒ Ð¿Ð¾Ð´ 24GB
cat >> .env << EOF
VLLM_MODEL=meta-llama/Meta-Llama-3.1-8B-Instruct
VLLM_GPU_MEMORY_UTILIZATION=0.9
VLLM_MAX_MODEL_LEN=8192
EOF

# 3. Ð—Ð°Ð¿ÑƒÑÑ‚Ð¸Ñ‚ÑŒ
make up-vllm
```

**ÐŸÐ¾Ð´Ð´ÐµÑ€Ð¶Ð¸Ð²Ð°ÐµÐ¼Ñ‹Ðµ Ð¼Ð¾Ð´ÐµÐ»Ð¸:**
- Llama-8B âœ…
- Mistral-7B âœ…
- Mixtral-8x7B (Ñ quantization)

### A100 40GB / H100 80GB

```bash
# 1. Ð¡Ð¾Ð·Ð´Ð°Ñ‚ÑŒ .env
cp .env.vllm .env

# 2. ÐÐ°ÑÑ‚Ñ€Ð¾Ð¸Ñ‚ÑŒ Ð´Ð»Ñ A100/H100
cat >> .env << EOF
VLLM_MODEL=meta-llama/Meta-Llama-3.1-8B-Instruct
VLLM_GPU_MEMORY_UTILIZATION=0.95
VLLM_MAX_MODEL_LEN=16384
VLLM_TENSOR_PARALLEL_SIZE=1
EOF

# 3. Ð—Ð°Ð¿ÑƒÑÑ‚Ð¸Ñ‚ÑŒ
make up-vllm
```

**Ð”Ð»Ñ H100 Ð¼Ð¾Ð¶Ð½Ð¾ Ð·Ð°Ð¿ÑƒÑÑ‚Ð¸Ñ‚ÑŒ:**
- Llama-70B âœ…
- Mixtral-8x7B âœ…
- Qwen-72B (Ñ FP8)

---

## ðŸ”„ ÐœÐ¸Ð³Ñ€Ð°Ñ†Ð¸Ñ Ñ Ollama Ð½Ð° vLLM {#Ð¼Ð¸Ð³Ñ€Ð°Ñ†Ð¸Ñ}

### Ð¨Ð°Ð³ 1: Ð£ÑÑ‚Ð°Ð½Ð¾Ð²Ð¸Ñ‚ÑŒ vLLM Ð½Ð° ÑÐµÑ€Ð²ÐµÑ€

```bash
# ÐšÐ»Ð¾Ð½Ð¸Ñ€Ð¾Ð²Ð°Ñ‚ÑŒ Ð¿Ñ€Ð¾ÐµÐºÑ‚
git clone https://github.com/your-org/ai-assistant-mvp.git
cd ai-assistant-mvp

# Ð˜ÑÐ¿Ð¾Ð»ÑŒÐ·Ð¾Ð²Ð°Ñ‚ÑŒ vLLM ÐºÐ¾Ð½Ñ„Ð¸Ð³ÑƒÑ€Ð°Ñ†Ð¸ÑŽ
cp .env.vllm .env
nano .env  # Ð”Ð¾Ð±Ð°Ð²Ð¸Ñ‚ÑŒ HF Ñ‚Ð¾ÐºÐµÐ½
```

### Ð¨Ð°Ð³ 2: Ð—Ð°Ð¿ÑƒÑÑ‚Ð¸Ñ‚ÑŒ vLLM Ð²Ð¼ÐµÑÑ‚Ð¾ Ollama

```bash
# ÐžÑÑ‚Ð°Ð½Ð¾Ð²Ð¸Ñ‚ÑŒ Ñ‚ÐµÐºÑƒÑ‰Ð¸Ð¹ ÑÑ‚ÐµÐº (Ollama)
make down

# Ð—Ð°Ð¿ÑƒÑÑ‚Ð¸Ñ‚ÑŒ vLLM ÑÑ‚ÐµÐº
make up-vllm

# ÐŸÑ€Ð¾Ð²ÐµÑ€Ð¸Ñ‚ÑŒ
curl http://localhost:8000/health
```

### Ð¨Ð°Ð³ 3: Ð˜Ð½Ð´ÐµÐºÑÐ°Ñ†Ð¸Ñ Ð´Ð°Ð½Ð½Ñ‹Ñ…

```bash
# Ð˜Ð½Ð´ÐµÐºÑÐ¸Ñ€Ð¾Ð²Ð°Ñ‚ÑŒ Ð´Ð¾ÐºÑƒÐ¼ÐµÐ½Ñ‚Ñ‹
make ingest SPACE=production

# Ð˜Ð»Ð¸ Ñ‡ÐµÑ€ÐµÐ· API
curl -X POST http://localhost:8000/ingest \
  -F "file=@document.pdf" \
  -F "space_id=production"
```

### Ð¨Ð°Ð³ 4: Ð¢ÐµÑÑ‚Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð¸Ðµ

```bash
# ÐŸÑ€Ð¾ÑÑ‚Ð¾Ð¹ Ð·Ð°Ð¿Ñ€Ð¾Ñ
curl -X POST http://localhost:8000/ask \
  -H "Content-Type: application/json" \
  -d '{
    "q": "ÐšÐ°ÐºÐ¸Ðµ Ð´ÐµÐ´Ð»Ð°Ð¹Ð½Ñ‹ Ñƒ Ð¿Ñ€Ð¾ÐµÐºÑ‚Ð°?",
    "space_id": "production",
    "top_k": 6
  }'
```

---

## ðŸ“Š ÐœÐ¾Ð½Ð¸Ñ‚Ð¾Ñ€Ð¸Ð½Ð³ Ð¸ Ð¾Ð¿Ñ‚Ð¸Ð¼Ð¸Ð·Ð°Ñ†Ð¸Ñ {#Ð¼Ð¾Ð½Ð¸Ñ‚Ð¾Ñ€Ð¸Ð½Ð³}

### ÐœÐ¾Ð½Ð¸Ñ‚Ð¾Ñ€Ð¸Ð½Ð³ GPU

```bash
# Ð¡Ð¼Ð¾Ñ‚Ñ€ÐµÑ‚ÑŒ Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·Ð¾Ð²Ð°Ð½Ð¸Ðµ GPU
watch -n 1 nvidia-smi

# ÐŸÑ€Ð¾Ð²ÐµÑ€Ð¸Ñ‚ÑŒ vLLM Ð»Ð¾Ð³Ð¸
make logs-vllm
```

### ÐœÐ¾Ð½Ð¸Ñ‚Ð¾Ñ€Ð¸Ð½Ð³ Ð¿Ñ€Ð¾Ð¸Ð·Ð²Ð¾Ð´Ð¸Ñ‚ÐµÐ»ÑŒÐ½Ð¾ÑÑ‚Ð¸

```bash
# Health check
curl http://localhost:8000/health | jq

# ÐœÐµÑ‚Ñ€Ð¸ÐºÐ¸
curl http://localhost:8000/metrics | jq

# ÐŸÐ¾Ð¿ÑƒÐ»ÑÑ€Ð½Ñ‹Ðµ Ð¿Ð¾Ð»Ñ:
# - "tokens_per_second" - ÑÐºÐ¾Ñ€Ð¾ÑÑ‚ÑŒ Ð³ÐµÐ½ÐµÑ€Ð°Ñ†Ð¸Ð¸
# - "cache_hit_rate" - ÑÑ„Ñ„ÐµÐºÑ‚Ð¸Ð²Ð½Ð¾ÑÑ‚ÑŒ ÐºÑÑˆÐ°
# - "avg_latency_ms" - ÑÑ€ÐµÐ´Ð½ÑÑ Ð·Ð°Ð´ÐµÑ€Ð¶ÐºÐ°
```

### ÐžÐ¿Ñ‚Ð¸Ð¼Ð¸Ð·Ð°Ñ†Ð¸Ñ Ð¿Ñ€Ð¾Ð¸Ð·Ð²Ð¾Ð´Ð¸Ñ‚ÐµÐ»ÑŒÐ½Ð¾ÑÑ‚Ð¸

#### 1. Ð£Ð²ÐµÐ»Ð¸Ñ‡Ð¸Ñ‚ÑŒ GPU memory utilization

```bash
# Ð’ docker-compose.vllm.yml Ð¸Ð»Ð¸ .env
VLLM_GPU_MEMORY_UTILIZATION=0.95  # Ð±Ñ‹Ð»Ð¾ 0.9
```

#### 2. Ð£Ð²ÐµÐ»Ð¸Ñ‡Ð¸Ñ‚ÑŒ batch size

```bash
VLLM_MAX_NUM_SEQS=512  # Ð±Ñ‹Ð»Ð¾ 256
```

#### 3. Ð’ÐºÐ»ÑŽÑ‡Ð¸Ñ‚ÑŒ Ð¾Ð¿Ñ‚Ð¸Ð¼Ð¸Ð·Ð°Ñ†Ð¸Ð¸ Blackwell

```bash
VLLM_ENABLE_FP8=true
VLLM_USE_FLASHINFER=true
VLLM_ENABLE_CHUNKED_PREFILL=true
```

#### 4. ÐžÐ¿Ñ‚Ð¸Ð¼Ð¸Ð·Ð¸Ñ€Ð¾Ð²Ð°Ñ‚ÑŒ ÐºÐ¾Ð½Ñ‚ÐµÐºÑÑ‚

```bash
# Ð’ backend/.env
CONTEXT_MAX_CHUNKS=8          # Ð±Ñ‹Ð»Ð¾ 6
CONTEXT_SNIPPET_MAX_CHARS=800  # Ð±Ñ‹Ð»Ð¾ 600
```

### ÐÐ°ÑÑ‚Ñ€Ð¾Ð¹ÐºÐ° Qdrant

```bash
# ÐžÐ¿Ñ‚Ð¸Ð¼Ð¸Ð·Ð¸Ñ€Ð¾Ð²Ð°Ñ‚ÑŒ HNSW Ð´Ð»Ñ ÑÐºÐ¾Ñ€Ð¾ÑÑ‚Ð¸
QDRANT_HNSW_M=32              # ÑƒÐ²ÐµÐ»Ð¸Ñ‡Ð¸Ñ‚ÑŒ
QDRANT_HNSW_EF_CONSTRUCT=150
QDRANT_HNSW_EF_SEARCH=128
```

---

## ðŸ”§ Troubleshooting {#troubleshooting}

### ÐŸÑ€Ð¾Ð±Ð»ÐµÐ¼Ð°: "CUDA out of memory"

**Ð ÐµÑˆÐµÐ½Ð¸Ðµ:**
```bash
# Ð£Ð¼ÐµÐ½ÑŒÑˆÐ¸Ñ‚ÑŒ memory utilization
VLLM_GPU_MEMORY_UTILIZATION=0.85

# Ð˜Ð»Ð¸ Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·Ð¾Ð²Ð°Ñ‚ÑŒ Ð¼ÐµÐ½ÑŒÑˆÑƒÑŽ Ð¼Ð¾Ð´ÐµÐ»ÑŒ
VLLM_MODEL=mistralai/Mistral-7B-Instruct-v0.2
```

### ÐŸÑ€Ð¾Ð±Ð»ÐµÐ¼Ð°: "Docker GPU not found"

**Ð ÐµÑˆÐµÐ½Ð¸Ðµ:**
```bash
# ÐŸÑ€Ð¾Ð²ÐµÑ€Ð¸Ñ‚ÑŒ NVIDIA Container Toolkit
docker info | grep -i nvidia

# ÐŸÐµÑ€ÐµÐ·Ð°Ð¿ÑƒÑÑ‚Ð¸Ñ‚ÑŒ Docker
sudo systemctl restart docker

# ÐŸÑ€Ð¾Ð²ÐµÑ€Ð¸Ñ‚ÑŒ GPU Ð² ÐºÐ¾Ð½Ñ‚ÐµÐ¹Ð½ÐµÑ€Ðµ
docker run --rm --gpus all nvidia/cuda:12.6.1-base nvidia-smi
```

### ÐŸÑ€Ð¾Ð±Ð»ÐµÐ¼Ð°: "vLLM Ð½Ðµ Ð·Ð°Ð¿ÑƒÑÐºÐ°ÐµÑ‚ÑÑ"

**Ð ÐµÑˆÐµÐ½Ð¸Ðµ:**
```bash
# ÐŸÑ€Ð¾Ð²ÐµÑ€Ð¸Ñ‚ÑŒ Ð»Ð¾Ð³Ð¸
make logs-vllm

# ÐžÐ±Ñ‰Ð¸Ðµ Ð¿Ñ€Ð¸Ñ‡Ð¸Ð½Ñ‹:
# 1. CUDA Ð²ÐµÑ€ÑÐ¸Ñ Ð½Ðµ ÑÐ¾Ð²Ð¼ÐµÑÑ‚Ð¸Ð¼Ð°
nvidia-smi  # Ð´Ð¾Ð»Ð¶Ð½Ð° Ð±Ñ‹Ñ‚ÑŒ 550+

# 2. ÐœÐ¾Ð´ÐµÐ»ÑŒ Ð½Ðµ ÑÐºÐ°Ñ‡Ð°Ð»Ð°ÑÑŒ
docker compose exec vllm ls /root/.cache/huggingface

# 3. ÐÐµÐ´Ð¾ÑÑ‚Ð°Ñ‚Ð¾Ñ‡Ð½Ð¾ VRAM
nvidia-smi  # Ð¿Ñ€Ð¾Ð²ÐµÑ€Ð¸Ñ‚ÑŒ Ð´Ð¾ÑÑ‚ÑƒÐ¿Ð½ÑƒÑŽ Ð¿Ð°Ð¼ÑÑ‚ÑŒ
```

### ÐŸÑ€Ð¾Ð±Ð»ÐµÐ¼Ð°: "ÐœÐµÐ´Ð»ÐµÐ½Ð½Ð°Ñ ÑÐºÐ¾Ñ€Ð¾ÑÑ‚ÑŒ Ð³ÐµÐ½ÐµÑ€Ð°Ñ†Ð¸Ð¸"

**ÐŸÑ€Ð¾Ð²ÐµÑ€ÐºÐ°:**
```bash
# 1. ÐŸÑ€Ð¾Ð²ÐµÑ€Ð¸Ñ‚ÑŒ Ñ‚Ð¾ÐºÐµÐ½/ÑÐµÐº
curl http://localhost:8000/metrics | jq .tokens_per_second

# 2. ÐŸÑ€Ð¾Ð²ÐµÑ€Ð¸Ñ‚ÑŒ Ð·Ð°Ð³Ñ€ÑƒÐ·ÐºÑƒ GPU
nvidia-smi

# 3. ÐŸÑ€Ð¾Ð²ÐµÑ€Ð¸Ñ‚ÑŒ Ð»Ð¾Ð³Ð¸ vLLM
make logs-vllm | grep "tokens/s"
```

**Ð ÐµÑˆÐµÐ½Ð¸Ðµ:**
```bash
# 1. Ð’ÐºÐ»ÑŽÑ‡Ð¸Ñ‚ÑŒ Ð¾Ð¿Ñ‚Ð¸Ð¼Ð¸Ð·Ð°Ñ†Ð¸Ð¸
VLLM_USE_FLASHINFER=true
VLLM_ENABLE_CHUNKED_PREFILL=true

# 2. Ð£Ð¼ÐµÐ½ÑŒÑˆÐ¸Ñ‚ÑŒ ÐºÐ¾Ð½Ñ‚ÐµÐºÑÑ‚
CONTEXT_MAX_CHUNKS=4

# 3. Ð˜ÑÐ¿Ð¾Ð»ÑŒÐ·Ð¾Ð²Ð°Ñ‚ÑŒ quantization
VLLM_MODEL=neuralmagic/Meta-Llama-3.1-8B-Instruct-AWQ
```

### ÐŸÑ€Ð¾Ð±Ð»ÐµÐ¼Ð°: "Backend Ð½Ðµ Ð¿Ð¾Ð´ÐºÐ»ÑŽÑ‡Ð°ÐµÑ‚ÑÑ Ðº vLLM"

**Ð ÐµÑˆÐµÐ½Ð¸Ðµ:**
```bash
# 1. ÐŸÑ€Ð¾Ð²ÐµÑ€Ð¸Ñ‚ÑŒ vLLM health
curl http://localhost:8001/health

# 2. ÐŸÑ€Ð¾Ð²ÐµÑ€Ð¸Ñ‚ÑŒ Ð¿ÐµÑ€ÐµÐ¼ÐµÐ½Ð½Ñ‹Ðµ Ð¾ÐºÑ€ÑƒÐ¶ÐµÐ½Ð¸Ñ
docker compose exec backend env | grep VLLM

# 3. ÐŸÐµÑ€ÐµÐ·Ð°Ð¿ÑƒÑÑ‚Ð¸Ñ‚ÑŒ backend
docker compose restart backend
```

---

## ðŸ“š ÐŸÐ¾Ð»ÐµÐ·Ð½Ñ‹Ðµ ÐºÐ¾Ð¼Ð°Ð½Ð´Ñ‹

### Ð£Ð¿Ñ€Ð°Ð²Ð»ÐµÐ½Ð¸Ðµ ÑÐµÑ€Ð²Ð¸ÑÐ°Ð¼Ð¸

```bash
# Ð—Ð°Ð¿ÑƒÑÑ‚Ð¸Ñ‚ÑŒ vLLM ÑÑ‚ÐµÐº
make up-vllm

# ÐžÑÑ‚Ð°Ð½Ð¾Ð²Ð¸Ñ‚ÑŒ
make down-vllm

# Ð›Ð¾Ð³Ð¸
make logs-vllm

# ÐŸÐµÑ€ÐµÐ·Ð°Ð¿ÑƒÑÑ‚Ð¸Ñ‚ÑŒ
docker compose -f docker-compose.vllm.yml restart
```

### Ð Ð°Ð±Ð¾Ñ‚Ð° Ñ Ð´Ð°Ð½Ð½Ñ‹Ð¼Ð¸

```bash
# Ð˜Ð½Ð´ÐµÐºÑÐ¸Ñ€Ð¾Ð²Ð°Ñ‚ÑŒ Ð´Ð¾ÐºÑƒÐ¼ÐµÐ½Ñ‚Ñ‹
make ingest SPACE=production

# ÐŸÑ€Ð¾Ñ‚ÐµÑÑ‚Ð¸Ñ€Ð¾Ð²Ð°Ñ‚ÑŒ Ð·Ð°Ð¿Ñ€Ð¾Ñ
make ask SPACE=production

# ÐŸÑ€Ð¾Ð²ÐµÑ€Ð¸Ñ‚ÑŒ health
make health
```

### ÐžÑ‚Ð»Ð°Ð´ÐºÐ°

```bash
# Ð›Ð¾Ð³Ð¸ backend
docker compose logs -f backend

# Ð›Ð¾Ð³Ð¸ vLLM
docker compose logs -f vLLM

# Ð›Ð¾Ð³Ð¸ Qdrant
docker compose logs -f qdrant

# Ð˜ÑÐ¿Ð¾Ð»ÑŒÐ·Ð¾Ð²Ð°Ð½Ð¸Ðµ Ñ€ÐµÑÑƒÑ€ÑÐ¾Ð²
docker stats
```

### ÐžÐ±Ð½Ð¾Ð²Ð»ÐµÐ½Ð¸Ðµ Ð¼Ð¾Ð´ÐµÐ»Ð¸

```bash
# 1. ÐžÑÑ‚Ð°Ð½Ð¾Ð²Ð¸Ñ‚ÑŒ vLLM
make down-vllm

# 2. Ð˜Ð·Ð¼ÐµÐ½Ð¸Ñ‚ÑŒ Ð¼Ð¾Ð´ÐµÐ»ÑŒ Ð² .env
VLLM_MODEL=meta-llama/Meta-Llama-3.1-70B-Instruct

# 3. Ð£Ð´Ð°Ð»Ð¸Ñ‚ÑŒ ÑÑ‚Ð°Ñ€Ñ‹Ð¹ ÐºÑÑˆ (Ð¾Ð¿Ñ†Ð¸Ð¾Ð½Ð°Ð»ÑŒÐ½Ð¾)
docker volume rm ai-assistant-mvp_vllm_cache

# 4. Ð—Ð°Ð¿ÑƒÑÑ‚Ð¸Ñ‚ÑŒ Ð·Ð°Ð½Ð¾Ð²Ð¾
make up-vllm
```

---

## ðŸŽ¯ Production Checklist

ÐŸÐµÑ€ÐµÐ´ Ð·Ð°Ð¿ÑƒÑÐºÐ¾Ð¼ Ð² production:

- [ ] ÐÐ°ÑÑ‚Ñ€Ð¾ÐµÐ½ firewall (Ñ‚Ð¾Ð»ÑŒÐºÐ¾ Ð½ÑƒÐ¶Ð½Ñ‹Ðµ Ð¿Ð¾Ñ€Ñ‚Ñ‹)
- [ ] SSL/TLS Ð½Ð°ÑÑ‚Ñ€Ð¾ÐµÐ½ (Ñ‡ÐµÑ€ÐµÐ· nginx)
- [ ] ÐÐ°ÑÑ‚Ñ€Ð¾ÐµÐ½ Ð¼Ð¾Ð½Ð¸Ñ‚Ð¾Ñ€Ð¸Ð½Ð³ (Prometheus + Grafana)
- [ ] ÐÐ°ÑÑ‚Ñ€Ð¾ÐµÐ½Ñ‹ Ð»Ð¾Ð³Ð¸ (ELK stack)
- [ ] ÐÐ°ÑÑ‚Ñ€Ð¾ÐµÐ½ backup Ð´Ð°Ð½Ð½Ñ‹Ñ… (Qdrant volumes)
- [ ] ÐÐ°ÑÑ‚Ñ€Ð¾ÐµÐ½Ð¾ Ð»Ð¾Ð³Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð¸Ðµ (log rotation)
- [ ] ÐÐ°ÑÑ‚Ñ€Ð¾ÐµÐ½Ð° Ð°Ð»ÐµÑ€Ñ‚Ð¸Ð½Ð³ (Slack/PagerDuty)
- [ ] ÐŸÑ€Ð¾Ñ‚ÐµÑÑ‚Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð° Ð½Ð°Ð³Ñ€ÑƒÐ·ÐºÐ° (benchmark)
- [ ] ÐÐ°ÑÑ‚Ñ€Ð¾ÐµÐ½Ñ‹ Ñ€ÐµÐ·ÐµÑ€Ð²Ð½Ñ‹Ðµ ÐºÐ¾Ð¿Ð¸Ð¸ (cron)
- [ ] Ð”Ð¾ÐºÑƒÐ¼ÐµÐ½Ñ‚Ð¸Ñ€Ð¾Ð²Ð°Ð½Ñ‹ Ð¿Ñ€Ð¾Ñ†ÐµÐ´ÑƒÑ€Ñ‹ Ð²Ð¾ÑÑÑ‚Ð°Ð½Ð¾Ð²Ð»ÐµÐ½Ð¸Ñ

---

## ðŸ“– Ð”Ð¾Ð¿Ð¾Ð»Ð½Ð¸Ñ‚ÐµÐ»ÑŒÐ½Ð°Ñ Ð´Ð¾ÐºÑƒÐ¼ÐµÐ½Ñ‚Ð°Ñ†Ð¸Ñ

- [BLACKWELL_QUICKSTART.md](../BLACKWELL_QUICKSTART.md) - Ð‘Ñ‹ÑÑ‚Ñ€Ñ‹Ð¹ ÑÑ‚Ð°Ñ€Ñ‚ Ð´Ð»Ñ Blackwell
- [VLLM_QUICKSTART.md](../VLLM_QUICKSTART.md) - vLLM Ñ€Ð°Ð·Ð²ÐµÑ€Ñ‚Ñ‹Ð²Ð°Ð½Ð¸Ðµ
- [MIG_QUICKSTART.md](../MIG_QUICKSTART.md) - MIG Ð´Ð»Ñ RTX 6000
- [docs/BLACKWELL_OPTIMIZATIONS.md](BLACKWELL_OPTIMIZATIONS.md) - ÐžÐ¿Ñ‚Ð¸Ð¼Ð¸Ð·Ð°Ñ†Ð¸Ð¸
- [docs/VLLM_DEPLOYMENT.md](VLLM_DEPLOYMENT.md) - Ð”ÐµÑ‚Ð°Ð»Ð¸ vLLM
- [deploy_recommendations.md](../deploy_recommendations.md) - Ð ÐµÐºÐ¾Ð¼ÐµÐ½Ð´Ð°Ñ†Ð¸Ð¸ Ð¿Ð¾ Ð¸Ð½Ñ„Ñ€Ð°ÑÑ‚Ñ€ÑƒÐºÑ‚ÑƒÑ€Ðµ
