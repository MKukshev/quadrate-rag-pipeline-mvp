# –†–∞–∑–≤–µ—Ä—Ç—ã–≤–∞–Ω–∏–µ —Å vLLM –Ω–∞ GPU —Å–µ—Ä–≤–µ—Ä–µ

## üéØ –û–±–∑–æ—Ä

vLLM - —ç—Ç–æ –≤—ã—Å–æ–∫–æ–ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–∞—è –±–∏–±–ª–∏–æ—Ç–µ–∫–∞ –¥–ª—è –∏–Ω—Ñ–µ—Ä–µ–Ω—Å–∞ LLM —Å –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è–º–∏:
- **PagedAttention** - —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–µ —É–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –ø–∞–º—è—Ç—å—é KV-cache
- **Continuous batching** - –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–æ–µ –±–∞—Ç—á–∏—Ä–æ–≤–∞–Ω–∏–µ –∑–∞–ø—Ä–æ—Å–æ–≤
- **–û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ CUDA kernels** - –¥–æ 24x –±—ã—Å—Ç—Ä–µ–µ —á–µ–º HuggingFace
- **Tensor parallelism** - —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ –Ω–∞ –Ω–µ—Å–∫–æ–ª—å–∫–æ GPU

### GPU –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è: RTX 6000 Ada (Blackwell) 96GB

–≠—Ç–∞ GPU –∏–¥–µ–∞–ª—å–Ω–∞ –¥–ª—è vLLM –±–ª–∞–≥–æ–¥–∞—Ä—è:
- ‚úÖ 96GB VRAM - –º–æ–∂–Ω–æ –∑–∞–ø—É—Å—Ç–∏—Ç—å –º–æ–¥–µ–ª–∏ –¥–æ 70B –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤
- ‚úÖ Ada Lovelace –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ —Å —É–ª—É—á—à–µ–Ω–Ω—ã–º–∏ Tensor Cores
- ‚úÖ PCIe 4.0 x16 –¥–ª—è –±—ã—Å—Ç—Ä–æ–π –∑–∞–≥—Ä—É–∑–∫–∏ –¥–∞–Ω–Ω—ã—Ö
- ‚úÖ –ü–æ–¥–¥–µ—Ä–∂–∫–∞ FP16, BF16, INT8 –¥–ª—è –∫–≤–∞–Ω—Ç–∏–∑–∞—Ü–∏–∏

## üì¶ –ß—Ç–æ –≤–∫–ª—é—á–µ–Ω–æ

### –ù–æ–≤—ã–µ —Ñ–∞–π–ª—ã

1. **`backend/Dockerfile.vllm`** - Docker –æ–±—Ä–∞–∑ –¥–ª—è vLLM —Å–µ—Ä–≤–∏—Å–∞
2. **`docker-compose.vllm.yml`** - Compose –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è —Å vLLM
3. **`backend/services/rag_vllm.py`** - vLLM –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è
4. **`.env.vllm`** - –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –¥–ª—è vLLM
5. **`.env.ollama`** - –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –¥–ª—è Ollama (–¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è)

### –û–±–Ω–æ–≤–ª–µ–Ω–Ω—ã–µ —Ñ–∞–π–ª—ã

- **`backend/services/rag.py`** - –¥–æ–±–∞–≤–ª–µ–Ω–∞ –ø–æ–¥–¥–µ—Ä–∂–∫–∞ vLLM —Ä–µ–∂–∏–º–∞

## üöÄ –ë—ã—Å—Ç—Ä—ã–π —Å—Ç–∞—Ä—Ç –Ω–∞ GPU —Å–µ—Ä–≤–µ—Ä–µ

### –®–∞–≥ 1: –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ —Å–∏—Å—Ç–µ–º—ã

```bash
# –ü—Ä–æ–≤–µ—Ä–∫–∞ GPU
nvidia-smi

# –î–æ–ª–∂–Ω–æ –ø–æ–∫–∞–∑–∞—Ç—å:
# +-----------------------------------------------------------------------------+
# | NVIDIA-SMI 535.xx       Driver Version: 535.xx       CUDA Version: 12.2     |
# |-------------------------------+----------------------+----------------------+
# |   0  NVIDIA RTX 6000...  | GPU-Util  0%     | Memory-Usage: 0MiB / 98304MiB|
# +-------------------------------+----------------------+----------------------+

# –£—Å—Ç–∞–Ω–æ–≤–∫–∞ NVIDIA Container Toolkit (–µ—Å–ª–∏ –µ—â–µ –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω)
distribution=$(. /etc/os-release;echo $ID$VERSION_ID)
curl -s -L https://nvidia.github.io/nvidia-docker/gpgkey | sudo apt-key add -
curl -s -L https://nvidia.github.io/nvidia-docker/$distribution/nvidia-docker.list | \
  sudo tee /etc/apt/sources.list.d/nvidia-docker.list

sudo apt-get update
sudo apt-get install -y nvidia-container-toolkit
sudo systemctl restart docker

# –ü—Ä–æ–≤–µ—Ä–∫–∞ Docker + GPU
docker run --rm --gpus all nvidia/cuda:12.1.0-base-ubuntu22.04 nvidia-smi
```

### –®–∞–≥ 2: –ö–ª–æ–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ –ø—Ä–æ–µ–∫—Ç–∞

```bash
git clone https://github.com/MKukshev/quadrate-rag-pipeline-mvp.git
cd quadrate-rag-pipeline-mvp
```

### –®–∞–≥ 3: –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –¥–ª—è vLLM

```bash
# –ö–æ–ø–∏—Ä–æ–≤–∞—Ç—å –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é vLLM
cp .env.vllm .env

# –û—Ç—Ä–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞—Ç—å –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é
nano .env

# –í–ê–ñ–ù–û: –î–æ–±–∞–≤–∏—Ç—å HuggingFace —Ç–æ–∫–µ–Ω –¥–ª—è –¥–æ—Å—Ç—É–ø–∞ –∫ Llama –º–æ–¥–µ–ª—è–º
# –ü–æ–ª—É—á–∏—Ç–µ —Ç–æ–∫–µ–Ω: https://huggingface.co/settings/tokens
HUGGING_FACE_HUB_TOKEN=hf_xxxxxxxxxxxxxxxxxxxxx
```

### –®–∞–≥ 4: –ó–∞–ø—É—Å–∫ —Å vLLM

```bash
# –ó–∞–ø—É—Å–∫ –≤—Å–µ–π —Å–∏—Å—Ç–µ–º—ã —Å vLLM
docker-compose -f docker-compose.vllm.yml up -d

# –ü—Ä–æ–≤–µ—Ä–∫–∞ –ª–æ–≥–æ–≤ vLLM (–ø–µ—Ä–≤—ã–π –∑–∞–ø—É—Å–∫ —Å–∫–∞—á–∞–µ—Ç –º–æ–¥–µ–ª—å ~16GB)
docker-compose -f docker-compose.vllm.yml logs -f vllm

# –ñ–¥–µ–º —Å–æ–æ–±—â–µ–Ω–∏–µ:
# "vLLM server started successfully"
# "Available routes: [GET] /health ..."

# –ü—Ä–æ–≤–µ—Ä–∫–∞ –∑–¥–æ—Ä–æ–≤—å—è —Å–∏—Å—Ç–µ–º—ã
curl http://localhost:8000/health | jq
```

### –®–∞–≥ 5: –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ä–∞–±–æ—Ç—ã vLLM

```bash
# –ü—Ä—è–º–æ–π –∑–∞–ø—Ä–æ—Å –∫ vLLM API
curl http://localhost:8001/v1/models

# –¢–µ—Å—Ç–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏
curl http://localhost:8001/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "meta-llama/Meta-Llama-3.1-8B-Instruct",
    "messages": [{"role": "user", "content": "Hello, who are you?"}],
    "max_tokens": 100
  }'
```

### –®–∞–≥ 6: –¢–µ—Å—Ç RAG —Å vLLM

```bash
# –ò–Ω–¥–µ–∫—Å–∞—Ü–∏—è –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
docker-compose -f docker-compose.vllm.yml exec backend \
  python -m cli.index_cli --dir /app/docs --space demo

# –¢–µ—Å—Ç–æ–≤—ã–π –≤–æ–ø—Ä–æ—Å
curl -X POST http://localhost:8000/ask \
  -H "Content-Type: application/json" \
  -d '{
    "q": "–ö–∞–∫–∏–µ –¥–µ–¥–ª–∞–π–Ω—ã –ø–æ –ø—Ä–æ–µ–∫—Ç—É?",
    "space_id": "demo",
    "top_k": 5
  }' | jq
```

## ‚öôÔ∏è –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è vLLM

### –û—Å–Ω–æ–≤–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –≤ `.env.vllm`

```bash
# –ú–æ–¥–µ–ª—å –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏
VLLM_MODEL=meta-llama/Meta-Llama-3.1-8B-Instruct

# GPU Memory Utilization (0.0-1.0)
# 0.90 = –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å 90% VRAM (–æ—Å—Ç–∞–≤–∏—Ç—å 10% –¥–ª—è –Ω–∞–∫–ª–∞–¥–Ω—ã—Ö —Ä–∞—Å—Ö–æ–¥–æ–≤)
VLLM_GPU_MEMORY_UTILIZATION=0.90

# –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏–Ω–∞ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
VLLM_MAX_MODEL_LEN=8192

# Tensor Parallelism (–∫–æ–ª–∏—á–µ—Å—Ç–≤–æ GPU –¥–ª—è —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –º–æ–¥–µ–ª–∏)
# 1 = –æ–¥–Ω–∞ GPU
VLLM_TENSOR_PARALLEL_SIZE=1

# –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤
VLLM_MAX_NUM_SEQS=256
```

### –†–µ–∫–æ–º–µ–Ω–¥—É–µ–º—ã–µ –º–æ–¥–µ–ª–∏ –¥–ª—è RTX 6000 96GB

| –ú–æ–¥–µ–ª—å | –ü–∞—Ä–∞–º–µ—Ç—Ä—ã | VRAM | Context | –ö–∞—á–µ—Å—Ç–≤–æ | –°–∫–æ—Ä–æ—Å—Ç—å |
|--------|-----------|------|---------|----------|----------|
| **Llama-3.1-8B-Instruct** ‚úÖ | 8B | ~16GB | 8K | ‚≠ê‚≠ê‚≠ê‚≠ê | ‚ö°‚ö°‚ö°‚ö°‚ö° |
| **Llama-3.1-70B-Instruct** | 70B | ~80GB | 8K | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | ‚ö°‚ö°‚ö° |
| **Mistral-7B-Instruct** | 7B | ~14GB | 8K | ‚≠ê‚≠ê‚≠ê‚≠ê | ‚ö°‚ö°‚ö°‚ö°‚ö° |
| **Mixtral-8x7B-Instruct** | 47B | ~90GB | 32K | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | ‚ö°‚ö°‚ö° |
| **Qwen2.5-72B-Instruct** | 72B | ~85GB | 32K | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | ‚ö°‚ö°‚ö° |

‚úÖ **–†–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è –¥–ª—è –Ω–∞—á–∞–ª–∞**: `meta-llama/Meta-Llama-3.1-8B-Instruct`

### –ò–∑–º–µ–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏

```bash
# –í .env.vllm
VLLM_MODEL=mistralai/Mistral-7B-Instruct-v0.2

# –ü–µ—Ä–µ—Å–æ–∑–¥–∞—Ç—å vLLM –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä
docker-compose -f docker-compose.vllm.yml up -d --force-recreate vllm

# –ú–æ–¥–µ–ª—å –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ —Å–∫–∞—á–∞–µ—Ç—Å—è –ø—Ä–∏ –ø–µ—Ä–≤–æ–º –∑–∞–ø—É—Å–∫–µ
```

## üìä –°—Ä–∞–≤–Ω–µ–Ω–∏–µ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏: Ollama vs vLLM

### –ë–µ–Ω—á–º–∞—Ä–∫ –Ω–∞ Llama-3.1-8B

| –ú–µ—Ç—Ä–∏–∫–∞ | Ollama (CPU) | Ollama (GPU) | vLLM (GPU) |
|---------|--------------|--------------|------------|
| **Tokens/sec** | 5-10 | 40-60 | **150-250** |
| **Latency (first token)** | 2-5s | 0.5-1s | **0.1-0.3s** |
| **Throughput (requests/sec)** | 0.5 | 2-3 | **15-30** |
| **Batch size support** | 1 | 1-4 | **256** |
| **Memory efficiency** | Medium | Medium | **High** |

### –†–µ–∞–ª—å–Ω—ã–π –ø—Ä–∏–º–µ—Ä (1000 –∑–∞–ø—Ä–æ—Å–æ–≤)

```bash
# Ollama: ~600 —Å–µ–∫—É–Ω–¥ (10 –º–∏–Ω—É—Ç)
# vLLM:   ~40 —Å–µ–∫—É–Ω–¥ (–¥–æ 15x –±—ã—Å—Ç—Ä–µ–µ!)
```

## üîß –ü—Ä–æ–¥–≤–∏–Ω—É—Ç–∞—è –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è

### –ö–≤–∞–Ω—Ç–∏–∑–∞—Ü–∏—è –¥–ª—è –±–æ–ª—å—à–∏—Ö –º–æ–¥–µ–ª–µ–π

–ï—Å–ª–∏ –Ω—É–∂–Ω–æ –∑–∞–ø—É—Å—Ç–∏—Ç—å –º–æ–¥–µ–ª—å > 70B, –∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ –∫–≤–∞–Ω—Ç–∏–∑–∞—Ü–∏—é:

```bash
# AWQ –∫–≤–∞–Ω—Ç–∏–∑–∞—Ü–∏—è (4-bit)
VLLM_MODEL=TheBloke/Llama-2-70B-Chat-AWQ
VLLM_QUANTIZATION=awq
VLLM_GPU_MEMORY_UTILIZATION=0.95
```

### Tensor Parallelism (–µ—Å–ª–∏ –Ω–µ—Å–∫–æ–ª—å–∫–æ GPU)

–ï—Å–ª–∏ —É –≤–∞—Å 2+ GPU:

```bash
# –†–∞—Å–ø—Ä–µ–¥–µ–ª–∏—Ç—å –º–æ–¥–µ–ª—å –Ω–∞ 2 GPU
VLLM_TENSOR_PARALLEL_SIZE=2

# –í docker-compose.vllm.yml:
deploy:
  resources:
    reservations:
      devices:
        - driver: nvidia
          count: 2  # –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å 2 GPU
          capabilities: [gpu]
```

### Pipeline Parallelism

–î–ª—è –æ—á–µ–Ω—å –±–æ–ª—å—à–∏—Ö –º–æ–¥–µ–ª–µ–π (> 100B):

```bash
VLLM_PIPELINE_PARALLEL_SIZE=2
VLLM_TENSOR_PARALLEL_SIZE=2
# –ò—Ç–æ–≥–æ: 4 GPU (2x2)
```

## üéõÔ∏è –ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ –∏ –æ—Ç–ª–∞–¥–∫–∞

### –ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ GPU

```bash
# –†–µ–∞–ª—å–Ω–æ–µ –≤—Ä–µ–º—è GPU utilization
watch -n 1 nvidia-smi

# Docker stats
docker stats vllm

# vLLM –º–µ—Ç—Ä–∏–∫–∏
curl http://localhost:8001/metrics
```

### –õ–æ–≥–∏ vLLM

```bash
# –°–ª–µ–¥–∏—Ç—å –∑–∞ –ª–æ–≥–∞–º–∏
docker-compose -f docker-compose.vllm.yml logs -f vllm

# –í–∞–∂–Ω—ã–µ —Å–æ–æ–±—â–µ–Ω–∏—è:
# "GPU blocks: 1234" - –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –±–ª–æ–∫–æ–≤ –¥–ª—è KV-cache
# "Avg prompt throughput: X tokens/s"
# "Avg generation throughput: Y tokens/s"
```

### Health check

```bash
# Backend health (–≤–∫–ª—é—á–∞–µ—Ç –ø—Ä–æ–≤–µ—Ä–∫—É vLLM)
curl http://localhost:8000/health | jq

# –ü—Ä—è–º–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ vLLM
curl http://localhost:8001/health
```

## üîÑ –ü–µ—Ä–µ–∫–ª—é—á–µ–Ω–∏–µ –º–µ–∂–¥—É Ollama –∏ vLLM

### –ü–µ—Ä–µ–∫–ª—é—á–µ–Ω–∏–µ –Ω–∞ vLLM

```bash
# –û—Å—Ç–∞–Ω–æ–≤–∏—Ç—å Ollama
docker-compose down

# –ó–∞–ø—É—Å—Ç–∏—Ç—å vLLM
cp .env.vllm .env
docker-compose -f docker-compose.vllm.yml up -d
```

### –í–æ–∑–≤—Ä–∞—Ç –∫ Ollama

```bash
# –û—Å—Ç–∞–Ω–æ–≤–∏—Ç—å vLLM
docker-compose -f docker-compose.vllm.yml down

# –ó–∞–ø—É—Å—Ç–∏—Ç—å Ollama
cp .env.ollama .env
docker-compose up -d
```

### –û–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–π –∑–∞–ø—É—Å–∫ (–¥–ª—è A/B —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è)

```bash
# Ollama –Ω–∞ –ø–æ—Ä—Ç—É 8000
docker-compose up -d

# vLLM –Ω–∞ –ø–æ—Ä—Ç—É 9000
# –ò–∑–º–µ–Ω–∏—Ç—å –ø–æ—Ä—Ç –≤ docker-compose.vllm.yml:
# backend:
#   ports: ["9000:8000"]

docker-compose -f docker-compose.vllm.yml up -d
```

## üêõ Troubleshooting

### –ü—Ä–æ–±–ª–µ–º–∞: Out of Memory (OOM)

```bash
# –£–º–µ–Ω—å—à–∏—Ç—å GPU memory utilization
VLLM_GPU_MEMORY_UTILIZATION=0.80

# –ò–ª–∏ —É–º–µ–Ω—å—à–∏—Ç—å max_model_len
VLLM_MAX_MODEL_LEN=4096

# –ò–ª–∏ —É–º–µ–Ω—å—à–∏—Ç—å batch size
VLLM_MAX_NUM_SEQS=128
```

### –ü—Ä–æ–±–ª–µ–º–∞: –ú–µ–¥–ª–µ–Ω–Ω–∞—è –∑–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏

```bash
# –ú–æ–¥–µ–ª–∏ –∫—ç—à–∏—Ä—É—é—Ç—Å—è –≤ volume vllm_cache
# –ü—Ä–∏ –ø–µ—Ä–≤–æ–º –∑–∞–ø—É—Å–∫–µ —Å–∫–∞—á–∏–≤–∞–µ—Ç—Å—è ~16GB –¥–ª—è Llama-3.1-8B

# –ü—Ä–æ–≤–µ—Ä–∏—Ç—å progress:
docker-compose -f docker-compose.vllm.yml logs -f vllm

# –£—Å–∫–æ—Ä–∏—Ç—å: —Å–∫–∞—á–∞—Ç—å –º–æ–¥–µ–ª—å –∑–∞—Ä–∞–Ω–µ–µ
docker-compose -f docker-compose.vllm.yml run --rm vllm \
  huggingface-cli download meta-llama/Meta-Llama-3.1-8B-Instruct
```

### –ü—Ä–æ–±–ª–µ–º–∞: CUDA out of memory

```bash
# –£–±–µ–¥–∏—Ç–µ—Å—å —á—Ç–æ GPU –Ω–µ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –¥—Ä—É–≥–∏–º–∏ –ø—Ä–æ—Ü–µ—Å—Å–∞–º–∏
nvidia-smi

# –£–±–∏—Ç—å –ø—Ä–æ—Ü–µ—Å—Å—ã –Ω–∞ GPU
sudo fuser -v /dev/nvidia0
sudo kill -9 <PID>

# –ü–µ—Ä–µ–∑–∞–ø—É—Å—Ç–∏—Ç—å –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä
docker-compose -f docker-compose.vllm.yml restart vllm
```

### –ü—Ä–æ–±–ª–µ–º–∞: HuggingFace token invalid

```bash
# –ü—Ä–æ–≤–µ—Ä–∏—Ç—å —Ç–æ–∫–µ–Ω
echo $HUGGING_FACE_HUB_TOKEN

# –û–±–Ω–æ–≤–∏—Ç—å —Ç–æ–∫–µ–Ω –≤ .env
nano .env.vllm

# –ü–µ—Ä–µ—Å–æ–∑–¥–∞—Ç—å –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä
docker-compose -f docker-compose.vllm.yml up -d --force-recreate vllm
```

## üìà –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏

### 1. –ù–∞—Å—Ç—Ä–æ–π–∫–∞ batch size

```bash
# –ë–æ–ª—å—à–µ = –≤—ã—à–µ throughput, –Ω–æ –±–æ–ª—å—à–µ latency
VLLM_MAX_NUM_SEQS=256  # Default
VLLM_MAX_NUM_SEQS=512  # –î–ª—è –≤—ã—Å–æ–∫–æ–≥–æ QPS
VLLM_MAX_NUM_SEQS=64   # –î–ª—è –Ω–∏–∑–∫–æ–π latency
```

### 2. –í—ã–±–æ—Ä –¥–ª–∏–Ω—ã –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞

```bash
# –ú–µ–Ω—å—à–µ context = –±–æ–ª—å—à–µ batch size
VLLM_MAX_MODEL_LEN=4096   # –î–ª—è –∫–æ—Ä–æ—Ç–∫–∏—Ö –∑–∞–ø—Ä–æ—Å–æ–≤
VLLM_MAX_MODEL_LEN=8192   # Default
VLLM_MAX_MODEL_LEN=32768  # –î–ª—è –¥–ª–∏–Ω–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ (Mixtral)
```

### 3. Speculative decoding

```bash
# –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –º–∞–ª–µ–Ω—å–∫—É—é –º–æ–¥–µ–ª—å –¥–ª—è "–ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è"
VLLM_SPECULATIVE_MODEL=meta-llama/Llama-2-7b-chat-hf
# –£—Å–∫–æ—Ä–µ–Ω–∏–µ –¥–æ 2x –¥–ª—è –ø—Ä–æ—Å—Ç—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤
```

## üîê Production —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏

### 1. Monitoring

```bash
# Prometheus metrics –∏–∑ vLLM
curl http://localhost:8001/metrics

# Grafana dashboard –¥–ª—è vLLM:
# https://grafana.com/grafana/dashboards/19020-vllm-dashboard/
```

### 2. Load balancing

```bash
# Nginx –¥–ª—è –±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∫–∏ –Ω–∞ –Ω–µ—Å–∫–æ–ª—å–∫–æ vLLM –∏–Ω—Å—Ç–∞–Ω—Å–æ–≤
upstream vllm_backend {
    server vllm1:8001;
    server vllm2:8001;
    least_conn;
}
```

### 3. Auto-scaling

```bash
# Kubernetes HPA based on GPU utilization
kubectl autoscale deployment vllm \
  --cpu-percent=70 \
  --min=1 \
  --max=10
```

## üìö –ü–æ–ª–µ–∑–Ω—ã–µ —Å—Å—ã–ª–∫–∏

- [vLLM Documentation](https://docs.vllm.ai/)
- [vLLM GitHub](https://github.com/vllm-project/vllm)
- [Supported Models](https://docs.vllm.ai/en/latest/models/supported_models.html)
- [Performance Benchmarks](https://blog.vllm.ai/2023/06/20/vllm.html)

---

## üéâ –ì–æ—Ç–æ–≤–æ!

–¢–µ–ø–µ—Ä—å —É –≤–∞—Å –µ—Å—Ç—å –≤—ã—Å–æ–∫–æ–ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω—ã–π RAG-–ø–∞–π–ø–ª–∞–π–Ω —Å vLLM –Ω–∞ GPU!

**–ü—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å:**
- ‚úÖ –î–æ 250 tokens/sec
- ‚úÖ –î–æ 30 –∑–∞–ø—Ä–æ—Å–æ–≤ –≤ —Å–µ–∫—É–Ω–¥—É
- ‚úÖ Latency < 300ms –¥–ª—è first token
- ‚úÖ –ü–æ–¥–¥–µ—Ä–∂–∫–∞ continuous batching
- ‚úÖ –≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ 96GB VRAM

