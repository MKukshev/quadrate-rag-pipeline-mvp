# LLM –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è - –ü–æ–¥—Ä–æ–±–Ω–æ–µ –æ–±—ä—è—Å–Ω–µ–Ω–∏–µ

## ü§î –ó–∞—á–µ–º –¥–≤–µ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –¥–ª—è –º–æ–¥–µ–ª–∏?

### –ö—Ä–∞—Ç–∫–∏–π –æ—Ç–≤–µ—Ç
- **`LLM_MODEL`** - –¥–ª—è Backend (Python)
- **`VLLM_MODEL`** - –¥–ª—è vLLM Container

**–î–æ–ª–∂–Ω—ã –±—ã—Ç—å –æ–¥–∏–Ω–∞–∫–æ–≤—ã–º–∏ –ø—Ä–∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–∏ vLLM!**

---

## üìä –ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞: Backend + vLLM

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  docker-compose.yml / docker-compose.vllm.yml               ‚îÇ
‚îÇ                                                             ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê      ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ
‚îÇ  ‚îÇ  Backend Container   ‚îÇ      ‚îÇ   vLLM Container      ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  (FastAPI)           ‚îÇ      ‚îÇ   (Model Server)      ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ                      ‚îÇ      ‚îÇ                       ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  LLM_MODE=vllm ‚óÑ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ –ö–∞–∫–æ–π –ø—Ä–æ–≤–∞–π–¥–µ—Ä   ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  LLM_MODEL=llama... ‚óÑ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ –î–ª—è –ª–æ–≥–æ–≤/–º–µ—Ç—Ä–∏–∫  ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  LLM_VLLM_URL=... ‚óÑ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ –ì–¥–µ vLLM —Å–µ—Ä–≤–µ—Ä   ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ                      ‚îÇ      ‚îÇ                       ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ                      ‚îÇ HTTP ‚îÇ   VLLM_MODEL=llama... ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  call_vllm(prompt)‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚ñ∫–ó–∞–≥—Ä—É–∂–∞–µ—Ç —ç—Ç—É –º–æ–¥–µ–ª—å‚îÇ  ‚îÇ
‚îÇ  ‚îÇ                      ‚îÇ      ‚îÇ   VLLM_GPU_MEM=0.95   ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ                      ‚îÇ      ‚îÇ   VLLM_MAX_LEN=16K    ‚îÇ  ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò      ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ
‚îÇ           Port 8000                     Port 8001          ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

---

## üîë –ö–ª—é—á–µ–≤—ã–µ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ

### 1. –ü–µ—Ä–µ–º–µ–Ω–Ω—ã–µ —É—Ä–æ–≤–Ω—è –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è (Backend)

```python
# backend/services/config.py
LLM_MODE = "vllm"           # –ö–∞–∫–æ–π –ø—Ä–æ–≤–∞–π–¥–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å
LLM_MODEL = "meta-llama..." # –ö–∞–∫—É—é –º–æ–¥–µ–ª—å (–¥–ª—è –º–µ—Ç—Ä–∏–∫/–ª–æ–≥–æ–≤)
LLM_VLLM_URL = "http://..."# –ì–¥–µ –Ω–∞—Ö–æ–¥–∏—Ç—Å—è vLLM API
LLM_TIMEOUT = 120           # –¢–∞–π–º–∞—É—Ç –∑–∞–ø—Ä–æ—Å–∞
LLM_MAX_TOKENS = 512        # –ú–∞–∫—Å —Ç–æ–∫–µ–Ω–æ–≤ –≤ –æ—Ç–≤–µ—Ç–µ
```

**–ò—Å–ø–æ–ª—å–∑—É—é—Ç—Å—è –≤:**
- `backend/services/rag.py` - –¥–ª—è –≤—ã–±–æ—Ä–∞ –ø—Ä–æ–≤–∞–π–¥–µ—Ä–∞
- `backend/app.py` - –¥–ª—è health check
- –õ–æ–≥–∏ –∏ –º–µ—Ç—Ä–∏–∫–∏

### 2. –ü–µ—Ä–µ–º–µ–Ω–Ω—ã–µ —É—Ä–æ–≤–Ω—è –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä–∞ (vLLM)

```bash
# docker-compose.vllm.yml
VLLM_MODEL=meta-llama/...           # –ö–∞–∫—É—é –º–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∑–∏—Ç—å
VLLM_GPU_MEMORY_UTILIZATION=0.95   # –°–∫–æ–ª—å–∫–æ GPU –ø–∞–º—è—Ç–∏
VLLM_MAX_MODEL_LEN=16384            # –î–ª–∏–Ω–∞ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
VLLM_TENSOR_PARALLEL_SIZE=1         # –ö–æ–ª-–≤–æ GPU
VLLM_ENABLE_FP8=true                # FP8 precision
```

**–ò—Å–ø–æ–ª—å–∑—É—é—Ç—Å—è –≤:**
- vLLM container –ø—Ä–∏ –∑–∞–ø—É—Å–∫–µ
- –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ –∏–∑ HuggingFace
- –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è GPU

---

## üí° –ü–æ—á–µ–º—É –Ω–µ –æ–¥–Ω–∞ –ø–µ—Ä–µ–º–µ–Ω–Ω–∞—è?

### –ü–æ–ø—ã—Ç–∫–∞ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å —Ç–æ–ª—å–∫–æ LLM_MODEL

```bash
# –ï—Å–ª–∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å —Ç–æ–ª—å–∫–æ LLM_MODEL
LLM_MODEL=llama3.1:8b

# –ü—Ä–æ–±–ª–µ–º—ã:
# 1. Backend –æ—Ç–ø—Ä–∞–≤–∏—Ç "llama3.1:8b" –≤ vLLM
# 2. vLLM –ø–æ–ø—ã—Ç–∞–µ—Ç—Å—è –∑–∞–≥—Ä—É–∑–∏—Ç—å "llama3.1:8b" –∏–∑ HuggingFace
# 3. –û—à–∏–±–∫–∞: –º–æ–¥–µ–ª—å –Ω–µ –Ω–∞–π–¥–µ–Ω–∞ (—ç—Ç–æ Ollama —Ñ–æ—Ä–º–∞—Ç, –Ω–µ HF!)
```

### –ü–æ–ø—ã—Ç–∫–∞ –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –∫–æ–Ω–≤–µ—Ä—Ç–∏—Ä–æ–≤–∞—Ç—å

```python
# –¢–µ–æ—Ä–µ—Ç–∏—á–µ—Å–∫–∏ –º–æ–∂–Ω–æ:
if LLM_MODE == "vllm":
    # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä–æ–≤–∞—Ç—å Ollama —Ñ–æ—Ä–º–∞—Ç –≤ HF
    if LLM_MODEL == "llama3.1:8b":
        vllm_model = "meta-llama/Meta-Llama-3.1-8B-Instruct"
    elif LLM_MODEL == "mistral:7b":
        vllm_model = "mistralai/Mistral-7B-Instruct-v0.2"
    # ...

# ‚ùå –ü—Ä–æ–±–ª–µ–º–∞: —Å–ª–∏—à–∫–æ–º –º–Ω–æ–≥–æ –≤–∞—Ä–∏–∞–Ω—Ç–æ–≤, –ª–µ–≥–∫–æ —Å–ª–æ–º–∞—Ç—å
```

---

## ‚úÖ –†–µ–∫–æ–º–µ–Ω–¥—É–µ–º—ã–π –ø–æ–¥—Ö–æ–¥

### –í–∞—Ä–∏–∞–Ω—Ç A: –°–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∞—Ü–∏—è –≤ .env (—Ç–µ–∫—É—â–∏–π)

```bash
# .env.vllm
LLM_MODE=vllm
LLM_MODEL=meta-llama/Meta-Llama-3.1-8B-Instruct
VLLM_MODEL=meta-llama/Meta-Llama-3.1-8B-Instruct
```

**–ü—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞:**
- ‚úÖ –Ø–≤–Ω–∞—è –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è
- ‚úÖ –ü–æ–ª–Ω—ã–π –∫–æ–Ω—Ç—Ä–æ–ª—å
- ‚úÖ –ù–µ—Ç –º–∞–≥–∏–∏

**–ù–µ–¥–æ—Å—Ç–∞—Ç–∫–∏:**
- ‚ùå –ù—É–∂–Ω–æ —Å–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∏—Ä–æ–≤–∞—Ç—å –≤—Ä—É—á–Ω—É—é

### –í–∞—Ä–∏–∞–Ω—Ç B: VLLM_MODEL –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é = LLM_MODEL

```bash
# .env.vllm (—É–ø—Ä–æ—â–µ–Ω–Ω—ã–π)
LLM_MODE=vllm
LLM_MODEL=meta-llama/Meta-Llama-3.1-8B-Instruct
# VLLM_MODEL –Ω–µ —É–∫–∞–∑–∞–Ω, –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ = LLM_MODEL
```

**–†–µ–∞–ª–∏–∑–∞—Ü–∏—è –≤ docker-compose.vllm.yml:**
```yaml
environment:
  - VLLM_MODEL=${VLLM_MODEL:-${LLM_MODEL}}
```

**–ü—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞:**
- ‚úÖ –ú–µ–Ω—å—à–µ –¥—É–±–ª–∏—Ä–æ–≤–∞–Ω–∏—è
- ‚úÖ –õ–µ–≥—á–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å

**–ù–µ–¥–æ—Å—Ç–∞—Ç–∫–∏:**
- ‚ùå –ù—É–∂–Ω–∞ –ø–æ–¥–¥–µ—Ä–∂–∫–∞ –≤ docker-compose

---

## üìù –¢–µ–∫—É—â–µ–µ —Å–æ—Å—Ç–æ—è–Ω–∏–µ (–ø–æ—Å–ª–µ –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è)

### –í –≤–∞—à–∏—Ö .env —Ñ–∞–π–ª–∞—Ö:

#### `.env.ollama`
```bash
LLM_MODE=ollama
LLM_MODEL=llama3.1:8b
# VLLM_MODEL –Ω–µ –Ω—É–∂–Ω–∞
```

#### `.env.vllm`
```bash
LLM_MODE=vllm
LLM_MODEL=meta-llama/Meta-Llama-3.1-8B-Instruct  # ‚Üê Backend
VLLM_MODEL=meta-llama/Meta-Llama-3.1-8B-Instruct # ‚Üê vLLM
# –û–±–µ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –Ω—É–∂–Ω—ã –∏ –¥–æ–ª–∂–Ω—ã —Å–æ–≤–ø–∞–¥–∞—Ç—å!
```

#### `.env.vllm-fp8`
```bash
LLM_MODE=vllm
LLM_MODEL=neuralmagic/Meta-Llama-3.1-70B-Instruct-FP8  # ‚Üê Backend
VLLM_MODEL=neuralmagic/Meta-Llama-3.1-70B-Instruct-FP8 # ‚Üê vLLM
```

---

## üéØ –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è

### –î–ª—è —É–ø—Ä–æ—â–µ–Ω–∏—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è

–Ø —Ä–µ–∫–æ–º–µ–Ω–¥—É—é **–æ—Å—Ç–∞–≤–∏—Ç—å –∫–∞–∫ –µ—Å—Ç—å** (–¥–≤–µ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ), –ø–æ—Ç–æ–º—É —á—Ç–æ:

1. **–Ø–≤–Ω–æ—Å—Ç—å –ª—É—á—à–µ –Ω–µ—è–≤–Ω–æ—Å—Ç–∏** - —Å—Ä–∞–∑—É –≤–∏–¥–Ω–æ —á—Ç–æ –Ω–∞—Å—Ç—Ä–∞–∏–≤–∞–µ—Ç—Å—è
2. **–ì–∏–±–∫–æ—Å—Ç—å** - –º–æ–∂–Ω–æ —É–∫–∞–∑–∞—Ç—å —Ä–∞–∑–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è –µ—Å–ª–∏ –Ω—É–∂–Ω–æ
3. **Debugging** - –ª–µ–≥—á–µ –Ω–∞–π—Ç–∏ –ø—Ä–æ–±–ª–µ–º—É –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏
4. **–î–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è** - –ø—Ä–æ—â–µ –æ–±—ä—è—Å–Ω–∏—Ç—å —á—Ç–æ –∫—É–¥–∞ –∏–¥—ë—Ç

### –ù–æ –¥–æ–±–∞–≤–∏—Ç—å –≤–∞–ª–∏–¥–∞—Ü–∏—é

–í `backend/services/rag_vllm.py` –º–æ–∂–Ω–æ –¥–æ–±–∞–≤–∏—Ç—å –ø—Ä–æ–≤–µ—Ä–∫—É:

```python
def call_vllm(prompt: str, model: Optional[str] = None, **kwargs) -> str:
    model = model or config.LLM_MODEL
    
    # –í–∞–ª–∏–¥–∞—Ü–∏—è: –ø—Ä–æ–≤–µ—Ä–∏—Ç—å —á—Ç–æ –º–æ–¥–µ–ª—å —Å–æ–≤–ø–∞–¥–∞–µ—Ç —Å VLLM_MODEL
    vllm_model = os.getenv("VLLM_MODEL")
    if vllm_model and model != vllm_model:
        logger.warning(
            f"LLM_MODEL ({model}) != VLLM_MODEL ({vllm_model}). "
            f"Using {model} for API call."
        )
    
    # ... –æ—Å—Ç–∞–ª—å–Ω–æ–π –∫–æ–¥
```

---

## üìö –°–º. —Ç–∞–∫–∂–µ

- [VLLM_QUICKSTART.md](../VLLM_QUICKSTART.md) - –±—ã—Å—Ç—Ä—ã–π —Å—Ç–∞—Ä—Ç
- [docs/VLLM_DEPLOYMENT.md](VLLM_DEPLOYMENT.md) - —Ä–∞–∑–≤–µ—Ä—Ç—ã–≤–∞–Ω–∏–µ
- [docs/BLACKWELL_OPTIMIZATIONS.md](BLACKWELL_OPTIMIZATIONS.md) - –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏

