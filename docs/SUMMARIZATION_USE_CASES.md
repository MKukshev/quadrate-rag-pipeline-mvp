# Ğ¡ÑƒĞ¼Ğ¼Ğ°Ñ€Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ² AnythingLLM - ĞšĞ°Ğº Ğ¸ Ğ·Ğ°Ñ‡ĞµĞ¼ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ÑÑ

## ğŸ¯ ĞÑĞ½Ğ¾Ğ²Ğ½Ğ¾Ğ¹ use case

**ĞŸÑ€Ğ¾Ğ±Ğ»ĞµĞ¼Ğ°:** Ğ”Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚ ÑĞ»Ğ¸ÑˆĞºĞ¾Ğ¼ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¾Ğ¹, Ğ½Ğµ Ğ²Ğ»ĞµĞ·Ğ°ĞµÑ‚ Ğ² ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚ LLM  
**Ğ ĞµÑˆĞµĞ½Ğ¸Ğµ:** Map-Reduce ÑÑƒĞ¼Ğ¼Ğ°Ñ€Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ñ‡ĞµÑ€ĞµĞ· LangChain

---

## ğŸ” ĞšĞ°Ğº Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ĞµÑ‚ ÑÑƒĞ¼Ğ¼Ğ°Ñ€Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ

### ĞĞ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼ Map-Reduce

```javascript
// server/utils/agents/aibitat/utils/summarize.js

async function summarizeContent({ provider, model, content }) {
  // 1. Ğ Ğ°Ğ·Ğ±Ğ¸Ñ‚ÑŒ Ğ½Ğ° Ñ‡Ğ°Ğ½ĞºĞ¸ (10K Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² ĞºĞ°Ğ¶Ğ´Ñ‹Ğ¹)
  const textSplitter = new RecursiveCharacterTextSplitter({
    chunkSize: 10000,
    chunkOverlap: 500,
  });
  const docs = await textSplitter.createDocuments([content]);
  
  // 2. Ğ¡Ğ¾Ğ·Ğ´Ğ°Ñ‚ÑŒ map-reduce chain
  const chain = loadSummarizationChain(llm, {
    type: "map_reduce",
    combinePrompt: mapPromptTemplate,
  });
  
  // 3. Ğ—Ğ°Ğ¿ÑƒÑÑ‚Ğ¸Ñ‚ÑŒ ÑÑƒĞ¼Ğ¼Ğ°Ñ€Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ
  const summary = await chain.call({ input_documents: docs });
  
  return summary.text;
}
```

### Ğ’Ğ¸Ğ·ÑƒĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ°

```
Ğ˜ÑÑ…Ğ¾Ğ´Ğ½Ñ‹Ğ¹ Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚ (100,000 Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ²)
         â”‚
         â”‚ Split Ğ½Ğ° Ñ‡Ğ°Ğ½ĞºĞ¸
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Chunk 1   Chunk 2   Chunk 3   ...  Chunk 10  â”‚
â”‚  (10K tok) (10K tok) (10K tok)      (10K tok)  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â”‚ MAP: Ğ¡ÑƒĞ¼Ğ¼Ğ°Ñ€Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ ĞºĞ°Ğ¶Ğ´Ñ‹Ğ¹ Ñ‡Ğ°Ğ½Ğº Ğ¿Ğ°Ñ€Ğ°Ğ»Ğ»ĞµĞ»ÑŒĞ½Ğ¾
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Summary 1 Summary 2 Summary 3 ... Summary 10 â”‚
â”‚  (500 tok) (500 tok) (500 tok)     (500 tok)  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â”‚ REDUCE: ĞĞ±ÑŠĞµĞ´Ğ¸Ğ½Ğ¸Ñ‚ÑŒ Ğ²ÑĞµ ÑÑƒĞ¼Ğ¼Ğ°Ñ€Ğ¸
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Final Summary                                 â”‚
â”‚  (2000 Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ²)                                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**ĞŸÑ€ĞµĞ¸Ğ¼ÑƒÑ‰ĞµÑÑ‚Ğ²Ğ°:**
- âœ… ĞœĞ¾Ğ¶ĞµÑ‚ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ñ‚ÑŒ Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ñ‹ Ğ»ÑĞ±Ğ¾Ğ³Ğ¾ Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ°
- âœ… ĞŸĞ°Ñ€Ğ°Ğ»Ğ»ĞµĞ»ÑŒĞ½Ğ°Ñ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ° Ñ‡Ğ°Ğ½ĞºĞ¾Ğ² (Ğ±Ñ‹ÑÑ‚Ñ€ĞµĞµ)
- âœ… Ğ¡Ğ¾Ñ…Ñ€Ğ°Ğ½ÑĞµÑ‚ ĞºĞ»ÑÑ‡ĞµĞ²ÑƒÑ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ Ğ¸Ğ· Ğ²ÑĞµÑ… Ñ‡Ğ°ÑÑ‚ĞµĞ¹

---

## ğŸ¤– Ğ“Ğ´Ğµ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ÑÑ: Document Summarizer Plugin

### Plugin Ğ´Ğ»Ñ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ²

```javascript
// server/utils/agents/aibitat/plugins/summarize.js

const docSummarizer = {
  name: "document-summarizer",
  plugin: function () {
    return {
      setup(aibitat) {
        aibitat.function({
          name: "document-summarizer",
          description: "Can get the list of files and summarize them",
          parameters: {
            action: "list" | "summarize",
            document_filename: "example.txt"
          },
          handler: async function ({ action, document_filename }) {
            if (action === "list") {
              return await this.listDocuments();  // Ğ¡Ğ¿Ğ¸ÑĞ¾Ğº Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ²
            }
            if (action === "summarize") {
              return await this.summarizeDoc(document_filename);  // Ğ¡ÑƒĞ¼Ğ¼Ğ°Ñ€Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ
            }
          }
        });
      }
    };
  }
};
```

### Ğ›Ğ¾Ğ³Ğ¸ĞºĞ° ÑÑƒĞ¼Ğ¼Ğ°Ñ€Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸

```javascript
summarizeDoc: async function (filename) {
  // 1. ĞĞ°Ğ¹Ñ‚Ğ¸ Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚ Ğ² workspace
  const document = await Document.content(docInfo.document_id);
  
  // 2. ĞŸÑ€Ğ¾Ğ²ĞµÑ€Ğ¸Ñ‚ÑŒ Ñ€Ğ°Ğ·Ğ¼ĞµÑ€
  const tokenCount = tokenManager.countFromString(document.content);
  const contextLimit = Provider.contextLimit(provider, model);
  
  // 3. Ğ•ÑĞ»Ğ¸ Ğ²Ğ»ĞµĞ·Ğ°ĞµÑ‚ Ğ² ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚ - Ğ²ĞµÑ€Ğ½ÑƒÑ‚ÑŒ ĞºĞ°Ğº ĞµÑÑ‚ÑŒ
  if (tokenCount < contextLimit) {
    return document.content;  // ĞŸĞ¾Ğ»Ğ½Ñ‹Ğ¹ Ñ‚ĞµĞºÑÑ‚
  }
  
  // 4. Ğ•ÑĞ»Ğ¸ ĞĞ• Ğ²Ğ»ĞµĞ·Ğ°ĞµÑ‚ - ÑÑƒĞ¼Ğ¼Ğ°Ñ€Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ
  return await summarizeContent({
    provider: this.super.provider,
    model: this.super.model,
    content: document.content
  });
}
```

---

## ğŸ’¡ Use Cases Ğ´Ğ»Ñ ÑÑƒĞ¼Ğ¼Ğ°Ñ€Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸

### 1. **ĞĞ³ĞµĞ½Ñ‚ Ğ·Ğ°Ğ¿Ñ€Ğ°ÑˆĞ¸Ğ²Ğ°ĞµÑ‚ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¾Ğ¹ Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚**

**Ğ¡Ñ†ĞµĞ½Ğ°Ñ€Ğ¸Ğ¹:**
```
User: "Ğ Ğ°ÑÑĞºĞ°Ğ¶Ğ¸ Ğ¿Ñ€Ğ¾ Ğ³Ğ¾Ğ´Ğ¾Ğ²Ğ¾Ğ¹ Ğ¾Ñ‚Ñ‡ĞµÑ‚ 2024"
  â”‚
  â–¼
Agent: Uses document-summarizer tool
  â”‚
  â–¼
Tool: ĞĞ°Ñ…Ğ¾Ğ´Ğ¸Ñ‚ annual_report_2024.pdf (200 ÑÑ‚Ñ€Ğ°Ğ½Ğ¸Ñ†, 50K Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ²)
  â”‚
  â–¼
Check: 50K tokens > model context (16K)
  â”‚
  â–¼
Action: Map-Reduce ÑÑƒĞ¼Ğ¼Ğ°Ñ€Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ â†’ 2K Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ²
  â”‚
  â–¼
Agent: "Ğ“Ğ¾Ğ´Ğ¾Ğ²Ğ¾Ğ¹ Ğ¾Ñ‚Ñ‡ĞµÑ‚ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ Ñ€Ğ¾ÑÑ‚ Ğ²Ñ‹Ñ€ÑƒÑ‡ĞºĞ¸ Ğ½Ğ° 15%..."
```

### 2. **ĞĞ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ° Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ²**

**ĞŸÑ€Ğ¸Ğ¼ĞµÑ€Ñ‹:**
- Ğ¢ĞµÑ…Ğ½Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ ÑĞ¿ĞµÑ†Ğ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸ (100+ ÑÑ‚Ñ€Ğ°Ğ½Ğ¸Ñ†)
- Ğ®Ñ€Ğ¸Ğ´Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ´Ğ¾Ğ³Ğ¾Ğ²Ğ¾Ñ€Ñ‹
- ĞĞ°ÑƒÑ‡Ğ½Ñ‹Ğµ ÑÑ‚Ğ°Ñ‚ÑŒĞ¸
- ĞŸÑ€Ğ¾Ñ‚Ğ¾ĞºĞ¾Ğ»Ñ‹ ÑĞ¾Ğ²ĞµÑ‰Ğ°Ğ½Ğ¸Ğ¹ (Ğ¼Ğ½Ğ¾Ğ³Ğ¾ Ğ²ÑÑ‚Ñ€ĞµÑ‡)

### 3. **Research agent workflow**

```javascript
// ĞŸÑ€Ğ¸Ğ¼ĞµÑ€: Research Ğ°Ğ³ĞµĞ½Ñ‚
aibitat
  .agent("researcher", {
    role: "You research topics and summarize findings"
  })
  .use(AgentPlugins.webBrowsing)       // Ğ˜Ñ‰ĞµÑ‚ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ
  .use(AgentPlugins.docSummarizer);    // Ğ¡ÑƒĞ¼Ğ¼Ğ°Ñ€Ğ¸Ğ·Ğ¸Ñ€ÑƒĞµÑ‚ Ğ½Ğ°Ğ¹Ğ´ĞµĞ½Ğ½Ğ¾Ğµ

// Workflow:
// 1. Web browsing â†’ Ğ½Ğ°Ñ…Ğ¾Ğ´Ğ¸Ñ‚ 10 ÑÑ‚Ğ°Ñ‚ĞµĞ¹
// 2. Document summarizer â†’ ÑÑƒĞ¼Ğ¼Ğ°Ñ€Ğ¸Ğ·Ğ¸Ñ€ÑƒĞµÑ‚ ĞºĞ°Ğ¶Ğ´ÑƒÑ
// 3. Researcher â†’ Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑĞµÑ‚ Ğ² Ñ„Ğ¸Ğ½Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ¾Ñ‚Ñ‡ĞµÑ‚
```

---

## ğŸ“Š Map-Reduce Summarization: Ğ”ĞµÑ‚Ğ°Ğ»Ğ¸

### Ğ§Ñ‚Ğ¾ Ñ‚Ğ°ĞºĞ¾Ğµ Map-Reduce?

```
MAP Phase (Ğ¿Ğ°Ñ€Ğ°Ğ»Ğ»ĞµĞ»ÑŒĞ½Ğ¾):
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Chunk 1    â”‚â”€â”€â–º LLM â”€â”€â–º Summary 1
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Chunk 2    â”‚â”€â”€â–º LLM â”€â”€â–º Summary 2
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Chunk 3    â”‚â”€â”€â–º LLM â”€â”€â–º Summary 3
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

REDUCE Phase (Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾):
Summary 1 + Summary 2 + Summary 3
              â”‚
              â–¼ LLM
         Final Summary
```

### ĞŸÑ€Ğ¾Ğ¼Ğ¿Ñ‚ Ğ´Ğ»Ñ ÑÑƒĞ¼Ğ¼Ğ°Ñ€Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸

```javascript
const mapPrompt = `
Write a detailed summary of the following text for a research purpose:
"{text}"
SUMMARY:
`;
```

**ĞŸĞ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ñ‹:**
- `temperature: 0` - Ğ´ĞµÑ‚ĞµÑ€Ğ¼Ğ¸Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚
- `chunkSize: 10000` - Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğµ Ñ‡Ğ°Ğ½ĞºĞ¸ (Ğ½Ğ¾ Ğ²Ğ»ĞµĞ·Ğ°ÑÑ‚ Ğ² ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚)
- `chunkOverlap: 500` - ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑĞµÑ‚ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚ Ğ¼ĞµĞ¶Ğ´Ñƒ Ñ‡Ğ°Ğ½ĞºĞ°Ğ¼Ğ¸

---

## ğŸ†š ĞĞ»ÑŒÑ‚ĞµÑ€Ğ½Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğµ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ñ‹ Ğº ÑÑƒĞ¼Ğ¼Ğ°Ñ€Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸

### 1. **Refine (LangChain)**

```javascript
// ĞŸĞ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ°Ñ ÑÑƒĞ¼Ğ¼Ğ°Ñ€Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ
const chain = loadSummarizationChain(llm, {
  type: "refine"  // Ğ²Ğ¼ĞµÑÑ‚Ğ¾ map_reduce
});

// ĞŸÑ€Ğ¾Ñ†ĞµÑÑ:
// Summary 1 = summarize(Chunk 1)
// Summary 2 = refine(Summary 1, Chunk 2)
// Summary 3 = refine(Summary 2, Chunk 3)
// ...
```

**ĞŸĞ»ÑÑÑ‹:**
- Ğ‘Ğ¾Ğ»ĞµĞµ ÑĞ²ÑĞ·Ğ½Ñ‹Ğ¹ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚
- Ğ›ÑƒÑ‡ÑˆĞµ Ğ´Ğ»Ñ Ğ½Ğ°Ñ€Ñ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²

**ĞœĞ¸Ğ½ÑƒÑÑ‹:**
- ĞœĞµĞ´Ğ»ĞµĞ½Ğ½ĞµĞµ (Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ°Ñ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ°)
- Ğ”Ğ¾Ñ€Ğ¾Ğ¶Ğµ (Ğ±Ğ¾Ğ»ÑŒÑˆĞµ LLM Ğ²Ñ‹Ğ·Ğ¾Ğ²Ğ¾Ğ²)

**AnythingLLM Ğ²Ñ‹Ğ±Ñ€Ğ°Ğ» map_reduce:** Ğ±Ñ‹ÑÑ‚Ñ€ĞµĞµ Ğ¸ Ğ´ĞµÑˆĞµĞ²Ğ»Ğµ

---

### 2. **Stuff (Ğ¿Ñ€Ğ¾ÑÑ‚ĞµĞ¹ÑˆĞ¸Ğ¹)**

```javascript
const chain = loadSummarizationChain(llm, {
  type: "stuff"  // Ğ’ÑÑ‘ Ğ² Ğ¾Ğ´Ğ¸Ğ½ Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚
});
```

**Ğ Ğ°Ğ±Ğ¾Ñ‚Ğ°ĞµÑ‚ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ ĞµÑĞ»Ğ¸:** document < context limit

---

### 3. **Custom chunked approach**

Ğ‘ĞµĞ· LangChain:

```python
# Ğ’Ğ°Ñˆ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ (Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ Ğ´Ğ¾Ğ±Ğ°Ğ²Ğ¸Ñ‚ÑŒ)
async def summarize_large_document(text: str, max_tokens: int = 16000):
    """Ğ¡ÑƒĞ¼Ğ¼Ğ°Ñ€Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¾Ğ³Ğ¾ Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ° Ñ‡ĞµÑ€ĞµĞ· Ñ‡Ğ°Ğ½ĞºĞ¸Ğ½Ğ³"""
    
    # 1. Ğ Ğ°Ğ·Ğ±Ğ¸Ñ‚ÑŒ Ğ½Ğ° Ñ‡Ğ°Ğ½ĞºĞ¸
    chunks = split_into_chunks(text, chunk_size=8000)
    
    # 2. Ğ¡ÑƒĞ¼Ğ¼Ğ°Ñ€Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ ĞºĞ°Ğ¶Ğ´Ñ‹Ğ¹ Ñ‡Ğ°Ğ½Ğº
    summaries = []
    for i, chunk in enumerate(chunks):
        prompt = f"Summarize the following text (part {i+1}/{len(chunks)}):\n\n{chunk}"
        summary = await call_llm(prompt)
        summaries.append(summary)
    
    # 3. ĞĞ±ÑŠĞµĞ´Ğ¸Ğ½Ğ¸Ñ‚ÑŒ ÑÑƒĞ¼Ğ¼Ğ°Ñ€Ğ¸
    combined = "\n\n".join(summaries)
    
    # 4. Ğ¤Ğ¸Ğ½Ğ°Ğ»ÑŒĞ½Ğ°Ñ ÑÑƒĞ¼Ğ¼Ğ°Ñ€Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ (ĞµÑĞ»Ğ¸ Ğ½ÑƒĞ¶Ğ½Ğ¾)
    if count_tokens(combined) > max_tokens:
        final_prompt = f"Create a final summary from these summaries:\n\n{combined}"
        return await call_llm(final_prompt)
    
    return combined
```

---

## ğŸ¯ ĞŸÑ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ğµ Ğº Ğ²Ğ°ÑˆĞµĞ¼Ñƒ Ğ¿Ñ€Ğ¾ĞµĞºÑ‚Ñƒ

### Use Case 1: Ğ¡ÑƒĞ¼Ğ¼Ğ°Ñ€Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ´Ğ»Ñ RAG

**ĞŸÑ€Ğ¾Ğ±Ğ»ĞµĞ¼Ğ°:** Ğ£ Ğ²Ğ°Ñ ĞµÑÑ‚ÑŒ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ğµ Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ñ‹ (email threads, technical docs)

**Ğ ĞµÑˆĞµĞ½Ğ¸Ğµ:**
```python
# backend/services/summarization.py

async def summarize_for_context(chunks: List[str], query: str) -> str:
    """
    Ğ¡ÑƒĞ¼Ğ¼Ğ°Ñ€Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ñ‡Ğ°Ğ½ĞºĞ¸ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»ÑŒĞ½Ğ¾ Ğ´Ğ»Ñ Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ°
    Ğ’Ğ¼ĞµÑÑ‚Ğ¾ Ñ‚Ğ¾Ğ³Ğ¾ Ñ‡Ñ‚Ğ¾Ğ±Ñ‹ Ğ´Ğ°Ğ²Ğ°Ñ‚ÑŒ Ğ²ÑĞµ Ñ‡Ğ°Ğ½ĞºĞ¸ Ğ² LLM - Ğ´Ğ°Ñ‚ÑŒ summary
    """
    
    # 1. ĞĞ°Ğ¹Ñ‚Ğ¸ Ñ€ĞµĞ»ĞµĞ²Ğ°Ğ½Ñ‚Ğ½Ñ‹Ğµ Ñ‡Ğ°Ğ½ĞºĞ¸ (hybrid search)
    relevant_chunks = search(query, top_k=20)  # Ğ‘Ğ¾Ğ»ÑŒÑˆĞµ Ñ‡ĞµĞ¼ Ğ¾Ğ±Ñ‹Ñ‡Ğ½Ğ¾
    
    # 2. Ğ•ÑĞ»Ğ¸ Ğ²Ğ»ĞµĞ·Ğ°ÑÑ‚ Ğ² ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚ - Ğ²ĞµÑ€Ğ½ÑƒÑ‚ÑŒ ĞºĞ°Ğº ĞµÑÑ‚ÑŒ
    total_tokens = sum(count_tokens(c) for c in relevant_chunks)
    if total_tokens < context_limit:
        return relevant_chunks
    
    # 3. Ğ¡ÑƒĞ¼Ğ¼Ğ°Ñ€Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ ĞºĞ°Ğ¶Ğ´Ñ‹Ğ¹ Ñ‡Ğ°Ğ½Ğº
    summaries = []
    for chunk in relevant_chunks:
        summary = await call_llm(
            f"Summarize this text focusing on: {query}\n\nText: {chunk}"
        )
        summaries.append(summary)
    
    return summaries  # Compressed context
```

**ĞŸĞ¾Ğ»ÑŒĞ·Ğ°:**
- âœ… Ğ‘Ğ¾Ğ»ÑŒÑˆĞµ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ° Ğ² Ñ‚Ğ¾Ñ‚ Ğ¶Ğµ context window
- âœ… Ğ¤Ğ¾ĞºÑƒÑ Ğ½Ğ° Ñ€ĞµĞ»ĞµĞ²Ğ°Ğ½Ñ‚Ğ½Ğ¾Ğ¹ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸
- âœ… Ğ›ÑƒÑ‡ÑˆĞ¸Ğµ Ğ¾Ñ‚Ğ²ĞµÑ‚Ñ‹ Ğ´Ğ»Ñ ÑˆĞ¸Ñ€Ğ¾ĞºĞ¸Ñ… Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑĞ¾Ğ²

---

### Use Case 2: Ğ¡ÑƒĞ¼Ğ¼Ğ°Ñ€Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ email threads

**Ğ£ Ğ²Ğ°Ñ ĞµÑÑ‚ÑŒ:** `email_correspondence` Ñ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ñ†ĞµĞ¿Ğ¾Ñ‡ĞºĞ°Ğ¼Ğ¸

**ĞœĞ¾Ğ¶Ğ½Ğ¾ Ğ´Ğ¾Ğ±Ğ°Ğ²Ğ¸Ñ‚ÑŒ:**
```python
# backend/services/email_summarizer.py

async def summarize_email_thread(emails: List[str]) -> str:
    """
    Ğ¡ÑƒĞ¼Ğ¼Ğ°Ñ€Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ´Ğ»Ğ¸Ğ½Ğ½ÑƒÑ email Ğ¿ĞµÑ€ĞµĞ¿Ğ¸ÑĞºÑƒ
    """
    
    # 1. ĞĞ±ÑŠĞµĞ´Ğ¸Ğ½Ğ¸Ñ‚ÑŒ Ğ²ÑĞµ emails
    thread = "\n\n---EMAIL---\n\n".join(emails)
    
    # 2. ĞŸÑ€Ğ¾Ğ²ĞµÑ€Ğ¸Ñ‚ÑŒ Ñ€Ğ°Ğ·Ğ¼ĞµÑ€
    if count_tokens(thread) < 4000:
        return thread  # ĞšĞ¾Ñ€Ğ¾Ñ‚ĞºĞ°Ñ Ğ¿ĞµÑ€ĞµĞ¿Ğ¸ÑĞºĞ°
    
    # 3. Map-Reduce ÑÑƒĞ¼Ğ¼Ğ°Ñ€Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ
    # MAP: ÑÑƒĞ¼Ğ¼Ğ°Ñ€Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ ĞºĞ°Ğ¶Ğ´Ğ¾Ğµ Ğ¿Ğ¸ÑÑŒĞ¼Ğ¾
    email_summaries = []
    for i, email in enumerate(emails):
        summary = await call_llm(
            f"Summarize this email (message {i+1}):\n{email}"
        )
        email_summaries.append(f"Message {i+1}: {summary}")
    
    # REDUCE: Ğ¾Ğ±Ñ‰Ğ°Ñ ÑÑƒĞ¼Ğ¼Ğ°Ñ€Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ñ‚Ñ€ĞµĞ´Ğ°
    combined = "\n\n".join(email_summaries)
    final_summary = await call_llm(
        f"Create a summary of this email thread:\n{combined}"
    )
    
    return final_summary
```

**ĞŸÑ€Ğ¸Ğ¼ĞµÑ€:**
```
Input: 50 emails Ğ² Ñ‚Ñ€ĞµĞ´Ğµ (30K Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ²)
Output: ĞšÑ€Ğ°Ñ‚ĞºĞ°Ñ ÑÑƒĞ¼Ğ¼Ğ°Ñ€Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ (1K Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ²)

"Ğ¢Ñ€ĞµĞ´ Ğ¾Ğ±ÑÑƒĞ¶Ğ´Ğ°ĞµÑ‚ Ğ¿Ñ€Ğ¾ĞµĞºÑ‚ Ğ¼Ğ¸Ğ³Ñ€Ğ°Ñ†Ğ¸Ğ¸. ĞÑĞ½Ğ¾Ğ²Ğ½Ñ‹Ğµ Ñ‚Ğ¾Ñ‡ĞºĞ¸:
- Ğ”Ğ¶Ğ¾Ğ½ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶Ğ¸Ğ» Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ÑŒ Kubernetes (15 Ğ¼Ğ°Ñ€Ñ‚Ğ°)
- ĞšĞ¾Ğ¼Ğ°Ğ½Ğ´Ğ° ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ»Ğ° Ğ±ÑĞ´Ğ¶ĞµÑ‚ $50K (18 Ğ¼Ğ°Ñ€Ñ‚Ğ°)  
- Ğ”ĞµĞ´Ğ»Ğ°Ğ¹Ğ½ Ğ¿ĞµÑ€ĞµĞ½ĞµÑĞµĞ½ Ğ½Ğ° 1 Ğ¸ÑĞ½Ñ (22 Ğ¼Ğ°Ñ€Ñ‚Ğ°)
- Ğ¢ĞµĞºÑƒÑ‰Ğ¸Ğ¹ ÑÑ‚Ğ°Ñ‚ÑƒÑ: Ğ² Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞµ"
```

---

### Use Case 3: Tool Ğ´Ğ»Ñ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² (ĞºĞ°Ğº Ğ² AnythingLLM)

**Ğ”Ğ¾Ğ±Ğ°Ğ²Ğ¸Ñ‚ÑŒ tool "summarize-document" Ğ´Ğ»Ñ Ğ±Ğ¾Ñ‚Ğ¾Ğ²:**

```python
# backend/services/agent_tools.py

class DocumentSummarizerTool(AgentTool):
    name = "summarize-document"
    description = "Summarize a document from the knowledge base"
    
    parameters = {
        "type": "object",
        "properties": {
            "doc_id": {
                "type": "string",
                "description": "Document ID to summarize"
            }
        }
    }
    
    async def execute(self, args: dict, context: AccessContext) -> str:
        doc_id = args["doc_id"]
        
        # 1. ĞŸĞ¾Ğ»ÑƒÑ‡Ğ¸Ñ‚ÑŒ Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚ (Ñ ACL Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ĞºĞ¾Ğ¹)
        chunks = get_document_chunks(doc_id, context)
        
        if not chunks:
            return "Document not found or no access"
        
        # 2. ĞĞ±ÑŠĞµĞ´Ğ¸Ğ½Ğ¸Ñ‚ÑŒ Ñ‡Ğ°Ğ½ĞºĞ¸
        full_text = "\n\n".join([c["text"] for c in chunks])
        
        # 3. ĞŸÑ€Ğ¾Ğ²ĞµÑ€Ğ¸Ñ‚ÑŒ Ñ€Ğ°Ğ·Ğ¼ĞµÑ€
        if count_tokens(full_text) < 4000:
            return full_text  # ĞšĞ¾Ñ€Ğ¾Ñ‚ĞºĞ¸Ğ¹ Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚
        
        # 4. Ğ¡ÑƒĞ¼Ğ¼Ğ°Ñ€Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ
        summary = await self.map_reduce_summarize(full_text)
        
        return summary
    
    async def map_reduce_summarize(self, text: str) -> str:
        """Map-Reduce ÑÑƒĞ¼Ğ¼Ğ°Ñ€Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ"""
        
        # MAP
        chunks = split_text(text, chunk_size=8000)
        summaries = []
        
        for i, chunk in enumerate(chunks):
            prompt = f"Summarize part {i+1}/{len(chunks)}:\n{chunk}"
            summary = await call_llm(prompt)
            summaries.append(summary)
        
        # REDUCE
        if len(summaries) == 1:
            return summaries[0]
        
        combined = "\n\n".join(summaries)
        final = await call_llm(
            f"Create final summary:\n{combined}"
        )
        
        return final
```

**Ğ˜ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ¼:**

```
User: "@research_bot summarize the cloud migration plan"
  â”‚
  â–¼
Research Bot: Calls document-summarizer tool
  â”‚
  â–¼
Tool: 
  1. ĞĞ°Ñ…Ğ¾Ğ´Ğ¸Ñ‚ "cloud_migration_plan.pdf" (50 ÑÑ‚Ñ€Ğ°Ğ½Ğ¸Ñ†)
  2. Map-Reduce ÑÑƒĞ¼Ğ¼Ğ°Ñ€Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ
  3. Ğ’Ğ¾Ğ·Ğ²Ñ€Ğ°Ñ‰Ğ°ĞµÑ‚ summary (500 ÑĞ»Ğ¾Ğ²)
  â”‚
  â–¼
Research Bot: "Cloud migration plan summary:
  - Budget: $200K
  - Timeline: 6 months
  - Technology: Kubernetes + AWS
  - Risks: Data migration complexity
  - Team: 5 engineers needed"
```

---

## ğŸ“Š ĞšĞ¾Ğ³Ğ´Ğ° Ğ½ÑƒĞ¶Ğ½Ğ° ÑÑƒĞ¼Ğ¼Ğ°Ñ€Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ?

### âœ… ĞÑƒĞ¶Ğ½Ğ° Ğ•Ğ¡Ğ›Ğ˜:

1. **Ğ”Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ñ‹ > context limit**
   - Ğ¢ĞµÑ…Ğ½Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ ÑĞ¿ĞµÑ†Ğ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸ (100+ ÑÑ‚Ñ€Ğ°Ğ½Ğ¸Ñ†)
   - Ğ”Ğ»Ğ¸Ğ½Ğ½Ñ‹Ğµ email threads (50+ ÑĞ¾Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸Ğ¹)
   - Ğ‘Ğ¾Ğ»ÑŒÑˆĞ¸Ğµ Ğ¿Ñ€Ğ¾Ñ‚Ğ¾ĞºĞ¾Ğ»Ñ‹

2. **ĞĞ³ĞµĞ½Ñ‚Ñ‹ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ÑÑ‚ Ñ Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ°Ğ¼Ğ¸**
   - Research bot Ğ´Ğ¾Ğ»Ğ¶ĞµĞ½ "Ğ¿Ñ€Ğ¾Ñ‡Ğ¸Ñ‚Ğ°Ñ‚ÑŒ" Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚
   - Support bot Ğ½ÑƒĞ¶ĞµĞ½ overview Ğ±Ğ¾Ğ»ÑŒÑˆĞ¾Ğ³Ğ¾ FAQ
   - Analytics bot Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¾Ñ‚Ñ‡ĞµÑ‚Ñ‹

3. **ĞŸĞ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒ Ğ¿Ñ€Ğ¾ÑĞ¸Ñ‚ summary**
   - "Summarize last month's emails"
   - "Give me TL;DR of this document"
   - "What's in this 50-page spec?"

### âŒ ĞĞ• Ğ½ÑƒĞ¶Ğ½Ğ° Ğ•Ğ¡Ğ›Ğ˜:

1. **Ğ’ÑĞµ Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ñ‹ ĞºĞ¾Ñ€Ğ¾Ñ‚ĞºĞ¸Ğµ** (<4K Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ²)
2. **Ğ˜ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚Ğµ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ search** (Ğ½Ğµ full document access)
3. **RAG Ğ²Ğ¾Ğ·Ğ²Ñ€Ğ°Ñ‰Ğ°ĞµÑ‚ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ñ€ĞµĞ»ĞµĞ²Ğ°Ğ½Ñ‚Ğ½Ñ‹Ğµ Ñ‡Ğ°Ğ½ĞºĞ¸** (ÑƒĞ¶Ğµ Ñ„Ğ¸Ğ»ÑŒÑ‚Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¾)

---

## ğŸ”§ Ğ ĞµĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ´Ğ»Ñ Ğ²Ğ°ÑˆĞµĞ³Ğ¾ Ğ¿Ñ€Ğ¾ĞµĞºÑ‚Ğ°

### Ğ’Ğ°Ñ€Ğ¸Ğ°Ğ½Ñ‚ 1: ĞŸÑ€Ğ¾ÑÑ‚Ğ°Ñ ÑÑƒĞ¼Ğ¼Ğ°Ñ€Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ (Ğ±ĞµĞ· LangChain)

```python
# backend/services/summarization.py

async def summarize_text(text: str, max_summary_tokens: int = 500) -> str:
    """
    ĞŸÑ€Ğ¾ÑÑ‚Ğ°Ñ ÑÑƒĞ¼Ğ¼Ğ°Ñ€Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ñ‡ĞµÑ€ĞµĞ· LLM
    """
    prompt = f"""Summarize the following text concisely in {max_summary_tokens} tokens or less:

{text}

SUMMARY:"""
    
    return await call_llm(prompt)


async def summarize_long_text(text: str, chunk_size: int = 8000) -> str:
    """
    Map-Reduce ÑÑƒĞ¼Ğ¼Ğ°Ñ€Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ´Ğ»Ñ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²
    """
    tokens = count_tokens(text)
    
    # Ğ•ÑĞ»Ğ¸ ĞºĞ¾Ñ€Ğ¾Ñ‚ĞºĞ¸Ğ¹ - Ğ¿Ñ€ÑĞ¼Ğ°Ñ ÑÑƒĞ¼Ğ¼Ğ°Ñ€Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ
    if tokens < chunk_size:
        return await summarize_text(text)
    
    # MAP: Ñ€Ğ°Ğ·Ğ±Ğ¸Ñ‚ÑŒ Ğ¸ ÑÑƒĞ¼Ğ¼Ğ°Ñ€Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ
    chunks = split_text_by_tokens(text, chunk_size)
    summaries = []
    
    for i, chunk in enumerate(chunks):
        print(f"Summarizing chunk {i+1}/{len(chunks)}...")
        summary = await summarize_text(chunk, max_summary_tokens=300)
        summaries.append(summary)
    
    # REDUCE: Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½Ğ¸Ñ‚ÑŒ ÑÑƒĞ¼Ğ¼Ğ°Ñ€Ğ¸
    combined = "\n\n".join(summaries)
    
    # Ğ¤Ğ¸Ğ½Ğ°Ğ»ÑŒĞ½Ğ°Ñ ÑÑƒĞ¼Ğ¼Ğ°Ñ€Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ ĞµÑĞ»Ğ¸ Ğ½ÑƒĞ¶Ğ½Ğ¾
    if count_tokens(combined) > chunk_size:
        return await summarize_text(combined, max_summary_tokens=800)
    
    return combined
```

**Ğ’Ñ€ĞµĞ¼Ñ Ñ€ĞµĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸:** 1-2 Ğ´Ğ½Ñ  
**Ğ—Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ğ¾ÑÑ‚Ğ¸:** ĞĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ñ… (Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµĞ¼ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğ¹ LLM)

---

### Ğ’Ğ°Ñ€Ğ¸Ğ°Ğ½Ñ‚ 2: Ğ¡ LangChain (Ğ±Ğ¾Ğ»ĞµĞµ Ğ¿Ñ€Ğ¾Ğ´Ğ²Ğ¸Ğ½ÑƒÑ‚Ñ‹Ğ¹)

```python
# backend/services/summarization_langchain.py

from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.chains.summarize import load_summarize_chain
from langchain.docstore.document import Document
from langchain_community.llms import Ollama  # Ğ¸Ğ»Ğ¸ vLLM

async def summarize_with_langchain(text: str) -> str:
    """
    Map-Reduce ÑÑƒĞ¼Ğ¼Ğ°Ñ€Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ñ‡ĞµÑ€ĞµĞ· LangChain
    """
    # 1. LLM
    llm = Ollama(base_url="http://ollama:11434", model="llama3.1:8b")
    
    # 2. Text splitter
    splitter = RecursiveCharacterTextSplitter(
        chunk_size=10000,
        chunk_overlap=500,
        separators=["\n\n", "\n", ". ", " "]
    )
    
    # 3. Ğ Ğ°Ğ·Ğ±Ğ¸Ñ‚ÑŒ Ğ½Ğ° Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ñ‹
    docs = splitter.create_documents([text])
    
    # 4. Ğ¡ÑƒĞ¼Ğ¼Ğ°Ñ€Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ chain
    chain = load_summarize_chain(
        llm,
        chain_type="map_reduce",
        verbose=True
    )
    
    # 5. Ğ—Ğ°Ğ¿ÑƒÑÑ‚Ğ¸Ñ‚ÑŒ
    result = chain.run(docs)
    
    return result
```

**Ğ’Ñ€ĞµĞ¼Ñ Ñ€ĞµĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸:** 2-3 Ğ´Ğ½Ñ  
**Ğ—Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ğ¾ÑÑ‚Ğ¸:** `langchain`, `langchain-community`

**ĞŸÑ€ĞµĞ¸Ğ¼ÑƒÑ‰ĞµÑÑ‚Ğ²Ğ° LangChain Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ°:**
- âœ… Ğ“Ğ¾Ñ‚Ğ¾Ğ²Ñ‹Ğ¹ map-reduce
- âœ… ĞĞ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚Ñ‹
- âœ… Retry logic
- âœ… Streaming support

**ĞĞµĞ´Ğ¾ÑÑ‚Ğ°Ñ‚ĞºĞ¸:**
- âŒ ĞĞ¾Ğ²Ğ°Ñ Ğ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ğ¾ÑÑ‚ÑŒ
- âŒ Ğ‘Ğ¾Ğ»ÑŒÑˆĞµ overhead

---

## ğŸ’¡ Ğ ĞµĞºĞ¾Ğ¼ĞµĞ½Ğ´Ğ°Ñ†Ğ¸Ğ¸

### Ğ”Ğ»Ñ Ğ²Ğ°ÑˆĞµĞ³Ğ¾ Ğ¿Ñ€Ğ¾ĞµĞºÑ‚Ğ°:

#### Ğ­Ñ‚Ğ°Ğ¿ 1: ĞŸÑ€Ğ¾ÑÑ‚Ğ°Ñ ÑÑƒĞ¼Ğ¼Ğ°Ñ€Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ (1-2 Ğ´Ğ½Ñ)

```python
# Ğ”Ğ¾Ğ±Ğ°Ğ²Ğ¸Ñ‚ÑŒ Ğ² backend/services/summarization.py
async def summarize_long_text(text: str) -> str:
    # Map-Reduce Ğ±ĞµĞ· LangChain
```

**Ğ˜ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ:**
```python
# Ğ’ RAG pipeline
if count_tokens(document) > context_limit:
    document = await summarize_long_text(document)

# RAG Ñ ÑÑƒĞ¼Ğ¼Ğ°Ñ€Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¼ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ¼
answer = call_llm(build_prompt(document, question))
```

---

#### Ğ­Ñ‚Ğ°Ğ¿ 2: Agent tool (2 Ğ´Ğ½Ñ)

```python
# Ğ”Ğ¾Ğ±Ğ°Ğ²Ğ¸Ñ‚ÑŒ Ğ² agent_tools.py
class SummarizeTool(AgentTool):
    name = "summarize-document"
    
    async def execute(self, args, context):
        doc_id = args["doc_id"]
        # Ğ¡ÑƒĞ¼Ğ¼Ğ°Ñ€Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ñ ACL Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ĞºĞ¾Ğ¹
```

**Ğ˜ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ:**
```
User: "@research_bot summarize migration_plan.pdf"
Bot: Uses summarize-document tool
Bot: "Summary: ... [500 words]"
```

---

#### Ğ­Ñ‚Ğ°Ğ¿ 3: LangChain integration (Ğ¾Ğ¿Ñ†Ğ¸Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½Ğ¾, 2-3 Ğ´Ğ½Ñ)

Ğ•ÑĞ»Ğ¸ Ğ¿Ğ¾Ğ½Ğ°Ğ´Ğ¾Ğ±Ğ¸Ñ‚ÑÑ Ğ±Ğ¾Ğ»ĞµĞµ Ğ¿Ñ€Ğ¾Ğ´Ğ²Ğ¸Ğ½ÑƒÑ‚Ğ°Ñ ÑÑƒĞ¼Ğ¼Ğ°Ñ€Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ:
- Refine chain Ğ´Ğ»Ñ Ğ½Ğ°Ñ€Ñ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²
- Custom Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚Ñ‹ Ğ´Ğ»Ñ Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… Ñ‚Ğ¸Ğ¿Ğ¾Ğ² Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ²
- Streaming ÑÑƒĞ¼Ğ¼Ğ°Ñ€Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸

---

## ğŸ“ˆ ĞŸÑ€Ğ¸Ğ¼ĞµÑ€Ñ‹ Ğ¸Ğ· Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ¶Ğ¸Ğ·Ğ½Ğ¸

### ĞŸÑ€Ğ¸Ğ¼ĞµÑ€ 1: Email thread ÑÑƒĞ¼Ğ¼Ğ°Ñ€Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ

**Input:** 
```
Email thread: "Project Alpha Discussion" (30 emails, 15K tokens)
- ĞÑ‚: John, Alice, Bob, Charlie
- ĞŸĞµÑ€Ğ¸Ğ¾Ğ´: March 1-30, 2024
```

**Output (ÑÑƒĞ¼Ğ¼Ğ°Ñ€Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ):**
```
Summary of "Project Alpha Discussion":

Key Decisions:
â€¢ Budget approved: $150K (March 5)
â€¢ Technology stack: React + Python (March 12)
â€¢ Timeline: 6 months, deadline Sept 1 (March 18)

Action Items:
â€¢ John: Hire 2 developers (by March 31)
â€¢ Alice: Finalize architecture (by April 15)
â€¢ Bob: Setup CI/CD (by April 30)

Risks Identified:
â€¢ Tight timeline mentioned by Charlie (March 22)
â€¢ Potential vendor lock-in with AWS (March 25)

Status: In planning phase, team assembled
```

**ĞŸĞ¾Ğ»ÑŒĞ·Ğ°:**
- ĞŸĞ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒ Ğ²Ğ¸Ğ´Ğ¸Ñ‚ ÑÑƒÑ‚ÑŒ Ğ·Ğ° ÑĞµĞºÑƒĞ½Ğ´Ñ‹
- ĞĞ³ĞµĞ½Ñ‚ Ğ¼Ğ¾Ğ¶ĞµÑ‚ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ÑŒ summary Ğ´Ğ»Ñ Ğ¿Ñ€Ğ¸Ğ½ÑÑ‚Ğ¸Ñ Ñ€ĞµÑˆĞµĞ½Ğ¸Ğ¹
- Ğ­ĞºĞ¾Ğ½Ğ¾Ğ¼Ğ¸Ñ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ´Ğ»Ñ LLM

---

### ĞŸÑ€Ğ¸Ğ¼ĞµÑ€ 2: Technical documentation

**Input:**
```
Document: "API Reference v2.0" (200 pages, 80K tokens)
```

**Map-Reduce Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑ:**
```
MAP:
  Chunk 1 (pages 1-20)   â†’ Summary: "Authentication section describes OAuth2..."
  Chunk 2 (pages 21-40)  â†’ Summary: "REST endpoints for user management..."
  Chunk 3 (pages 41-60)  â†’ Summary: "WebSocket API for real-time..."
  ... (10 chunks total)

REDUCE:
  All 10 summaries â†’ "API v2.0 provides OAuth2 authentication, 
                      REST endpoints for CRUD, WebSocket for real-time,
                      rate limiting 1000 req/min, supports JSON/XML..."
```

**Output:** 2K Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ²Ğ¼ĞµÑÑ‚Ğ¾ 80K

---

### ĞŸÑ€Ğ¸Ğ¼ĞµÑ€ 3: Multi-document research

**Ğ—Ğ°Ğ¿Ñ€Ğ¾Ñ:** "Summarize all cloud migration documents"

**ĞŸÑ€Ğ¾Ñ†ĞµÑÑ:**
```python
# 1. ĞĞ°Ğ¹Ñ‚Ğ¸ Ğ²ÑĞµ Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ Ñ‚ĞµĞ¼Ğµ
docs = search("cloud migration", doc_types=["technical_docs", "work_plans"])

# 2. Ğ¡ÑƒĞ¼Ğ¼Ğ°Ñ€Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ ĞºĞ°Ğ¶Ğ´Ñ‹Ğ¹
summaries = []
for doc in docs:
    summary = await summarize_document(doc.id)
    summaries.append({
        "doc_id": doc.id,
        "title": doc.title,
        "summary": summary
    })

# 3. ĞĞ±ÑŠĞµĞ´Ğ¸Ğ½Ğ¸Ñ‚ÑŒ Ğ² Ñ„Ğ¸Ğ½Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ report
report = await call_llm(f"""
Create a comprehensive report about cloud migration based on these summaries:

{json.dumps(summaries, indent=2)}

Focus on: timeline, budget, technologies, risks.
""")

return report
```

---

## ğŸ¯ Ğ˜Ğ¢ĞĞ“ĞĞ’ĞĞ• Ğ—ĞĞšĞ›Ğ®Ğ§Ğ•ĞĞ˜Ğ•

### ĞšĞ°Ğº Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ÑÑ ÑÑƒĞ¼Ğ¼Ğ°Ñ€Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ² AnythingLLM?

**ĞÑ‚Ğ²ĞµÑ‚:**

1. **Agent tool** - Ğ±Ğ¾Ñ‚Ñ‹ Ğ¼Ğ¾Ğ³ÑƒÑ‚ ÑÑƒĞ¼Ğ¼Ğ°Ñ€Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ñ‹
2. **Fallback Ğ´Ğ»Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ²** - ĞµÑĞ»Ğ¸ Ğ½Ğµ Ğ²Ğ»ĞµĞ·Ğ°ĞµÑ‚ Ğ² ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚
3. **Map-Reduce Ñ‡ĞµÑ€ĞµĞ· LangChain** - Ğ¿Ğ°Ñ€Ğ°Ğ»Ğ»ĞµĞ»ÑŒĞ½Ğ°Ñ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ° Ñ‡Ğ°Ğ½ĞºĞ¾Ğ²

### Ğ—Ğ°Ñ‡ĞµĞ¼ Ğ½ÑƒĞ¶Ğ½Ğ° ÑÑƒĞ¼Ğ¼Ğ°Ñ€Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ?

**3 Ğ¾ÑĞ½Ğ¾Ğ²Ğ½Ñ‹Ğµ Ğ¿Ñ€Ğ¸Ñ‡Ğ¸Ğ½Ñ‹:**

1. **Context window limits** - Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚ > model context
2. **Agent workflows** - Ğ±Ğ¾Ñ‚Ñƒ Ğ½ÑƒĞ¶ĞµĞ½ overview Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ°
3. **User experience** - "Ğ´Ğ°Ğ¹ Ğ¼Ğ½Ğµ TL;DR"

### Ğ¡Ñ‚Ğ¾Ğ¸Ñ‚ Ğ»Ğ¸ Ğ´Ğ¾Ğ±Ğ°Ğ²Ğ»ÑÑ‚ÑŒ Ğ² Ğ²Ğ°Ñˆ Ğ¿Ñ€Ğ¾ĞµĞºÑ‚?

**ĞÑ‚Ğ²ĞµÑ‚:** âœ… **Ğ”Ğ, Ğ½Ğ¾ ÑĞ½Ğ°Ñ‡Ğ°Ğ»Ğ° Ğ¿Ñ€Ğ¾ÑÑ‚Ğ¾Ğ¹ Ğ²Ğ°Ñ€Ğ¸Ğ°Ğ½Ñ‚**

**ĞŸĞ»Ğ°Ğ½:**

1. **Ğ¡ĞµĞ¹Ñ‡Ğ°Ñ (1-2 Ğ´Ğ½Ñ):** ĞŸÑ€Ğ¾ÑÑ‚Ğ°Ñ map-reduce Ğ±ĞµĞ· LangChain
   ```python
   async def summarize_long_text(text: str) -> str:
       # Custom implementation
   ```

2. **ĞŸĞ¾Ñ‚Ğ¾Ğ¼ (2 Ğ´Ğ½Ñ):** Agent tool Ğ´Ğ»Ñ Ğ±Ğ¾Ñ‚Ğ¾Ğ²
   ```python
   class SummarizeTool(AgentTool):
       # Tool Ğ´Ğ»Ñ ÑÑƒĞ¼Ğ¼Ğ°Ñ€Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸
   ```

3. **ĞĞ¿Ñ†Ğ¸Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½Ğ¾:** LangChain integration ĞµÑĞ»Ğ¸ Ğ¿Ğ¾Ğ½Ğ°Ğ´Ğ¾Ğ±Ğ¸Ñ‚ÑÑ

**ĞŸĞ¾Ğ»ÑŒĞ·Ğ°:** â­â­â­â­ Ğ’Ñ‹ÑĞ¾ĞºĞ°Ñ (Ğ¾ÑĞ¾Ğ±ĞµĞ½Ğ½Ğ¾ Ğ´Ğ»Ñ email threads Ğ¸ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… docs)

**Ğ”Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚:** `docs/SUMMARIZATION_USE_CASES.md`
