# üöÄ –ë—ã—Å—Ç—Ä—ã–π —Å—Ç–∞—Ä—Ç: –†–∞–∑–≤–µ—Ä—Ç—ã–≤–∞–Ω–∏–µ –Ω–∞ GPU —Å–µ—Ä–≤–µ—Ä–µ

## ‚ö° –í 3 —à–∞–≥–∞

### 1Ô∏è‚É£ –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ —Å–µ—Ä–≤–µ—Ä–∞

```bash
# –ü—Ä–æ–≤–µ—Ä–∏—Ç—å GPU
nvidia-smi

# –£—Å—Ç–∞–Ω–æ–≤–∏—Ç—å Docker + NVIDIA Container Toolkit
curl -fsSL https://get.docker.com | sh
distribution=$(. /etc/os-release;echo $ID$VERSION_ID)
curl -s -L https://nvidia.github.io/nvidia-docker/gpgkey | sudo apt-key add -
curl -s -L https://nvidia.github.io/nvidia-docker/$distribution/nvidia-docker.list | sudo tee /etc/apt/sources.list.d/nvidia-docker.list
sudo apt-get update && sudo apt-get install -y nvidia-container-toolkit
sudo systemctl restart docker

# –ü—Ä–æ–≤–µ—Ä–∏—Ç—å GPU –≤ Docker
docker run --rm --gpus all nvidia/cuda:12.6.1-base-ubuntu22.04 nvidia-smi
```

### 2Ô∏è‚É£ –ö–ª–æ–Ω–∏—Ä–æ–≤–∞—Ç—å –∏ –Ω–∞—Å—Ç—Ä–æ–∏—Ç—å

```bash
# –ö–ª–æ–Ω–∏—Ä–æ–≤–∞—Ç—å —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏–π
git clone https://github.com/your-org/ai-assistant-mvp.git
cd ai-assistant-mvp

# –°–æ–∑–¥–∞—Ç—å .env
cp .env.vllm .env
nano .env  # –î–æ–±–∞–≤–∏—Ç—å HUGGING_FACE_HUB_TOKEN=hf_xxxxx
```

### 3Ô∏è‚É£ –ó–∞–ø—É—Å—Ç–∏—Ç—å

```bash
# –ó–∞–ø—É—Å—Ç–∏—Ç—å –≤–µ—Å—å —Å—Ç–µ–∫
make up-vllm

# –î–æ–∂–¥–∞—Ç—å—Å—è –≥–æ—Ç–æ–≤–Ω–æ—Å—Ç–∏
make wait

# –ü—Ä–æ–≤–µ—Ä–∏—Ç—å
curl http://localhost:8000/health
```

‚úÖ **–ì–æ—Ç–æ–≤–æ!** API –¥–æ—Å—Ç—É–ø–µ–Ω –Ω–∞ `http://localhost:8000`

---

## üéØ –í—ã–±–æ—Ä –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏

### RTX 6000 Blackwell 96GB (–†–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è)

```bash
# –í–∞—Ä–∏–∞–Ω—Ç A: Llama-8B (–±—ã—Å—Ç—Ä–æ)
cp .env.vllm .env
make up-vllm
# ‚Üí 300-400 tokens/sec

# –í–∞—Ä–∏–∞–Ω—Ç B: Llama-70B FP8 (–∫–∞—á–µ—Å—Ç–≤–æ) 
cp .env.vllm-fp8 .env
nano .env  # –î–æ–±–∞–≤–∏—Ç—å HF —Ç–æ–∫–µ–Ω
make up-vllm
# ‚Üí 200-300 tokens/sec, –≤–ª–µ–∑–∞–µ—Ç –≤ 96GB!
```

### –î—Ä—É–≥–∏–µ GPU (RTX 4090, A100, H100)

```bash
# 1. –°–æ–∑–¥–∞—Ç—å .env
cp .env.vllm .env

# 2. –ù–∞—Å—Ç—Ä–æ–∏—Ç—å –¥–ª—è –≤–∞—à–µ–≥–æ GPU
nano .env

# –ü—Ä–∏–º–µ—Ä –¥–ª—è RTX 4090 (24GB):
VLLM_MODEL=meta-llama/Meta-Llama-3.1-8B-Instruct
VLLM_GPU_MEMORY_UTILIZATION=0.9
VLLM_MAX_MODEL_LEN=8192

# 3. –ó–∞–ø—É—Å—Ç–∏—Ç—å
make up-vllm
```

---

## üìä –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ä–∞–±–æ—Ç—ã

### 1. Health Check

```bash
curl http://localhost:8000/health | jq

# –î–æ–ª–∂–µ–Ω –≤–µ—Ä–Ω—É—Ç—å:
# {
#   "status": "ok",
#   "qdrant": "connected",
#   "llm": "vllm",
#   "model": "Meta-Llama-3.1-8B-Instruct"
# }
```

### 2. –ò–Ω–¥–µ–∫—Å–∞—Ü–∏—è –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤

```bash
# –ò–Ω–¥–µ–∫—Å–∏—Ä–æ–≤–∞—Ç—å –ø—Ä–∏–º–µ—Ä—ã –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
make ingest SPACE=production

# –ò–ª–∏ —á–µ—Ä–µ–∑ API
curl -X POST http://localhost:8000/ingest \
  -F "file=@document.pdf" \
  -F "space_id=production"
```

### 3. –¢–µ—Å—Ç–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å

```bash
curl -X POST http://localhost:8000/ask \
  -H "Content-Type: application/json" \
  -d '{
    "q": "–ö–∞–∫–∏–µ –¥–µ–¥–ª–∞–π–Ω—ã —É –ø—Ä–æ–µ–∫—Ç–∞?",
    "space_id": "production",
    "top_k": 6
  }' | jq
```

---

## üîç –ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥

### –ü—Ä–æ–≤–µ—Ä–∫–∞ GPU

```bash
# –í —Ä–µ–∞–ª—å–Ω–æ–º –≤—Ä–µ–º–µ–Ω–∏
watch -n 1 nvidia-smi

# –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –ø–∞–º—è—Ç–∏, –∑–∞–≥—Ä—É–∑–∫–∞, —Ç–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∞
```

### –ü—Ä–æ–≤–µ—Ä–∫–∞ –º–µ—Ç—Ä–∏–∫

```bash
# –¢–æ–∫–µ–Ω/—Å–µ–∫, –∑–∞–¥–µ—Ä–∂–∫–∞, cache hit rate
curl http://localhost:8000/metrics | jq

# –õ–æ–≥–∏ vLLM
make logs-vllm
```

---

## üéõÔ∏è –ü–æ–ª–µ–∑–Ω—ã–µ –∫–æ–º–∞–Ω–¥—ã

```bash
# –ó–∞–ø—É—Å–∫
make up-vllm

# –û—Å—Ç–∞–Ω–æ–≤–∫–∞
make down-vllm

# –õ–æ–≥–∏
make logs-vllm

# –ü–µ—Ä–µ–∑–∞–ø—É—Å–∫
docker compose -f docker-compose.vllm.yml restart

# –û–±–Ω–æ–≤–∏—Ç—å –º–æ–¥–µ–ª—å
nano .env  # –ò–∑–º–µ–Ω–∏—Ç—å VLLM_MODEL
make down-vllm && make up-vllm
```

---

## ‚öôÔ∏è –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏

### –î–ª—è Blackwell

```bash
# –í .env –¥–æ–±–∞–≤–∏—Ç—å –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏:
VLLM_ENABLE_FP8=true
VLLM_USE_FLASHINFER=true
VLLM_ENABLE_CHUNKED_PREFILL=true
VLLM_MAX_NUM_SEQS=512
VLLM_GPU_MEMORY_UTILIZATION=0.95
```

### –î–ª—è –¥—Ä—É–≥–∏—Ö GPU

```bash
# –í backend/.env –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞—Ç—å –∫–æ–Ω—Ç–µ–∫—Å—Ç:
CONTEXT_MAX_CHUNKS=8
CONTEXT_SNIPPET_MAX_CHARS=800
```

---

## üö® Troubleshooting

### –ü—Ä–æ–±–ª–µ–º–∞: "CUDA out of memory"

```bash
# –£–º–µ–Ω—å—à–∏—Ç—å memory utilization
VLLM_GPU_MEMORY_UTILIZATION=0.85
```

### –ü—Ä–æ–±–ª–µ–º–∞: "Docker GPU not found"

```bash
sudo systemctl restart docker
docker run --rm --gpus all nvidia/cuda:12.6.1-base nvidia-smi
```

### –ü—Ä–æ–±–ª–µ–º–∞: "–ú–µ–¥–ª–µ–Ω–Ω–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è"

```bash
# –í–∫–ª—é—á–∏—Ç—å –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏
VLLM_USE_FLASHINFER=true
VLLM_ENABLE_CHUNKED_PREFILL=true

# –£–º–µ–Ω—å—à–∏—Ç—å –∫–æ–Ω—Ç–µ–∫—Å—Ç
CONTEXT_MAX_CHUNKS=4
```

---

## üìö –î–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è

- [–ü–æ–ª–Ω–æ–µ —Ä—É–∫–æ–≤–æ–¥—Å—Ç–≤–æ –ø–æ —Ä–∞–∑–≤–µ—Ä—Ç—ã–≤–∞–Ω–∏—é](docs/GPU_DEPLOYMENT_GUIDE.md)
- [Blackwell –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏](docs/BLACKWELL_OPTIMIZATIONS.md)
- [vLLM —Ä–∞–∑–≤–µ—Ä—Ç—ã–≤–∞–Ω–∏–µ](docs/VLLM_DEPLOYMENT.md)
- [–†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –ø–æ –∏–Ω—Ñ—Ä–∞—Å—Ç—Ä—É–∫—Ç—É—Ä–µ](deploy_recommendations.md)

---

## üéØ Production Checklist

–ü–µ—Ä–µ–¥ –∑–∞–ø—É—Å–∫–æ–º –≤ production:

- [ ] Firewall –Ω–∞—Å—Ç—Ä–æ–µ–Ω
- [ ] SSL/TLS —á–µ—Ä–µ–∑ nginx
- [ ] –ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ –Ω–∞—Å—Ç—Ä–æ–µ–Ω
- [ ] –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –Ω–∞—Å—Ç—Ä–æ–µ–Ω–æ
- [ ] Backup –Ω–∞—Å—Ç—Ä–æ–µ–Ω
- [ ] –¢–µ—Å—Ç—ã –ø—Ä–æ–≤–µ–¥–µ–Ω—ã
