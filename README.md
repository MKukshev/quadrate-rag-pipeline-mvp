
# AI Assistant MVP (Offline)

Локальный RAG: Ingestion → Index (Qdrant + BM25) → Hybrid Search → RAG через локальную LLM (Ollama/др.).

## Быстрый старт

```bash
make up           # qdrant + ollama + backend
make wait         # дождаться готовности backend (health)
make pull-model   # скачайте локальную модель (MODEL=llama3.1:8b по умолчанию)
make ingest       # проиндексируйте примеры из ./docs (SPACE=space_demo)
make ask          # тестовый вопрос к RAG
make health       # проверить состояние сервисов (backend/embeddings/llm/qdrant)
```

Откройте Swagger: http://localhost:8000/docs

## Структура
- `backend/` — FastAPI и сервисы (парсинг, чанк, эмбеддинги, Qdrant, BM25, RRF, RAG)
- `cli/` — пакетная индексация каталога
- `docs/` — примеры документов для индексации (поддерживаются подпапки‑категории)

## Примечания
- Всё офлайн. Без внешних API.
- Эмбеддинги: Sentence-Transformers (`all-MiniLM-L6-v2`).
- Векторный поиск: Qdrant (HNSW, фильтры по `space_id` и `doc_type`).
- Ключевой поиск: Whoosh (BM25F) с полем `doc_type`.
- LLM Ollama: параметры (`LLM_MODEL`, `LLM_TIMEOUT`, `LLM_MAX_TOKENS`, `LLM_STREAM_ENABLED`) настраиваются через переменные окружения / `.env`.

## Категории документов и классификация
- Категории: `email_correspondence`, `messenger_correspondence`, `presentations`, `protocols`, `technical_docs`, `work_plans`, `unstructured`.
- Лучше складывать документы по подпапкам `./docs/<category>/...`. Для файлов в корне `./docs` сработает эвристическая классификация (ключевые слова + MiniLM + логрегрессия, если модель обучена).
- CLI (`make ingest` → `python -m cli.index_cli`) и API `/ingest` сохраняют `doc_type` в Qdrant/Whoosh. Результаты `/search` и `/ask` содержат `doc_type` в payload/source.
- Укажите `doc_type` явно при загрузке (`multipart/form-data` поле `doc_type`). Если не указано — вызовется `services.categories.guess_doc_type`.
- Переменная `DOC_TYPE_MODEL_PATH` (см. `.env.example`) указывает на pickle с классификатором (логистическая регрессия по эмбеддингам MiniLM). Если файла нет, используется только правило‑база.
- Контекст ограничен параметрами `CONTEXT_MAX_CHUNKS` (по умолчанию 6) и `CHUNK_TOKENS` (по умолчанию 400) — можно корректировать через `.env`.
- MMR включён по умолчанию (`MMR_ENABLED`), управляется параметрами `MMR_LAMBDA` (по умолчанию 0.7) и `MMR_CANDIDATE_MULTIPLIER` (сколько кандидатов брать перед диверсификацией).
- Кэширование запросов (`CACHE_ENABLED`, `CACHE_TTL_SECONDS`, `CACHE_MAX_ITEMS`) снижает повторные задержки.
- По умолчанию используется один чанк на документ (`ONE_CHUNK_PER_DOC=true`); при нехватке контекста система автоматически добавляет другие результаты.

### Настройка окружения
| Переменная | По умолчанию | Описание | Изменение параметра |
|------------|--------------|----------|--------------------|
| `LLM_MODE` | `ollama` | Провайдер LLM (`ollama`, `none`, подготовка к `vllm`/`tgi`). | Переключение на `vllm/tgi` ускоряет генерацию (нужен GPU); `none` выключает ответы. Диапазон: `ollama` / `vllm` / `tgi` / `none`. |
| `LLM_MODEL` | `llama3.1:8b` | Имя модели для генерации ответов. | Более крупная модель = качество↑, задержка/память↑; более лёгкая = быстрее, но точность↓. Диапазон: любая модель, поддерживаемая выбранным провайдером. |
| `LLM_TIMEOUT` | `240` | Таймаут запроса к LLM (сек). | Увеличить — меньше таймаутов, но дольше ждать; уменьшить — быстрее фейл, возможны ложные таймауты. Разумно: 60–600 с. |
| `LLM_MAX_TOKENS` | `256` | Максимум новых токенов в ответе (ограничивает длину). | ↑ — полнота↑, latency↑; ↓ — быстрее, но риск усечённых ответов. Разумно: 64–512. |
| `LLM_STREAM_ENABLED` | `false` | Если `true`, Ollama шлёт поток; backend собирает его куски. | Вкл. — сокращает TTFB; выкл. — проще обработка, но ответ целиком. Варианты: `true`/`false`. |
| `QDRANT_URL` | `http://qdrant:6333` | Адрес Qdrant. | Локальный/удалённый кластер; зависит от развертывания. |
| `QDRANT_COLLECTION` | `docs` | Название коллекции. | Можно разделять среды (prod/stage) отдельными коллекциями. |
| `QDRANT_HNSW_M` | `16` | Параметр `m` (разветвлённость графа) при создании коллекции. | ↑ — recall↑, память/индексация↑; ↓ — легче по ресурсам. Разумно: 12–32. |
| `QDRANT_HNSW_EF_CONSTRUCT` | `100` | `ef_construct` для индекса HNSW. | ↑ — точность индексации↑, но запись медленнее; ↓ — наоборот. Разумно: 80–200. |
| `QDRANT_HNSW_EF_SEARCH` | `64` | `ef` при поиске (баланс точности/латентности). | ↑ — recall↑, latency↑; ↓ — быстрее, но риск пропусков. Разумно: 32–128. |
| `DOC_TYPE_MODEL_PATH` | `backend/models/doc_type_classifier.joblib` | Путь к классификатору типов документов. | Указать свой путь — использовать новую модель; пусто — только эвристики. Значение: произвольный путь к `.joblib`. |
| `AUTO_DOC_TYPES` | `true` | Автодобавление `doc_types` по ключевым словам запроса. | Вкл. — меньше шума, но зависит от словаря; выкл. — полный контроль у клиента. Варианты: `true`/`false`. |
| `CHUNK_TOKENS` | `400` | Размер чанка при индексации (словами). | ↑ — меньше чанков, но тяжёлый контекст; ↓ — точнее, но больше записей. Разумно: 200–800. |
| `CHUNK_OVERLAP` | `50` | Перехлёст чанков (слов). | ↑ — меньше разрывов, но больше объём; ↓ — быстрее индексация, риск потери связок. Разумно: 30–200. |
| `CONTEXT_MAX_CHUNKS` | `6` | Максимальное число чанков, передаваемых в LLM. | ↑ — больше фактов, latency↑; ↓ — быстрее, но риск пропусков. Разумно: 4–10. |
| `ONE_CHUNK_PER_DOC` | `true` | Сильно ограничивает результат по одному чанку каждого документа с адаптивной догрузкой. | Вкл. — меньше дубликатов, токенов↓; выкл. — больше деталей из одного документа. Варианты: `true`/`false`. |
| `CONTEXT_SNIPPET_MAX_CHARS` | `600` | Максимальная длина сжатого фрагмента, передаваемого в промпт. | ↑ — контекст богаче, latency↑; ↓ — быстрее, но риск усечений. Разумно: 400–1000. |
| `TOP_K_DEFAULT` | `6` | Базовое значение `top_k` (адаптивно корректируется). | ↑ — больше кандидатов, latency↑; ↓ — быстрее, но меньше вариантов. Разумно: 4–8. |
| `TOP_K_MIN` | `4` | Минимальное `top_k` для “узких” запросов. | ↑ — даже узкие запросы берут больше контента; ↓ — агрессивно сжимаем контекст. Разумно: 3–5. |
| `TOP_K_MAX` | `8` | Верхний предел `top_k` для “широких” запросов. | ↑ — запас для широких запросов, latency↑; ↓ — быстрее, но меньше покрытия. Разумно: 6–10. |
| `MMR_ENABLED` | `true` | Включает Maximal Marginal Relevance перед RAG. | Вкл. — разнообразие↑; выкл. — быстрее, но больше повторов. Варианты: `true`/`false`. |
| `MMR_LAMBDA` | `0.7` | Баланс релевантность/диверсификация (0–1). | →1 — ближе к запросу; →0 — больше разнообразия. Диапазон: 0.3–0.9. |
| `MMR_CANDIDATE_MULTIPLIER` | `3` | Во сколько раз расширять пул кандидатов перед MMR. | ↑ — качество↑, latency↑; ↓ — быстрее, но меньше эффект MMR. Разумно: 2–4. |
| `CACHE_ENABLED` | `true` | Включает кэш для `/search` и `/ask`. | Вкл. — повторные запросы быстрее; выкл. — всегда свежие ответы. Варианты: `true`/`false`. |
| `CACHE_TTL_SECONDS` | `300` | TTL (сек) для элементов кэша. | ↑ — больше reuse, но риск устаревших ответов; ↓ — чаще обновляется. Разумно: 60–900. |
| `CACHE_MAX_ITEMS` | `256` | Максимум элементов в кэше (LRU). | ↑ — выше hit rate, память↑; ↓ — экономия памяти. Разумно: 128–512. |
| `RERANK_ENABLED` | `false` | Включает cross-encoder rerank. | Вкл. — качество↑, задержка↑; выкл. — быстрее. Варианты: `true`/`false`. |
| `RERANK_MODEL` | `cross-encoder/ms-marco-MiniLM-L-6-v2` | HuggingFace модель для rerank. | Более тяжёлая модель = точность↑/ресурсы↑; лёгкая — наоборот. |
| `RERANK_MAX_CANDIDATES` | `32` | Сколько кандидатов передавать reranker. | ↑ — качество↑, latency↑; ↓ — быстрее. Разумно: 16–64. |
| `RERANK_BATCH_SIZE` | `16` | Батч при инференсе cross-encoder. | ↑ — лучше для GPU, память↑; ↓ — безопаснее на CPU. Разумно: 8–32. |
- После обновления схемы рекомендуется пересобрать индекс: `make ingest SPACE=<space>`.
- Для обучения/обновления классификатора: `make train-doctypes` (выполняется внутри backend-контейнера, сохраняет модель по пути `/app/backend/models/doc_type_classifier.joblib`). Модель монтируется на хост (`./backend/models`), поэтому переживает перезапуски. При необходимости можно вручную заменить `doc_type_classifier.joblib` и перезапустить backend.

### Поиск по категориям
- `/search` и `/ask` принимают параметр/поле `doc_types` (список строк). Пример: `GET /search?q=...&doc_types=email_correspondence&doc_types=protocols`.
- Без указания `doc_types` сервис пытается определить категорию автоматически по тексту запроса; отключить можно через `AUTO_DOC_TYPES=false`.
- Дополнительно можно реализовать запросы в свободной форме (например “найди в email‑переписке”) на стороне клиента, сопоставляя ключевые слова с `doc_types`.

## Healthcheck
- Эндпоинт `GET http://localhost:8000/health` возвращает состояние backend, подключение к Qdrant, загрузку эмбеддера и доступность Ollama.
- Docker healthcheck настроен для `backend` сервиса.
- Эндпоинт `GET http://localhost:8000/metrics` выдаёт агрегированные метрики (латентность, токены, cache hit rate).
- Диаграмма пайплайна: см. [`docs/pipeline_diagram.md`](docs/pipeline_diagram.md) (mermaid-блок описывает обработку `/search` и `/ask`).

## Лицензия
MIT (пример).
