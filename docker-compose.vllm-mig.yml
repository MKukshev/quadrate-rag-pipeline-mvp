# Docker Compose configuration with vLLM using NVIDIA MIG
# MIG (Multi-Instance GPU) allows partitioning a single GPU into multiple isolated instances
# Usage: docker-compose -f docker-compose.vllm-mig.yml up -d

version: "3.9"

services:
  qdrant:
    build:
      context: .
      dockerfile: Dockerfile.qdrant
    image: quadrate-rag-pipeline-mvp-qdrant
    ports: ["6333:6333"]
    volumes:
      - qdrant_storage:/qdrant/storage
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-fsS", "http://127.0.0.1:6333/healthz"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s

  # vLLM сервис #1: MEDIUM model (большая модель, требует больше памяти)
  vllm-medium:
    build:
      context: .
      dockerfile: backend/Dockerfile.vllm
    container_name: vllm-medium
    ports: ["8001:8001"]
    environment:
      # Model configuration
      - VLLM_MODEL=${VLLM_MODEL_MEDIUM:-OpenGPT/gpt-oss-20b}
      
      # GPU parameters (оптимизировано для medium модели)
      - VLLM_GPU_MEMORY_UTILIZATION=${VLLM_GPU_MEMORY_UTILIZATION_MEDIUM:-0.85}
      - VLLM_MAX_MODEL_LEN=${VLLM_MAX_MODEL_LEN_MEDIUM:-8192}
      - VLLM_TENSOR_PARALLEL_SIZE=${VLLM_TENSOR_PARALLEL_SIZE:-1}
      
      # Additional parameters (уменьшено для экономии памяти)
      - VLLM_MAX_NUM_SEQS=${VLLM_MAX_NUM_SEQS_MEDIUM:-128}
      - VLLM_ENFORCE_EAGER=${VLLM_ENFORCE_EAGER:-false}
      
      # Оптимизации для больших моделей
      - VLLM_ENABLE_CHUNKED_PREFILL=true
      - HF_HUB_OFFLINE=${HF_HUB_OFFLINE:-0}
      
      # HuggingFace token (для gated моделей)
      - HF_TOKEN=${HF_TOKEN:-}
      
      # MIG specific - используем 2g.48gb инстанс для medium модели
      - NVIDIA_VISIBLE_DEVICES=${MIG_MEDIUM:-all}
    
    volumes:
      - vllm_cache:/root/.cache/huggingface
    
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              # MIG device для medium модели (обычно 2g.48gb)
              # Format: MIG-GPU-UUID/GI/CI
              device_ids: ["${MIG_MEDIUM:-0}"]
              capabilities: [gpu]
    
    restart: unless-stopped
    
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8001/health"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 120s
    
    shm_size: '4gb'

  # vLLM сервис #2: SMALL model (меньшая модель, быстрее работает)
  vllm-small:
    build:
      context: .
      dockerfile: backend/Dockerfile.vllm
    container_name: vllm-small
    ports: ["8002:8001"]
    environment:
      # Model configuration
      - VLLM_MODEL=${VLLM_MODEL_SMALL:-mistralai/Mistral-7B-Instruct-v0.3}
      
      # GPU parameters (оптимизировано для small модели)
      - VLLM_GPU_MEMORY_UTILIZATION=${VLLM_GPU_MEMORY_UTILIZATION_SMALL:-0.90}
      - VLLM_MAX_MODEL_LEN=${VLLM_MAX_MODEL_LEN_SMALL:-16384}
      - VLLM_TENSOR_PARALLEL_SIZE=${VLLM_TENSOR_PARALLEL_SIZE:-1}
      
      # Additional parameters (можно больше для меньшей модели)
      - VLLM_MAX_NUM_SEQS=${VLLM_MAX_NUM_SEQS_SMALL:-256}
      - VLLM_ENFORCE_EAGER=${VLLM_ENFORCE_EAGER:-false}
      
      # HuggingFace token (для gated моделей)
      - HF_TOKEN=${HF_TOKEN:-}
      - HF_HUB_OFFLINE=${HF_HUB_OFFLINE:-0}
      
      # MIG specific - используем 1g.24gb инстанс для small модели
      # Можно также использовать 2g.48gb если есть свободный
      - NVIDIA_VISIBLE_DEVICES=${MIG_SMALL:-all}
    
    volumes:
      - vllm_cache:/root/.cache/huggingface
    
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              # MIG device для small модели (обычно 1g.24gb)
              device_ids: ["${MIG_SMALL:-1}"]
              capabilities: [gpu]
    
    restart: unless-stopped
    
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8001/health"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 90s
    
    shm_size: '2gb'

  backend:
    build:
      context: .
      dockerfile: backend/Dockerfile.vllm
    command: ["uvicorn", "backend.app:app", "--host", "0.0.0.0", "--port", "8000"]
    ports: ["8000:8000"]
    environment:
      # Qdrant
      - QDRANT_URL=http://qdrant:6333
      - QDRANT_COLLECTION=docs
      
      # Embeddings
      - EMBED_MODEL=sentence-transformers/all-MiniLM-L6-v2
      
      # Keyword Index
      - KEYWORD_INDEX_DIR=/data/whoosh_index
      
      # LLM Configuration - vLLM mode
      - LLM_MODE=vllm
      - LLM_VLLM_URL=http://vllm-medium:8001/v1
      - LLM_MODEL=${VLLM_MODEL_MEDIUM:-OpenGPT/gpt-oss-20b}
      - LLM_TIMEOUT=120
      - LLM_MAX_TOKENS=512
      - LLM_TEMPERATURE=0.3
      
      # Document Processing
      - CHUNK_TOKENS=400
      - CHUNK_OVERLAP=50
      
      # Search & RAG
      - CONTEXT_MAX_CHUNKS=6
      - TOP_K_DEFAULT=6
      
      # Caching
      - CACHE_ENABLED=true
      - CACHE_TTL_SECONDS=600
      
      # MIG specific - CUDA visible devices
      # Используем 1g.24gb инстанс для backend (embeddings)
      - NVIDIA_VISIBLE_DEVICES=${MIG_1G_24GB:-all}
    
    volumes:
      - backend_data:/data
      - ./backend/models:/app/backend/models
      - ./backend:/app/backend:ro
      - ./docs:/app/docs:ro
      - ./config:/app/config:ro
      - ./cli:/app/cli:ro
    
    depends_on:
      qdrant:
        condition: service_healthy
      vllm-medium:
        condition: service_healthy
      vllm-small:
        condition: service_healthy
    
    restart: unless-stopped
    
    deploy:
      resources:
        limits:
          memory: 8G
        reservations:
          memory: 4G
          devices:
            - driver: nvidia
              # MIG device для 1g.24gb (Backend embeddings)
              device_ids: ["${MIG_1G_24GB:-0}"]
              capabilities: [gpu]
    
    healthcheck:
      test: ["CMD", "curl", "-fsS", "http://localhost:8000/health"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 60s

volumes:
  qdrant_storage:
  vllm_cache:
  backend_data:
